{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShareBottomModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, p = 0.2):\n",
    "        super().__init__()\n",
    "        self.bottom = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.tower1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.tower2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.tower3 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.tower4 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.tower5 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bottom(x)\n",
    "        return self.tower1(x), self.tower2(x), self.tower3(x), self.tower4(x), self.tower5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, expert_size):\n",
    "        super().__init__()\n",
    "        self.expert_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, expert_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.expert_layer(x)\n",
    "\n",
    "\n",
    "# 门控专家网络\n",
    "class ExpertGate(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, expert_size, n_expert, n_task, use_gate = True, multigate = True):\n",
    "        super().__init__()\n",
    "        self.n_task = n_task\n",
    "        self.use_gate = use_gate\n",
    "        self.multigate = multigate\n",
    "        \n",
    "        # 专家网络 #\n",
    "        # 每个专家网络接受一个输入向量，输出一个向量\n",
    "        for i in range(n_expert):\n",
    "            setattr(self, \"expert_layer\" + str( i + 1), Expert(input_size, hidden_size, expert_size))\n",
    "        self.expert_layers = [getattr(self, \"expert_layer\" + str(i + 1)) for i in range(n_expert)]\n",
    "        \n",
    "        # 门控网络 #\n",
    "        # 如果是多门控机制，针对每个子任务设置一组门控单元；如果是单门控，则只使用一组门控单元\n",
    "        if multigate:\n",
    "            for i in range(n_task):\n",
    "                setattr(self, \"gate_layer\" + str(i + 1), nn.Sequential(nn.Linear(input_size, n_expert), nn.Softmax(dim = 1)))\n",
    "            self.gate_layers = [getattr(self, \"gate_layer\" + str(i + 1)) for i in range(n_task)]\n",
    "        else:\n",
    "            self.gate_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, n_expert),\n",
    "                nn.Softmax(dim = 1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.use_gate:\n",
    "            E_net = [expert(x) for expert in self.expert_layers]\n",
    "            E_net = torch.cat(([e[:, np.newaxis, :] for e in E_net]), dim = 1) # 维度 (batch_size, n_expert, expert_dim)\n",
    "            towers = []\n",
    "            \n",
    "            if self.multigate:\n",
    "                gate_net = [gate(x) for gate in self.gate_layers] # n_task个(batch_size, n_expert)\n",
    "                for i in range(self.n_task):\n",
    "                    g = gate_net[i].unsqueeze(2) # (batch_size, n_expert, 1)\n",
    "                    tower = torch.matmul(E_net.transpose(1, 2), g) # (batch_size, expert_dim, 1)\n",
    "                    towers.append(tower.squeeze(-1)) # (batch_size, expert_dim)\n",
    "            else:\n",
    "                gate = self.gate_layer(x)\n",
    "                for i in range(self.n_task):\n",
    "                    g = gate.unsqueeze(2) \n",
    "                    tower = torch.matmul(E_net.transpose(1, 2), g) \n",
    "                    towers.append(tower.squeeze(-1)) \n",
    "        else:\n",
    "            E_net = [expert(x) for expert in self.expert_layers]\n",
    "            towers = sum(E_net) / len(E_net)\n",
    "        return towers\n",
    "\n",
    "\n",
    "# MMoE 模型；当不使用多门控机制时退化为单门控MoE模型\n",
    "class MMoE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, expert_size, n_expert, n_task, use_gate = True, multigate = True):\n",
    "        super().__init__()\n",
    "        self.use_gate = use_gate\n",
    "        self.expert_gate = ExpertGate(input_size, hidden_size, expert_size, n_expert, n_task, use_gate, multigate)\n",
    "        \n",
    "        self.tower1 = nn.Sequential(\n",
    "            nn.Linear(expert_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.tower2 = nn.Sequential(\n",
    "            nn.Linear(expert_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.tower3 = nn.Sequential(\n",
    "            nn.Linear(expert_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.tower4 = nn.Sequential(\n",
    "            nn.Linear(expert_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.tower5 = nn.Sequential(\n",
    "            nn.Linear(expert_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        towers = self.expert_gate(x)\n",
    "        if self.use_gate:\n",
    "            output1 = self.tower1(towers[0])\n",
    "            output2 = self.tower2(towers[1])\n",
    "            output3 = self.tower3(towers[2])\n",
    "            output4 = self.tower4(towers[3])\n",
    "            output5 = self.tower5(towers[4])\n",
    "        else:\n",
    "            output1 = self.tower1(towers)\n",
    "            output2 = self.tower2(towers)\n",
    "            output3 = self.tower3(towers)\n",
    "            output4 = self.tower4(towers)\n",
    "            output5 = self.tower5(towers)\n",
    "        return output1, output2, output3, output4, output5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmoe = MMoE(16, 8, 8, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMoE(\n",
      "  (expert_gate): ExpertGate(\n",
      "    (expert_layer1): Expert(\n",
      "      (expert_layer): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (expert_layer2): Expert(\n",
      "      (expert_layer): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (expert_layer3): Expert(\n",
      "      (expert_layer): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=8, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (gate_layer1): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=3, bias=True)\n",
      "      (1): Softmax(dim=1)\n",
      "    )\n",
      "    (gate_layer2): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=3, bias=True)\n",
      "      (1): Softmax(dim=1)\n",
      "    )\n",
      "    (gate_layer3): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=3, bias=True)\n",
      "      (1): Softmax(dim=1)\n",
      "    )\n",
      "    (gate_layer4): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=3, bias=True)\n",
      "      (1): Softmax(dim=1)\n",
      "    )\n",
      "    (gate_layer5): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=3, bias=True)\n",
      "      (1): Softmax(dim=1)\n",
      "    )\n",
      "  )\n",
      "  (tower1): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      "  (tower2): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      "  (tower3): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      "  (tower4): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      "  (tower5): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mmoe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = io.loadmat(\"/Users/zhangyuyao/Desktop/大创/chunxuan/UserInfo/static/data1.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"X\"][:, :12]\n",
    "Score = data[\"Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 1.65000000e+02, 6.00000000e+01, 1.95000000e+03,\n",
       "       2.01857143e+03, 3.00000000e+00, 4.00000000e+00, 3.50000000e+01,\n",
       "       2.00000000e+00, 4.00000000e+00])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdscler = StandardScaler()\n",
    "X = stdscler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Score, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ADDataset(Dataset):\n",
    "    def __init__(self, X_train, y_train) -> None:\n",
    "        super().__init__()\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_train[index], self.y_train[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "train_set = ADDataset(X_train, y_train)\n",
    "test_set = ADDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_set, \n",
    "                          batch_size = 32,\n",
    "                          shuffle = True)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size = len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sbm():\n",
    "    epochs = 300\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    criterion = nn.MSELoss()\n",
    "    sbm = ShareBottomModel(12, 16)\n",
    "    optimizer = torch.optim.AdamW(sbm.parameters(), lr = 5e-4)\n",
    "    for epoch in range(epochs):\n",
    "        Loss = 0.0\n",
    "        for data in train_loader:\n",
    "            x, y = data\n",
    "            optimizer.zero_grad()\n",
    "            y1, y2, y3, y4, y5 = sbm(x.float())\n",
    "            loss = criterion(y1, y[:, 0].unsqueeze(-1)) + criterion(y2, y[:, 1].unsqueeze(-1)) + criterion(y3, y[:, 2].unsqueeze(-1)) + criterion(y4, y[:, 3].unsqueeze(-1)) + criterion(y5, y[:, 4].unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            Loss += loss.item()\n",
    "        train_loss.append(Loss)\n",
    "        with torch.no_grad():\n",
    "            y1, y2, y3, y4, y5 = sbm(X_test)\n",
    "            test = (criterion(y1, y_test[:, 0].unsqueeze(-1)) + criterion(y2, y_test[:, 1].unsqueeze(-1)) + criterion(y3, y_test[:, 2].unsqueeze(-1)) + criterion(y4, y_test[:, 3].unsqueeze(-1)) + criterion(y5, y_test[:, 4].unsqueeze(-1))).item()\n",
    "            test_loss.append(test)\n",
    "        print(\"epoch: {}, batch loss: {}\".format(epoch + 1, Loss / len(train_loader)))\n",
    "    return np.array(test_loss[101:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].plot(train_loss)\n",
    "axes[1].plot(test_loss)\n",
    "plt.savefig(\"sbm.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26.5810753712103,\n",
       " 26.00145764566546,\n",
       " 25.525253295898438,\n",
       " 25.563189645508427,\n",
       " 26.20262561012153,\n",
       " 24.9569956621333,\n",
       " 21.588152391826686,\n",
       " 22.578990658323967,\n",
       " 24.904189392549906,\n",
       " 21.873278392619223]"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mmoe():\n",
    "    epochs = 300\n",
    "    criterion = nn.MSELoss()\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    mmoe = MMoE(12, 8, 8, 3, 5) \n",
    "    optimizer = torch.optim.AdamW(mmoe.parameters(), lr = 5e-4)\n",
    "    for epoch in range(epochs):\n",
    "        Loss = 0.0\n",
    "        for data in train_loader:\n",
    "            x, y = data\n",
    "            optimizer.zero_grad()\n",
    "            y1, y2, y3, y4, y5 = mmoe(x)\n",
    "            loss = criterion(y1, y[:, 0].unsqueeze(-1)) + criterion(y2, y[:, 1].unsqueeze(-1)) + criterion(y3, y[:, 2].unsqueeze(-1)) + criterion(y4, y[:, 3].unsqueeze(-1)) + criterion(y5, y[:, 4].unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            Loss += loss.item()\n",
    "        train_loss.append(Loss)\n",
    "        with torch.no_grad():\n",
    "            y1, y2, y3, y4, y5 = mmoe(X_test)\n",
    "            test = (criterion(y1, y_test[:, 0].unsqueeze(-1)) + criterion(y2, y_test[:, 1].unsqueeze(-1)) + criterion(y3, y_test[:, 2].unsqueeze(-1)) + criterion(y4, y_test[:, 3].unsqueeze(-1)) + criterion(y5, y_test[:, 4].unsqueeze(-1))).item()\n",
    "            test_loss.append(test)\n",
    "        print(\"epoch: {}, batch loss: {}\".format(epoch + 1, Loss / len(train_loader)))\n",
    "    return np.array(test_loss[101:]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, batch loss: 785.8115946451823\n",
      "epoch: 2, batch loss: 778.9855804443359\n",
      "epoch: 3, batch loss: 774.926259358724\n",
      "epoch: 4, batch loss: 774.2130025227865\n",
      "epoch: 5, batch loss: 769.5435994466146\n",
      "epoch: 6, batch loss: 762.9115193684896\n",
      "epoch: 7, batch loss: 755.5383961995443\n",
      "epoch: 8, batch loss: 748.0529123942057\n",
      "epoch: 9, batch loss: 739.8250172932943\n",
      "epoch: 10, batch loss: 719.3639373779297\n",
      "epoch: 11, batch loss: 699.974110921224\n",
      "epoch: 12, batch loss: 672.7014312744141\n",
      "epoch: 13, batch loss: 639.5906168619791\n",
      "epoch: 14, batch loss: 588.3909861246744\n",
      "epoch: 15, batch loss: 529.0458857218424\n",
      "epoch: 16, batch loss: 471.8142344156901\n",
      "epoch: 17, batch loss: 400.6237258911133\n",
      "epoch: 18, batch loss: 332.39956919352215\n",
      "epoch: 19, batch loss: 260.3936411539714\n",
      "epoch: 20, batch loss: 213.66571299235025\n",
      "epoch: 21, batch loss: 164.3678824106852\n",
      "epoch: 22, batch loss: 134.91260846455893\n",
      "epoch: 23, batch loss: 115.64458084106445\n",
      "epoch: 24, batch loss: 97.73488807678223\n",
      "epoch: 25, batch loss: 89.96422894795735\n",
      "epoch: 26, batch loss: 81.42546272277832\n",
      "epoch: 27, batch loss: 76.24142583211263\n",
      "epoch: 28, batch loss: 69.74770228068034\n",
      "epoch: 29, batch loss: 66.95483144124348\n",
      "epoch: 30, batch loss: 62.53537686665853\n",
      "epoch: 31, batch loss: 60.070793787638344\n",
      "epoch: 32, batch loss: 56.698469479878746\n",
      "epoch: 33, batch loss: 53.07919216156006\n",
      "epoch: 34, batch loss: 51.75227928161621\n",
      "epoch: 35, batch loss: 50.80106417338053\n",
      "epoch: 36, batch loss: 49.5845677057902\n",
      "epoch: 37, batch loss: 47.135809898376465\n",
      "epoch: 38, batch loss: 48.968453884124756\n",
      "epoch: 39, batch loss: 47.704097270965576\n",
      "epoch: 40, batch loss: 44.06979465484619\n",
      "epoch: 41, batch loss: 44.267552057902016\n",
      "epoch: 42, batch loss: 43.08735275268555\n",
      "epoch: 43, batch loss: 44.011751651763916\n",
      "epoch: 44, batch loss: 41.95400587717692\n",
      "epoch: 45, batch loss: 41.833799044291176\n",
      "epoch: 46, batch loss: 41.12599356969198\n",
      "epoch: 47, batch loss: 40.373839378356934\n",
      "epoch: 48, batch loss: 40.545382499694824\n",
      "epoch: 49, batch loss: 39.259618282318115\n",
      "epoch: 50, batch loss: 38.23537874221802\n",
      "epoch: 51, batch loss: 39.30482482910156\n",
      "epoch: 52, batch loss: 38.40972487131754\n",
      "epoch: 53, batch loss: 38.22704537709554\n",
      "epoch: 54, batch loss: 39.0068351427714\n",
      "epoch: 55, batch loss: 36.61772473653158\n",
      "epoch: 56, batch loss: 37.28548717498779\n",
      "epoch: 57, batch loss: 37.76790634791056\n",
      "epoch: 58, batch loss: 36.756003856658936\n",
      "epoch: 59, batch loss: 35.70359627405802\n",
      "epoch: 60, batch loss: 36.78326082229614\n",
      "epoch: 61, batch loss: 36.10926580429077\n",
      "epoch: 62, batch loss: 35.57691113154093\n",
      "epoch: 63, batch loss: 34.25419282913208\n",
      "epoch: 64, batch loss: 35.59093523025513\n",
      "epoch: 65, batch loss: 34.370784282684326\n",
      "epoch: 66, batch loss: 33.883718490600586\n",
      "epoch: 67, batch loss: 33.62188768386841\n",
      "epoch: 68, batch loss: 34.18714952468872\n",
      "epoch: 69, batch loss: 33.005586783091225\n",
      "epoch: 70, batch loss: 32.386954148610435\n",
      "epoch: 71, batch loss: 33.351075172424316\n",
      "epoch: 72, batch loss: 32.078284740448\n",
      "epoch: 73, batch loss: 32.134048302968345\n",
      "epoch: 74, batch loss: 31.634087721506756\n",
      "epoch: 75, batch loss: 31.313575744628906\n",
      "epoch: 76, batch loss: 31.43492841720581\n",
      "epoch: 77, batch loss: 30.83125654856364\n",
      "epoch: 78, batch loss: 31.349541982014973\n",
      "epoch: 79, batch loss: 30.878923575083416\n",
      "epoch: 80, batch loss: 30.83458201090495\n",
      "epoch: 81, batch loss: 30.86545769373576\n",
      "epoch: 82, batch loss: 32.527802308400474\n",
      "epoch: 83, batch loss: 29.97517426808675\n",
      "epoch: 84, batch loss: 30.30347553888957\n",
      "epoch: 85, batch loss: 30.51362689336141\n",
      "epoch: 86, batch loss: 29.78741200764974\n",
      "epoch: 87, batch loss: 30.265934467315674\n",
      "epoch: 88, batch loss: 29.568336804707844\n",
      "epoch: 89, batch loss: 29.049806118011475\n",
      "epoch: 90, batch loss: 30.16957712173462\n",
      "epoch: 91, batch loss: 29.465613842010498\n",
      "epoch: 92, batch loss: 29.46637996037801\n",
      "epoch: 93, batch loss: 28.718334674835205\n",
      "epoch: 94, batch loss: 28.55307102203369\n",
      "epoch: 95, batch loss: 28.105597496032715\n",
      "epoch: 96, batch loss: 28.47772725423177\n",
      "epoch: 97, batch loss: 28.754976908365887\n",
      "epoch: 98, batch loss: 28.351698398590088\n",
      "epoch: 99, batch loss: 28.27158037821452\n",
      "epoch: 100, batch loss: 28.34951988855998\n",
      "epoch: 101, batch loss: 28.059228897094727\n",
      "epoch: 102, batch loss: 27.40202236175537\n",
      "epoch: 103, batch loss: 27.65025250116984\n",
      "epoch: 104, batch loss: 28.744314670562744\n",
      "epoch: 105, batch loss: 27.5235595703125\n",
      "epoch: 106, batch loss: 27.104736010233562\n",
      "epoch: 107, batch loss: 28.125255584716797\n",
      "epoch: 108, batch loss: 27.3730575243632\n",
      "epoch: 109, batch loss: 26.775051752726238\n",
      "epoch: 110, batch loss: 27.849137783050537\n",
      "epoch: 111, batch loss: 27.048640727996826\n",
      "epoch: 112, batch loss: 26.982901255289715\n",
      "epoch: 113, batch loss: 26.85047133763631\n",
      "epoch: 114, batch loss: 26.01038424173991\n",
      "epoch: 115, batch loss: 26.532891909281414\n",
      "epoch: 116, batch loss: 26.96136220296224\n",
      "epoch: 117, batch loss: 26.129263559977215\n",
      "epoch: 118, batch loss: 25.830220222473145\n",
      "epoch: 119, batch loss: 26.09212001164754\n",
      "epoch: 120, batch loss: 26.72052812576294\n",
      "epoch: 121, batch loss: 27.92547607421875\n",
      "epoch: 122, batch loss: 26.230266729990642\n",
      "epoch: 123, batch loss: 25.84472878774007\n",
      "epoch: 124, batch loss: 26.381417115529377\n",
      "epoch: 125, batch loss: 25.20510260264079\n",
      "epoch: 126, batch loss: 25.845829645792644\n",
      "epoch: 127, batch loss: 25.330563227335613\n",
      "epoch: 128, batch loss: 24.723772207895916\n",
      "epoch: 129, batch loss: 25.070970853169758\n",
      "epoch: 130, batch loss: 24.885982195536297\n",
      "epoch: 131, batch loss: 25.02181800206502\n",
      "epoch: 132, batch loss: 24.8276154200236\n",
      "epoch: 133, batch loss: 24.886081059773762\n",
      "epoch: 134, batch loss: 25.1399884223938\n",
      "epoch: 135, batch loss: 25.02453867594401\n",
      "epoch: 136, batch loss: 24.37309233347575\n",
      "epoch: 137, batch loss: 25.265273571014404\n",
      "epoch: 138, batch loss: 24.771604855855305\n",
      "epoch: 139, batch loss: 24.247292121251423\n",
      "epoch: 140, batch loss: 25.817304770151775\n",
      "epoch: 141, batch loss: 24.559877554575603\n",
      "epoch: 142, batch loss: 24.175249576568604\n",
      "epoch: 143, batch loss: 24.81596342722575\n",
      "epoch: 144, batch loss: 24.6085368792216\n",
      "epoch: 145, batch loss: 24.431374073028564\n",
      "epoch: 146, batch loss: 24.237955570220947\n",
      "epoch: 147, batch loss: 24.30528497695923\n",
      "epoch: 148, batch loss: 25.27354907989502\n",
      "epoch: 149, batch loss: 24.12810405095418\n",
      "epoch: 150, batch loss: 24.504432280858357\n",
      "epoch: 151, batch loss: 24.13433535893758\n",
      "epoch: 152, batch loss: 23.6636799176534\n",
      "epoch: 153, batch loss: 24.17379331588745\n",
      "epoch: 154, batch loss: 23.865377108256023\n",
      "epoch: 155, batch loss: 23.54308255513509\n",
      "epoch: 156, batch loss: 24.339073816935223\n",
      "epoch: 157, batch loss: 23.56588904062907\n",
      "epoch: 158, batch loss: 23.547633012135822\n",
      "epoch: 159, batch loss: 23.636130650838215\n",
      "epoch: 160, batch loss: 24.01056432723999\n",
      "epoch: 161, batch loss: 23.084499835968018\n",
      "epoch: 162, batch loss: 24.068220297495525\n",
      "epoch: 163, batch loss: 24.519261837005615\n",
      "epoch: 164, batch loss: 23.430314699808758\n",
      "epoch: 165, batch loss: 23.847463448842365\n",
      "epoch: 166, batch loss: 23.33017571767171\n",
      "epoch: 167, batch loss: 23.08927647272746\n",
      "epoch: 168, batch loss: 23.716989676157635\n",
      "epoch: 169, batch loss: 22.714480876922607\n",
      "epoch: 170, batch loss: 23.0111403465271\n",
      "epoch: 171, batch loss: 24.080535252888996\n",
      "epoch: 172, batch loss: 23.472487608591717\n",
      "epoch: 173, batch loss: 22.875049273173016\n",
      "epoch: 174, batch loss: 23.166348457336426\n",
      "epoch: 175, batch loss: 23.464084148406982\n",
      "epoch: 176, batch loss: 23.05438741048177\n",
      "epoch: 177, batch loss: 22.92402760187785\n",
      "epoch: 178, batch loss: 22.661319732666016\n",
      "epoch: 179, batch loss: 22.487459897994995\n",
      "epoch: 180, batch loss: 22.436367670694988\n",
      "epoch: 181, batch loss: 22.598298867543537\n",
      "epoch: 182, batch loss: 23.197907606760662\n",
      "epoch: 183, batch loss: 22.51647885640462\n",
      "epoch: 184, batch loss: 22.69679530461629\n",
      "epoch: 185, batch loss: 23.229994376500446\n",
      "epoch: 186, batch loss: 22.59685246149699\n",
      "epoch: 187, batch loss: 22.491108894348145\n",
      "epoch: 188, batch loss: 23.035642623901367\n",
      "epoch: 189, batch loss: 22.72100631395976\n",
      "epoch: 190, batch loss: 22.809440453847248\n",
      "epoch: 191, batch loss: 22.63170560201009\n",
      "epoch: 192, batch loss: 22.915021260579426\n",
      "epoch: 193, batch loss: 22.38692347208659\n",
      "epoch: 194, batch loss: 22.450905481974285\n",
      "epoch: 195, batch loss: 23.116323630015057\n",
      "epoch: 196, batch loss: 22.65958547592163\n",
      "epoch: 197, batch loss: 22.168001969655354\n",
      "epoch: 198, batch loss: 22.53875748316447\n",
      "epoch: 199, batch loss: 22.443105061848957\n",
      "epoch: 200, batch loss: 21.939085642496746\n",
      "epoch: 201, batch loss: 22.174400965372723\n",
      "epoch: 202, batch loss: 22.58850383758545\n",
      "epoch: 203, batch loss: 22.09565083185832\n",
      "epoch: 204, batch loss: 22.303719520568848\n",
      "epoch: 205, batch loss: 22.684277216593426\n",
      "epoch: 206, batch loss: 21.857661644617718\n",
      "epoch: 207, batch loss: 21.897919178009033\n",
      "epoch: 208, batch loss: 22.293668111165363\n",
      "epoch: 209, batch loss: 22.0350817044576\n",
      "epoch: 210, batch loss: 23.149583101272583\n",
      "epoch: 211, batch loss: 21.847220102945965\n",
      "epoch: 212, batch loss: 22.816625118255615\n",
      "epoch: 213, batch loss: 21.714595794677734\n",
      "epoch: 214, batch loss: 21.881420612335205\n",
      "epoch: 215, batch loss: 22.537428220113117\n",
      "epoch: 216, batch loss: 21.78020429611206\n",
      "epoch: 217, batch loss: 22.19011052449544\n",
      "epoch: 218, batch loss: 22.441508769989014\n",
      "epoch: 219, batch loss: 22.3675643603007\n",
      "epoch: 220, batch loss: 22.069314002990723\n",
      "epoch: 221, batch loss: 21.997897307078045\n",
      "epoch: 222, batch loss: 23.17435034116109\n",
      "epoch: 223, batch loss: 21.705413738886516\n",
      "epoch: 224, batch loss: 21.47471086184184\n",
      "epoch: 225, batch loss: 21.8710618019104\n",
      "epoch: 226, batch loss: 21.86451530456543\n",
      "epoch: 227, batch loss: 21.840367476145428\n",
      "epoch: 228, batch loss: 21.50818419456482\n",
      "epoch: 229, batch loss: 21.441234429677326\n",
      "epoch: 230, batch loss: 21.85799702008565\n",
      "epoch: 231, batch loss: 22.175676663716633\n",
      "epoch: 232, batch loss: 21.67146905263265\n",
      "epoch: 233, batch loss: 21.556827704111736\n",
      "epoch: 234, batch loss: 21.992859443028767\n",
      "epoch: 235, batch loss: 21.638879140218098\n",
      "epoch: 236, batch loss: 22.02931046485901\n",
      "epoch: 237, batch loss: 21.412141640981037\n",
      "epoch: 238, batch loss: 22.338382085164387\n",
      "epoch: 239, batch loss: 22.230135440826416\n",
      "epoch: 240, batch loss: 21.55982844034831\n",
      "epoch: 241, batch loss: 22.09279155731201\n",
      "epoch: 242, batch loss: 21.795430660247803\n",
      "epoch: 243, batch loss: 21.19321648279826\n",
      "epoch: 244, batch loss: 21.35720920562744\n",
      "epoch: 245, batch loss: 22.04722007115682\n",
      "epoch: 246, batch loss: 21.284687995910645\n",
      "epoch: 247, batch loss: 21.739168167114258\n",
      "epoch: 248, batch loss: 21.846270243326824\n",
      "epoch: 249, batch loss: 20.896336793899536\n",
      "epoch: 250, batch loss: 21.883204380671184\n",
      "epoch: 251, batch loss: 21.936303933461506\n",
      "epoch: 252, batch loss: 22.442034085591633\n",
      "epoch: 253, batch loss: 20.892894824345905\n",
      "epoch: 254, batch loss: 21.773035685221355\n",
      "epoch: 255, batch loss: 21.77916423479716\n",
      "epoch: 256, batch loss: 22.95478081703186\n",
      "epoch: 257, batch loss: 21.833733876546223\n",
      "epoch: 258, batch loss: 21.146974484125774\n",
      "epoch: 259, batch loss: 21.62346275647481\n",
      "epoch: 260, batch loss: 21.994492610295612\n",
      "epoch: 261, batch loss: 21.302094618479412\n",
      "epoch: 262, batch loss: 21.16621740659078\n",
      "epoch: 263, batch loss: 21.50211723645528\n",
      "epoch: 264, batch loss: 21.25627056757609\n",
      "epoch: 265, batch loss: 20.945066690444946\n",
      "epoch: 266, batch loss: 22.049590905507404\n",
      "epoch: 267, batch loss: 21.031429290771484\n",
      "epoch: 268, batch loss: 20.644635995229084\n",
      "epoch: 269, batch loss: 20.777938445409138\n",
      "epoch: 270, batch loss: 21.40274667739868\n",
      "epoch: 271, batch loss: 22.29071060816447\n",
      "epoch: 272, batch loss: 20.816897869110107\n",
      "epoch: 273, batch loss: 21.10888131459554\n",
      "epoch: 274, batch loss: 21.008296330769856\n",
      "epoch: 275, batch loss: 21.97481632232666\n",
      "epoch: 276, batch loss: 22.280824899673462\n",
      "epoch: 277, batch loss: 20.88096245129903\n",
      "epoch: 278, batch loss: 21.573811133702595\n",
      "epoch: 279, batch loss: 20.747961680094402\n",
      "epoch: 280, batch loss: 20.725438435872395\n",
      "epoch: 281, batch loss: 20.61962564786275\n",
      "epoch: 282, batch loss: 20.664488792419434\n",
      "epoch: 283, batch loss: 21.228047529856365\n",
      "epoch: 284, batch loss: 21.490752061208088\n",
      "epoch: 285, batch loss: 20.99586057662964\n",
      "epoch: 286, batch loss: 21.282743612925213\n",
      "epoch: 287, batch loss: 20.937586307525635\n",
      "epoch: 288, batch loss: 21.141343752543133\n",
      "epoch: 289, batch loss: 21.041314125061035\n",
      "epoch: 290, batch loss: 21.20260747273763\n",
      "epoch: 291, batch loss: 21.16874138514201\n",
      "epoch: 292, batch loss: 21.16380484898885\n",
      "epoch: 293, batch loss: 20.856847604115803\n",
      "epoch: 294, batch loss: 20.877172708511353\n",
      "epoch: 295, batch loss: 21.027368625005085\n",
      "epoch: 296, batch loss: 20.878278652826946\n",
      "epoch: 297, batch loss: 21.05971360206604\n",
      "epoch: 298, batch loss: 20.79087249437968\n",
      "epoch: 299, batch loss: 20.868506749471027\n",
      "epoch: 300, batch loss: 21.976606607437134\n",
      "epoch: 1, batch loss: 780.8133341471354\n",
      "epoch: 2, batch loss: 787.8939056396484\n",
      "epoch: 3, batch loss: 776.0572814941406\n",
      "epoch: 4, batch loss: 768.9418385823568\n",
      "epoch: 5, batch loss: 765.4270884195963\n",
      "epoch: 6, batch loss: 755.6094868977865\n",
      "epoch: 7, batch loss: 748.1787668863932\n",
      "epoch: 8, batch loss: 732.0614166259766\n",
      "epoch: 9, batch loss: 713.5704854329427\n",
      "epoch: 10, batch loss: 686.0077412923177\n",
      "epoch: 11, batch loss: 652.3225504557291\n",
      "epoch: 12, batch loss: 610.1932220458984\n",
      "epoch: 13, batch loss: 558.047243754069\n",
      "epoch: 14, batch loss: 498.3738962809245\n",
      "epoch: 15, batch loss: 427.3796666463216\n",
      "epoch: 16, batch loss: 359.7171147664388\n",
      "epoch: 17, batch loss: 300.98606872558594\n",
      "epoch: 18, batch loss: 236.1662801106771\n",
      "epoch: 19, batch loss: 187.9077377319336\n",
      "epoch: 20, batch loss: 148.97336387634277\n",
      "epoch: 21, batch loss: 119.61790466308594\n",
      "epoch: 22, batch loss: 91.52566019694011\n",
      "epoch: 23, batch loss: 79.31777381896973\n",
      "epoch: 24, batch loss: 68.18260796864827\n",
      "epoch: 25, batch loss: 60.51801776885986\n",
      "epoch: 26, batch loss: 56.188812573750816\n",
      "epoch: 27, batch loss: 52.018609046936035\n",
      "epoch: 28, batch loss: 51.81455707550049\n",
      "epoch: 29, batch loss: 46.8363987604777\n",
      "epoch: 30, batch loss: 47.62618891398112\n",
      "epoch: 31, batch loss: 44.45991849899292\n",
      "epoch: 32, batch loss: 42.666808128356934\n",
      "epoch: 33, batch loss: 43.69827143351237\n",
      "epoch: 34, batch loss: 42.58688767751058\n",
      "epoch: 35, batch loss: 39.91578737894694\n",
      "epoch: 36, batch loss: 39.428783098856606\n",
      "epoch: 37, batch loss: 39.95019404093424\n",
      "epoch: 38, batch loss: 38.259762605031334\n",
      "epoch: 39, batch loss: 38.05103794733683\n",
      "epoch: 40, batch loss: 37.5273962020874\n",
      "epoch: 41, batch loss: 37.765299002329506\n",
      "epoch: 42, batch loss: 37.13064734141032\n",
      "epoch: 43, batch loss: 36.42628701527914\n",
      "epoch: 44, batch loss: 36.356737772623696\n",
      "epoch: 45, batch loss: 34.84165461858114\n",
      "epoch: 46, batch loss: 35.6249893506368\n",
      "epoch: 47, batch loss: 34.677153746287026\n",
      "epoch: 48, batch loss: 33.782713413238525\n",
      "epoch: 49, batch loss: 34.13421297073364\n",
      "epoch: 50, batch loss: 34.09395710627238\n",
      "epoch: 51, batch loss: 32.89549525578817\n",
      "epoch: 52, batch loss: 32.413609186808266\n",
      "epoch: 53, batch loss: 31.966136773427326\n",
      "epoch: 54, batch loss: 33.05682897567749\n",
      "epoch: 55, batch loss: 32.348256746927895\n",
      "epoch: 56, batch loss: 32.37474171320597\n",
      "epoch: 57, batch loss: 31.836958249409992\n",
      "epoch: 58, batch loss: 33.132437229156494\n",
      "epoch: 59, batch loss: 31.365317662556965\n",
      "epoch: 60, batch loss: 32.545553048451744\n",
      "epoch: 61, batch loss: 30.471964995066326\n",
      "epoch: 62, batch loss: 29.982617378234863\n",
      "epoch: 63, batch loss: 32.38428513209025\n",
      "epoch: 64, batch loss: 29.848353385925293\n",
      "epoch: 65, batch loss: 30.301703135172527\n",
      "epoch: 66, batch loss: 30.015493869781494\n",
      "epoch: 67, batch loss: 30.09201733271281\n",
      "epoch: 68, batch loss: 29.736448287963867\n",
      "epoch: 69, batch loss: 29.15604003270467\n",
      "epoch: 70, batch loss: 29.498907725016277\n",
      "epoch: 71, batch loss: 29.460084438323975\n",
      "epoch: 72, batch loss: 29.2901136080424\n",
      "epoch: 73, batch loss: 28.38036060333252\n",
      "epoch: 74, batch loss: 28.52167336146037\n",
      "epoch: 75, batch loss: 28.41733201344808\n",
      "epoch: 76, batch loss: 29.20814911524455\n",
      "epoch: 77, batch loss: 28.659631888071697\n",
      "epoch: 78, batch loss: 27.56387710571289\n",
      "epoch: 79, batch loss: 27.68891970316569\n",
      "epoch: 80, batch loss: 27.60391362508138\n",
      "epoch: 81, batch loss: 29.16946840286255\n",
      "epoch: 82, batch loss: 27.371145566304524\n",
      "epoch: 83, batch loss: 27.52306826909383\n",
      "epoch: 84, batch loss: 28.030651251475017\n",
      "epoch: 85, batch loss: 28.566299597422283\n",
      "epoch: 86, batch loss: 27.682512283325195\n",
      "epoch: 87, batch loss: 26.698736508687336\n",
      "epoch: 88, batch loss: 26.65839433670044\n",
      "epoch: 89, batch loss: 26.531747976938885\n",
      "epoch: 90, batch loss: 26.702720801035564\n",
      "epoch: 91, batch loss: 26.84990056355794\n",
      "epoch: 92, batch loss: 26.590843359629314\n",
      "epoch: 93, batch loss: 27.23627758026123\n",
      "epoch: 94, batch loss: 26.23855972290039\n",
      "epoch: 95, batch loss: 25.681944529215496\n",
      "epoch: 96, batch loss: 26.061489423116047\n",
      "epoch: 97, batch loss: 25.800882816314697\n",
      "epoch: 98, batch loss: 25.62630605697632\n",
      "epoch: 99, batch loss: 25.971765359242756\n",
      "epoch: 100, batch loss: 25.55630095799764\n",
      "epoch: 101, batch loss: 25.096137682596844\n",
      "epoch: 102, batch loss: 25.868784586588543\n",
      "epoch: 103, batch loss: 25.357210795084637\n",
      "epoch: 104, batch loss: 26.07111883163452\n",
      "epoch: 105, batch loss: 25.0203963915507\n",
      "epoch: 106, batch loss: 25.54068899154663\n",
      "epoch: 107, batch loss: 25.595404624938965\n",
      "epoch: 108, batch loss: 24.416947682698567\n",
      "epoch: 109, batch loss: 25.400563557942707\n",
      "epoch: 110, batch loss: 24.77734359105428\n",
      "epoch: 111, batch loss: 24.567694981892902\n",
      "epoch: 112, batch loss: 25.975445588429768\n",
      "epoch: 113, batch loss: 24.801694869995117\n",
      "epoch: 114, batch loss: 24.293639659881592\n",
      "epoch: 115, batch loss: 25.011741638183594\n",
      "epoch: 116, batch loss: 24.729376475016277\n",
      "epoch: 117, batch loss: 24.667548815409344\n",
      "epoch: 118, batch loss: 24.35416078567505\n",
      "epoch: 119, batch loss: 25.551727771759033\n",
      "epoch: 120, batch loss: 24.340365886688232\n",
      "epoch: 121, batch loss: 24.17305787404378\n",
      "epoch: 122, batch loss: 24.034553686777752\n",
      "epoch: 123, batch loss: 23.887852668762207\n",
      "epoch: 124, batch loss: 24.317245165507\n",
      "epoch: 125, batch loss: 24.868836879730225\n",
      "epoch: 126, batch loss: 24.33246072133382\n",
      "epoch: 127, batch loss: 23.489070733388264\n",
      "epoch: 128, batch loss: 23.635304768880207\n",
      "epoch: 129, batch loss: 23.607611020406086\n",
      "epoch: 130, batch loss: 23.544339656829834\n",
      "epoch: 131, batch loss: 24.26923481623332\n",
      "epoch: 132, batch loss: 23.56114451090495\n",
      "epoch: 133, batch loss: 23.41301949818929\n",
      "epoch: 134, batch loss: 24.159611384073894\n",
      "epoch: 135, batch loss: 25.40294949213664\n",
      "epoch: 136, batch loss: 24.002864360809326\n",
      "epoch: 137, batch loss: 23.15870412190755\n",
      "epoch: 138, batch loss: 23.20565700531006\n",
      "epoch: 139, batch loss: 23.121967951456707\n",
      "epoch: 140, batch loss: 24.126487096150715\n",
      "epoch: 141, batch loss: 23.075908819834392\n",
      "epoch: 142, batch loss: 23.52136214574178\n",
      "epoch: 143, batch loss: 23.435391426086426\n",
      "epoch: 144, batch loss: 23.60514195760091\n",
      "epoch: 145, batch loss: 23.09462372461955\n",
      "epoch: 146, batch loss: 23.279924710591633\n",
      "epoch: 147, batch loss: 23.142542997996014\n",
      "epoch: 148, batch loss: 23.09882100423177\n",
      "epoch: 149, batch loss: 23.00816027323405\n",
      "epoch: 150, batch loss: 23.024585882822674\n",
      "epoch: 151, batch loss: 22.830969015757244\n",
      "epoch: 152, batch loss: 23.9210147857666\n",
      "epoch: 153, batch loss: 23.643896261850994\n",
      "epoch: 154, batch loss: 22.93156099319458\n",
      "epoch: 155, batch loss: 22.66385316848755\n",
      "epoch: 156, batch loss: 23.49942413965861\n",
      "epoch: 157, batch loss: 23.253297964731853\n",
      "epoch: 158, batch loss: 23.309910774230957\n",
      "epoch: 159, batch loss: 22.61642599105835\n",
      "epoch: 160, batch loss: 22.21778170267741\n",
      "epoch: 161, batch loss: 22.54341204961141\n",
      "epoch: 162, batch loss: 22.27501352628072\n",
      "epoch: 163, batch loss: 23.78998788197835\n",
      "epoch: 164, batch loss: 22.512330373128254\n",
      "epoch: 165, batch loss: 22.36475960413615\n",
      "epoch: 166, batch loss: 23.081112384796143\n",
      "epoch: 167, batch loss: 22.252307017644245\n",
      "epoch: 168, batch loss: 22.00235112508138\n",
      "epoch: 169, batch loss: 22.844704786936443\n",
      "epoch: 170, batch loss: 22.24228636423747\n",
      "epoch: 171, batch loss: 22.49101988474528\n",
      "epoch: 172, batch loss: 22.494145393371582\n",
      "epoch: 173, batch loss: 22.127177238464355\n",
      "epoch: 174, batch loss: 23.85034243265788\n",
      "epoch: 175, batch loss: 22.46876573562622\n",
      "epoch: 176, batch loss: 22.29068613052368\n",
      "epoch: 177, batch loss: 22.519386132558186\n",
      "epoch: 178, batch loss: 21.682698726654053\n",
      "epoch: 179, batch loss: 22.425011157989502\n",
      "epoch: 180, batch loss: 22.04435968399048\n",
      "epoch: 181, batch loss: 22.068485895792644\n",
      "epoch: 182, batch loss: 21.937324682871502\n",
      "epoch: 183, batch loss: 22.40707794825236\n",
      "epoch: 184, batch loss: 23.035881280899048\n",
      "epoch: 185, batch loss: 22.246883233388264\n",
      "epoch: 186, batch loss: 22.53537368774414\n",
      "epoch: 187, batch loss: 22.32318075497945\n",
      "epoch: 188, batch loss: 21.68502187728882\n",
      "epoch: 189, batch loss: 22.28421227137248\n",
      "epoch: 190, batch loss: 22.232206503550213\n",
      "epoch: 191, batch loss: 22.836525599161785\n",
      "epoch: 192, batch loss: 22.004632711410522\n",
      "epoch: 193, batch loss: 22.621729771296184\n",
      "epoch: 194, batch loss: 21.95770724614461\n",
      "epoch: 195, batch loss: 22.51345745722453\n",
      "epoch: 196, batch loss: 22.39785671234131\n",
      "epoch: 197, batch loss: 21.749677817026775\n",
      "epoch: 198, batch loss: 22.36222569147746\n",
      "epoch: 199, batch loss: 22.005624771118164\n",
      "epoch: 200, batch loss: 22.510515054066975\n",
      "epoch: 201, batch loss: 22.160170396169026\n",
      "epoch: 202, batch loss: 21.4216525554657\n",
      "epoch: 203, batch loss: 21.699102878570557\n",
      "epoch: 204, batch loss: 22.200214862823486\n",
      "epoch: 205, batch loss: 21.543946663538616\n",
      "epoch: 206, batch loss: 21.74044926961263\n",
      "epoch: 207, batch loss: 21.310457626978557\n",
      "epoch: 208, batch loss: 22.549243132273357\n",
      "epoch: 209, batch loss: 21.515564918518066\n",
      "epoch: 210, batch loss: 21.422375679016113\n",
      "epoch: 211, batch loss: 22.028972625732422\n",
      "epoch: 212, batch loss: 22.060696125030518\n",
      "epoch: 213, batch loss: 21.273110310236614\n",
      "epoch: 214, batch loss: 21.526726404825848\n",
      "epoch: 215, batch loss: 21.268574555714924\n",
      "epoch: 216, batch loss: 21.87788724899292\n",
      "epoch: 217, batch loss: 22.019407590230305\n",
      "epoch: 218, batch loss: 22.415321350097656\n",
      "epoch: 219, batch loss: 21.56603455543518\n",
      "epoch: 220, batch loss: 21.85016663869222\n",
      "epoch: 221, batch loss: 21.165985425313313\n",
      "epoch: 222, batch loss: 22.18578791618347\n",
      "epoch: 223, batch loss: 21.780858675638836\n",
      "epoch: 224, batch loss: 22.297630786895752\n",
      "epoch: 225, batch loss: 22.277220408121746\n",
      "epoch: 226, batch loss: 21.268148342768352\n",
      "epoch: 227, batch loss: 21.06273078918457\n",
      "epoch: 228, batch loss: 21.189733505249023\n",
      "epoch: 229, batch loss: 22.069276332855225\n",
      "epoch: 230, batch loss: 21.463953415552776\n",
      "epoch: 231, batch loss: 21.191156546274822\n",
      "epoch: 232, batch loss: 21.498584270477295\n",
      "epoch: 233, batch loss: 21.079035838445026\n",
      "epoch: 234, batch loss: 21.287351369857788\n",
      "epoch: 235, batch loss: 20.687002579371136\n",
      "epoch: 236, batch loss: 21.42023277282715\n",
      "epoch: 237, batch loss: 22.00371551513672\n",
      "epoch: 238, batch loss: 21.31316836675008\n",
      "epoch: 239, batch loss: 21.601146539052326\n",
      "epoch: 240, batch loss: 21.00682306289673\n",
      "epoch: 241, batch loss: 21.37693230311076\n",
      "epoch: 242, batch loss: 21.07426881790161\n",
      "epoch: 243, batch loss: 22.303294738133747\n",
      "epoch: 244, batch loss: 21.335830847422283\n",
      "epoch: 245, batch loss: 21.49285348256429\n",
      "epoch: 246, batch loss: 21.37457784016927\n",
      "epoch: 247, batch loss: 20.77440134684245\n",
      "epoch: 248, batch loss: 21.575059572855633\n",
      "epoch: 249, batch loss: 20.700233300526936\n",
      "epoch: 250, batch loss: 21.11455527941386\n",
      "epoch: 251, batch loss: 20.98388679822286\n",
      "epoch: 252, batch loss: 22.10362195968628\n",
      "epoch: 253, batch loss: 21.571757793426514\n",
      "epoch: 254, batch loss: 21.206520716349285\n",
      "epoch: 255, batch loss: 20.535766919453938\n",
      "epoch: 256, batch loss: 20.982468525568645\n",
      "epoch: 257, batch loss: 21.863128980000813\n",
      "epoch: 258, batch loss: 21.79811143875122\n",
      "epoch: 259, batch loss: 21.25519323348999\n",
      "epoch: 260, batch loss: 20.702687978744507\n",
      "epoch: 261, batch loss: 20.808093865712483\n",
      "epoch: 262, batch loss: 21.108454704284668\n",
      "epoch: 263, batch loss: 20.916746616363525\n",
      "epoch: 264, batch loss: 21.39286478360494\n",
      "epoch: 265, batch loss: 20.50223708152771\n",
      "epoch: 266, batch loss: 20.5306609471639\n",
      "epoch: 267, batch loss: 20.407217899958294\n",
      "epoch: 268, batch loss: 21.475996255874634\n",
      "epoch: 269, batch loss: 20.70954434076945\n",
      "epoch: 270, batch loss: 20.765167872111004\n",
      "epoch: 271, batch loss: 21.89977725346883\n",
      "epoch: 272, batch loss: 21.47973887125651\n",
      "epoch: 273, batch loss: 20.493231852849323\n",
      "epoch: 274, batch loss: 21.117703278859455\n",
      "epoch: 275, batch loss: 20.710252205530804\n",
      "epoch: 276, batch loss: 20.97780927022298\n",
      "epoch: 277, batch loss: 21.229315916697185\n",
      "epoch: 278, batch loss: 22.371352910995483\n",
      "epoch: 279, batch loss: 20.940629243850708\n",
      "epoch: 280, batch loss: 20.552819569905598\n",
      "epoch: 281, batch loss: 20.64124409357707\n",
      "epoch: 282, batch loss: 20.630632718404133\n",
      "epoch: 283, batch loss: 20.753868023554485\n",
      "epoch: 284, batch loss: 21.12607018152873\n",
      "epoch: 285, batch loss: 20.48698393503825\n",
      "epoch: 286, batch loss: 20.59864393870036\n",
      "epoch: 287, batch loss: 20.858025232950848\n",
      "epoch: 288, batch loss: 20.577670494715374\n",
      "epoch: 289, batch loss: 20.404587427775066\n",
      "epoch: 290, batch loss: 20.13071084022522\n",
      "epoch: 291, batch loss: 20.712460120519\n",
      "epoch: 292, batch loss: 21.435698588689167\n",
      "epoch: 293, batch loss: 20.956974983215332\n",
      "epoch: 294, batch loss: 20.432270129521687\n",
      "epoch: 295, batch loss: 20.457656860351562\n",
      "epoch: 296, batch loss: 20.57895572980245\n",
      "epoch: 297, batch loss: 20.51138146718343\n",
      "epoch: 298, batch loss: 22.331454277038574\n",
      "epoch: 299, batch loss: 20.45358967781067\n",
      "epoch: 300, batch loss: 20.37086288134257\n",
      "epoch: 1, batch loss: 791.3778737386068\n",
      "epoch: 2, batch loss: 785.6006317138672\n",
      "epoch: 3, batch loss: 782.5337117513021\n",
      "epoch: 4, batch loss: 775.7575276692709\n",
      "epoch: 5, batch loss: 772.0817464192709\n",
      "epoch: 6, batch loss: 766.1746266682943\n",
      "epoch: 7, batch loss: 754.7883707682291\n",
      "epoch: 8, batch loss: 745.7522532145182\n",
      "epoch: 9, batch loss: 731.6039276123047\n",
      "epoch: 10, batch loss: 713.5199228922526\n",
      "epoch: 11, batch loss: 687.4869893391927\n",
      "epoch: 12, batch loss: 655.3589884440104\n",
      "epoch: 13, batch loss: 610.5549875895182\n",
      "epoch: 14, batch loss: 561.2552541097006\n",
      "epoch: 15, batch loss: 503.1259485880534\n",
      "epoch: 16, batch loss: 442.3145395914714\n",
      "epoch: 17, batch loss: 373.7921905517578\n",
      "epoch: 18, batch loss: 313.7608757019043\n",
      "epoch: 19, batch loss: 256.4417470296224\n",
      "epoch: 20, batch loss: 206.5497080485026\n",
      "epoch: 21, batch loss: 161.2580935160319\n",
      "epoch: 22, batch loss: 132.22376124064127\n",
      "epoch: 23, batch loss: 104.98803265889485\n",
      "epoch: 24, batch loss: 88.27462069193523\n",
      "epoch: 25, batch loss: 75.73152351379395\n",
      "epoch: 26, batch loss: 65.27393531799316\n",
      "epoch: 27, batch loss: 60.257603327433266\n",
      "epoch: 28, batch loss: 56.88927014668783\n",
      "epoch: 29, batch loss: 53.96733570098877\n",
      "epoch: 30, batch loss: 53.33937486012777\n",
      "epoch: 31, batch loss: 51.37824535369873\n",
      "epoch: 32, batch loss: 49.17983309427897\n",
      "epoch: 33, batch loss: 48.594850858052574\n",
      "epoch: 34, batch loss: 47.77396011352539\n",
      "epoch: 35, batch loss: 47.481255849202476\n",
      "epoch: 36, batch loss: 45.572157859802246\n",
      "epoch: 37, batch loss: 46.64457289377848\n",
      "epoch: 38, batch loss: 46.72969500223795\n",
      "epoch: 39, batch loss: 43.67815891901652\n",
      "epoch: 40, batch loss: 43.48943487803141\n",
      "epoch: 41, batch loss: 44.49154615402222\n",
      "epoch: 42, batch loss: 42.85816733042399\n",
      "epoch: 43, batch loss: 41.99296760559082\n",
      "epoch: 44, batch loss: 42.68451118469238\n",
      "epoch: 45, batch loss: 42.876124699910484\n",
      "epoch: 46, batch loss: 41.54634141921997\n",
      "epoch: 47, batch loss: 41.56565634409586\n",
      "epoch: 48, batch loss: 41.09092871348063\n",
      "epoch: 49, batch loss: 42.33460760116577\n",
      "epoch: 50, batch loss: 43.71073945363363\n",
      "epoch: 51, batch loss: 41.05494499206543\n",
      "epoch: 52, batch loss: 40.79125356674194\n",
      "epoch: 53, batch loss: 39.30076519648234\n",
      "epoch: 54, batch loss: 38.94202995300293\n",
      "epoch: 55, batch loss: 38.63810189565023\n",
      "epoch: 56, batch loss: 38.16639550526937\n",
      "epoch: 57, batch loss: 38.808984915415444\n",
      "epoch: 58, batch loss: 38.765716552734375\n",
      "epoch: 59, batch loss: 37.86888869603475\n",
      "epoch: 60, batch loss: 37.385077476501465\n",
      "epoch: 61, batch loss: 37.811841328938804\n",
      "epoch: 62, batch loss: 36.99791431427002\n",
      "epoch: 63, batch loss: 39.170821030934654\n",
      "epoch: 64, batch loss: 36.134109338124595\n",
      "epoch: 65, batch loss: 38.28766520818075\n",
      "epoch: 66, batch loss: 36.29553492863973\n",
      "epoch: 67, batch loss: 37.0465833346049\n",
      "epoch: 68, batch loss: 35.165682315826416\n",
      "epoch: 69, batch loss: 34.955325285593666\n",
      "epoch: 70, batch loss: 35.71353896458944\n",
      "epoch: 71, batch loss: 35.872024059295654\n",
      "epoch: 72, batch loss: 34.659428437550865\n",
      "epoch: 73, batch loss: 35.501766204833984\n",
      "epoch: 74, batch loss: 34.025590578715004\n",
      "epoch: 75, batch loss: 34.64087247848511\n",
      "epoch: 76, batch loss: 33.63816245396932\n",
      "epoch: 77, batch loss: 34.915854930877686\n",
      "epoch: 78, batch loss: 34.31317933400472\n",
      "epoch: 79, batch loss: 33.80260388056437\n",
      "epoch: 80, batch loss: 33.225186347961426\n",
      "epoch: 81, batch loss: 33.970837434132896\n",
      "epoch: 82, batch loss: 33.467386404673256\n",
      "epoch: 83, batch loss: 32.872791608174644\n",
      "epoch: 84, batch loss: 33.203248818715416\n",
      "epoch: 85, batch loss: 32.910052140553795\n",
      "epoch: 86, batch loss: 31.591790676116943\n",
      "epoch: 87, batch loss: 32.973626931508385\n",
      "epoch: 88, batch loss: 32.25528001785278\n",
      "epoch: 89, batch loss: 31.315630594889324\n",
      "epoch: 90, batch loss: 31.23423957824707\n",
      "epoch: 91, batch loss: 32.499996185302734\n",
      "epoch: 92, batch loss: 31.041377862294514\n",
      "epoch: 93, batch loss: 31.310337861378986\n",
      "epoch: 94, batch loss: 30.58779287338257\n",
      "epoch: 95, batch loss: 30.504850069681805\n",
      "epoch: 96, batch loss: 30.97382402420044\n",
      "epoch: 97, batch loss: 30.10269037882487\n",
      "epoch: 98, batch loss: 30.171830813090008\n",
      "epoch: 99, batch loss: 30.336681365966797\n",
      "epoch: 100, batch loss: 30.640852451324463\n",
      "epoch: 101, batch loss: 29.8002765973409\n",
      "epoch: 102, batch loss: 29.825369040171307\n",
      "epoch: 103, batch loss: 28.975472927093506\n",
      "epoch: 104, batch loss: 29.11889139811198\n",
      "epoch: 105, batch loss: 29.747056007385254\n",
      "epoch: 106, batch loss: 28.4971129099528\n",
      "epoch: 107, batch loss: 27.986891587575276\n",
      "epoch: 108, batch loss: 29.193710803985596\n",
      "epoch: 109, batch loss: 27.733095169067383\n",
      "epoch: 110, batch loss: 28.102391084035236\n",
      "epoch: 111, batch loss: 29.30025005340576\n",
      "epoch: 112, batch loss: 27.551774978637695\n",
      "epoch: 113, batch loss: 27.79107729593913\n",
      "epoch: 114, batch loss: 27.55490255355835\n",
      "epoch: 115, batch loss: 26.989564736684162\n",
      "epoch: 116, batch loss: 26.948291301727295\n",
      "epoch: 117, batch loss: 26.461262226104736\n",
      "epoch: 118, batch loss: 26.526249408721924\n",
      "epoch: 119, batch loss: 26.556940237681072\n",
      "epoch: 120, batch loss: 27.55092414220174\n",
      "epoch: 121, batch loss: 27.283578395843506\n",
      "epoch: 122, batch loss: 26.255953311920166\n",
      "epoch: 123, batch loss: 26.64360507329305\n",
      "epoch: 124, batch loss: 27.28828477859497\n",
      "epoch: 125, batch loss: 26.216569423675537\n",
      "epoch: 126, batch loss: 25.867007891337078\n",
      "epoch: 127, batch loss: 26.858426411946613\n",
      "epoch: 128, batch loss: 25.83735195795695\n",
      "epoch: 129, batch loss: 25.786099116007488\n",
      "epoch: 130, batch loss: 25.95027732849121\n",
      "epoch: 131, batch loss: 26.967069307963055\n",
      "epoch: 132, batch loss: 25.62663221359253\n",
      "epoch: 133, batch loss: 25.496275583902996\n",
      "epoch: 134, batch loss: 24.717859029769897\n",
      "epoch: 135, batch loss: 24.63533854484558\n",
      "epoch: 136, batch loss: 25.9460236231486\n",
      "epoch: 137, batch loss: 25.261213461558025\n",
      "epoch: 138, batch loss: 25.3907577196757\n",
      "epoch: 139, batch loss: 25.27748187383016\n",
      "epoch: 140, batch loss: 24.869717280069988\n",
      "epoch: 141, batch loss: 24.736376126607258\n",
      "epoch: 142, batch loss: 25.01788838704427\n",
      "epoch: 143, batch loss: 24.40353266398112\n",
      "epoch: 144, batch loss: 25.071459929148357\n",
      "epoch: 145, batch loss: 24.550877888997395\n",
      "epoch: 146, batch loss: 24.09992488225301\n",
      "epoch: 147, batch loss: 24.59243170420329\n",
      "epoch: 148, batch loss: 24.03016948699951\n",
      "epoch: 149, batch loss: 24.731557846069336\n",
      "epoch: 150, batch loss: 24.1834290822347\n",
      "epoch: 151, batch loss: 25.201018969217937\n",
      "epoch: 152, batch loss: 24.045385996500652\n",
      "epoch: 153, batch loss: 23.471431493759155\n",
      "epoch: 154, batch loss: 23.662583192189533\n",
      "epoch: 155, batch loss: 23.045852025349934\n",
      "epoch: 156, batch loss: 24.038909594217937\n",
      "epoch: 157, batch loss: 23.983673731486004\n",
      "epoch: 158, batch loss: 23.517106533050537\n",
      "epoch: 159, batch loss: 23.758503834406536\n",
      "epoch: 160, batch loss: 23.9774276415507\n",
      "epoch: 161, batch loss: 23.11040385564168\n",
      "epoch: 162, batch loss: 23.230590502421062\n",
      "epoch: 163, batch loss: 23.376434167226154\n",
      "epoch: 164, batch loss: 23.5527081489563\n",
      "epoch: 165, batch loss: 23.563445409138996\n",
      "epoch: 166, batch loss: 23.01603603363037\n",
      "epoch: 167, batch loss: 23.155084292093914\n",
      "epoch: 168, batch loss: 23.233441670735676\n",
      "epoch: 169, batch loss: 22.64490230878194\n",
      "epoch: 170, batch loss: 22.678881009419758\n",
      "epoch: 171, batch loss: 22.71492139498393\n",
      "epoch: 172, batch loss: 23.03832467397054\n",
      "epoch: 173, batch loss: 23.5342378616333\n",
      "epoch: 174, batch loss: 23.554541269938152\n",
      "epoch: 175, batch loss: 23.067437012990315\n",
      "epoch: 176, batch loss: 22.645652850468952\n",
      "epoch: 177, batch loss: 23.896459023157757\n",
      "epoch: 178, batch loss: 22.72314453125\n",
      "epoch: 179, batch loss: 22.80961497624715\n",
      "epoch: 180, batch loss: 23.568777481714886\n",
      "epoch: 181, batch loss: 22.792972564697266\n",
      "epoch: 182, batch loss: 23.10693089167277\n",
      "epoch: 183, batch loss: 22.456031322479248\n",
      "epoch: 184, batch loss: 22.564730167388916\n",
      "epoch: 185, batch loss: 23.23003355662028\n",
      "epoch: 186, batch loss: 22.49685271581014\n",
      "epoch: 187, batch loss: 22.624064922332764\n",
      "epoch: 188, batch loss: 22.597706476847332\n",
      "epoch: 189, batch loss: 21.894917170206707\n",
      "epoch: 190, batch loss: 22.563769658406574\n",
      "epoch: 191, batch loss: 22.02074996630351\n",
      "epoch: 192, batch loss: 22.42502983411153\n",
      "epoch: 193, batch loss: 22.30561876296997\n",
      "epoch: 194, batch loss: 22.32229709625244\n",
      "epoch: 195, batch loss: 22.094779014587402\n",
      "epoch: 196, batch loss: 22.066565195719402\n",
      "epoch: 197, batch loss: 22.106544812520344\n",
      "epoch: 198, batch loss: 22.02097686131795\n",
      "epoch: 199, batch loss: 21.900433937708538\n",
      "epoch: 200, batch loss: 22.050580342610676\n",
      "epoch: 201, batch loss: 22.102059841156006\n",
      "epoch: 202, batch loss: 23.648483753204346\n",
      "epoch: 203, batch loss: 21.900010983149212\n",
      "epoch: 204, batch loss: 22.207661628723145\n",
      "epoch: 205, batch loss: 21.75199015935262\n",
      "epoch: 206, batch loss: 21.907132466634113\n",
      "epoch: 207, batch loss: 22.292240460713703\n",
      "epoch: 208, batch loss: 22.353955268859863\n",
      "epoch: 209, batch loss: 22.21126874287923\n",
      "epoch: 210, batch loss: 22.634949207305908\n",
      "epoch: 211, batch loss: 21.507190386454266\n",
      "epoch: 212, batch loss: 21.90648603439331\n",
      "epoch: 213, batch loss: 21.6629060904185\n",
      "epoch: 214, batch loss: 21.3350031375885\n",
      "epoch: 215, batch loss: 22.139148871103924\n",
      "epoch: 216, batch loss: 21.339494705200195\n",
      "epoch: 217, batch loss: 22.2814621925354\n",
      "epoch: 218, batch loss: 21.85475715001424\n",
      "epoch: 219, batch loss: 22.353275934855144\n",
      "epoch: 220, batch loss: 21.386113325754803\n",
      "epoch: 221, batch loss: 21.61901330947876\n",
      "epoch: 222, batch loss: 21.66756296157837\n",
      "epoch: 223, batch loss: 21.542476177215576\n",
      "epoch: 224, batch loss: 22.117305278778076\n",
      "epoch: 225, batch loss: 21.839511076609295\n",
      "epoch: 226, batch loss: 21.582844575246174\n",
      "epoch: 227, batch loss: 21.988781134287517\n",
      "epoch: 228, batch loss: 22.061761379241943\n",
      "epoch: 229, batch loss: 21.23989208539327\n",
      "epoch: 230, batch loss: 22.02623414993286\n",
      "epoch: 231, batch loss: 21.260268131891888\n",
      "epoch: 232, batch loss: 21.878674189249676\n",
      "epoch: 233, batch loss: 21.196253935496014\n",
      "epoch: 234, batch loss: 21.369401772816975\n",
      "epoch: 235, batch loss: 21.167045434315998\n",
      "epoch: 236, batch loss: 22.240894953409832\n",
      "epoch: 237, batch loss: 21.97364815076192\n",
      "epoch: 238, batch loss: 21.134640057881672\n",
      "epoch: 239, batch loss: 21.048167785008747\n",
      "epoch: 240, batch loss: 21.272650559743244\n",
      "epoch: 241, batch loss: 21.58231274286906\n",
      "epoch: 242, batch loss: 21.940888325373333\n",
      "epoch: 243, batch loss: 21.979559580485027\n",
      "epoch: 244, batch loss: 21.118454933166504\n",
      "epoch: 245, batch loss: 21.08849048614502\n",
      "epoch: 246, batch loss: 21.577290296554565\n",
      "epoch: 247, batch loss: 21.107831875483196\n",
      "epoch: 248, batch loss: 21.014864683151245\n",
      "epoch: 249, batch loss: 21.285587390263874\n",
      "epoch: 250, batch loss: 20.887277762095135\n",
      "epoch: 251, batch loss: 21.144379138946533\n",
      "epoch: 252, batch loss: 22.20656983057658\n",
      "epoch: 253, batch loss: 21.004734992980957\n",
      "epoch: 254, batch loss: 21.37988082567851\n",
      "epoch: 255, batch loss: 20.75195376078288\n",
      "epoch: 256, batch loss: 21.39296825726827\n",
      "epoch: 257, batch loss: 21.452241738637287\n",
      "epoch: 258, batch loss: 20.93865696589152\n",
      "epoch: 259, batch loss: 20.94049859046936\n",
      "epoch: 260, batch loss: 22.06356716156006\n",
      "epoch: 261, batch loss: 22.34614586830139\n",
      "epoch: 262, batch loss: 20.63696050643921\n",
      "epoch: 263, batch loss: 20.736151456832886\n",
      "epoch: 264, batch loss: 20.857446034749348\n",
      "epoch: 265, batch loss: 20.70060634613037\n",
      "epoch: 266, batch loss: 20.805794318517048\n",
      "epoch: 267, batch loss: 20.70839023590088\n",
      "epoch: 268, batch loss: 21.09446668624878\n",
      "epoch: 269, batch loss: 20.730944633483887\n",
      "epoch: 270, batch loss: 20.86551896731059\n",
      "epoch: 271, batch loss: 21.08723219235738\n",
      "epoch: 272, batch loss: 21.117194255193073\n",
      "epoch: 273, batch loss: 21.376784404118855\n",
      "epoch: 274, batch loss: 20.884424368540447\n",
      "epoch: 275, batch loss: 21.096056938171387\n",
      "epoch: 276, batch loss: 20.824519793192547\n",
      "epoch: 277, batch loss: 20.761096159617107\n",
      "epoch: 278, batch loss: 20.96045748392741\n",
      "epoch: 279, batch loss: 20.399221261342365\n",
      "epoch: 280, batch loss: 21.863622029622395\n",
      "epoch: 281, batch loss: 20.62498148282369\n",
      "epoch: 282, batch loss: 20.668322483698528\n",
      "epoch: 283, batch loss: 20.74337911605835\n",
      "epoch: 284, batch loss: 20.932891925175984\n",
      "epoch: 285, batch loss: 20.707677125930786\n",
      "epoch: 286, batch loss: 21.085656960805256\n",
      "epoch: 287, batch loss: 21.320173581441242\n",
      "epoch: 288, batch loss: 20.67003933588664\n",
      "epoch: 289, batch loss: 21.14560063680013\n",
      "epoch: 290, batch loss: 20.64081382751465\n",
      "epoch: 291, batch loss: 20.69295597076416\n",
      "epoch: 292, batch loss: 20.975989023844402\n",
      "epoch: 293, batch loss: 21.133830547332764\n",
      "epoch: 294, batch loss: 21.182194471359253\n",
      "epoch: 295, batch loss: 20.56520438194275\n",
      "epoch: 296, batch loss: 20.46475354830424\n",
      "epoch: 297, batch loss: 20.26206676165263\n",
      "epoch: 298, batch loss: 21.85450021425883\n",
      "epoch: 299, batch loss: 20.93949333826701\n",
      "epoch: 300, batch loss: 20.593701759974163\n",
      "epoch: 1, batch loss: 788.0555674235026\n",
      "epoch: 2, batch loss: 784.7166035970052\n",
      "epoch: 3, batch loss: 785.5992075602213\n",
      "epoch: 4, batch loss: 785.4013214111328\n",
      "epoch: 5, batch loss: 771.8631744384766\n",
      "epoch: 6, batch loss: 760.3454081217448\n",
      "epoch: 7, batch loss: 761.7153269449869\n",
      "epoch: 8, batch loss: 741.9749501546224\n",
      "epoch: 9, batch loss: 730.2129923502604\n",
      "epoch: 10, batch loss: 702.7683359781901\n",
      "epoch: 11, batch loss: 670.6080780029297\n",
      "epoch: 12, batch loss: 633.8273061116537\n",
      "epoch: 13, batch loss: 592.2642974853516\n",
      "epoch: 14, batch loss: 529.0631052652994\n",
      "epoch: 15, batch loss: 460.6633071899414\n",
      "epoch: 16, batch loss: 391.88894907633465\n",
      "epoch: 17, batch loss: 323.09593836466473\n",
      "epoch: 18, batch loss: 256.770694732666\n",
      "epoch: 19, batch loss: 208.00136057535806\n",
      "epoch: 20, batch loss: 166.62531344095865\n",
      "epoch: 21, batch loss: 132.3976510365804\n",
      "epoch: 22, batch loss: 115.72278340657552\n",
      "epoch: 23, batch loss: 104.11662864685059\n",
      "epoch: 24, batch loss: 94.6461861928304\n",
      "epoch: 25, batch loss: 82.08441162109375\n",
      "epoch: 26, batch loss: 74.73452981313069\n",
      "epoch: 27, batch loss: 69.23335520426433\n",
      "epoch: 28, batch loss: 68.81859556833903\n",
      "epoch: 29, batch loss: 62.80909442901611\n",
      "epoch: 30, batch loss: 61.13344637552897\n",
      "epoch: 31, batch loss: 59.23610782623291\n",
      "epoch: 32, batch loss: 59.35870933532715\n",
      "epoch: 33, batch loss: 55.46576182047526\n",
      "epoch: 34, batch loss: 53.51095740000407\n",
      "epoch: 35, batch loss: 52.89818127950033\n",
      "epoch: 36, batch loss: 51.00403372446696\n",
      "epoch: 37, batch loss: 50.270996729532875\n",
      "epoch: 38, batch loss: 49.5156135559082\n",
      "epoch: 39, batch loss: 49.613308588663735\n",
      "epoch: 40, batch loss: 46.994654973347984\n",
      "epoch: 41, batch loss: 47.58238093058268\n",
      "epoch: 42, batch loss: 46.77463706334432\n",
      "epoch: 43, batch loss: 46.0175666809082\n",
      "epoch: 44, batch loss: 45.722546418507896\n",
      "epoch: 45, batch loss: 45.411858240763344\n",
      "epoch: 46, batch loss: 43.6324192682902\n",
      "epoch: 47, batch loss: 44.43981250127157\n",
      "epoch: 48, batch loss: 43.645326932271324\n",
      "epoch: 49, batch loss: 44.718936602274574\n",
      "epoch: 50, batch loss: 42.42855644226074\n",
      "epoch: 51, batch loss: 40.78611914316813\n",
      "epoch: 52, batch loss: 40.01694933573405\n",
      "epoch: 53, batch loss: 39.503200689951576\n",
      "epoch: 54, batch loss: 40.58357779184977\n",
      "epoch: 55, batch loss: 39.59150187174479\n",
      "epoch: 56, batch loss: 39.75356594721476\n",
      "epoch: 57, batch loss: 37.55325269699097\n",
      "epoch: 58, batch loss: 38.78642225265503\n",
      "epoch: 59, batch loss: 39.31335179011027\n",
      "epoch: 60, batch loss: 37.40506092707316\n",
      "epoch: 61, batch loss: 36.34783490498861\n",
      "epoch: 62, batch loss: 36.66063372294108\n",
      "epoch: 63, batch loss: 36.29765605926514\n",
      "epoch: 64, batch loss: 35.79883734385172\n",
      "epoch: 65, batch loss: 36.08852084477743\n",
      "epoch: 66, batch loss: 35.60914961496989\n",
      "epoch: 67, batch loss: 36.484304110209145\n",
      "epoch: 68, batch loss: 34.36733945210775\n",
      "epoch: 69, batch loss: 35.126603285471596\n",
      "epoch: 70, batch loss: 34.13247092564901\n",
      "epoch: 71, batch loss: 35.06575155258179\n",
      "epoch: 72, batch loss: 33.33853801091512\n",
      "epoch: 73, batch loss: 33.39013957977295\n",
      "epoch: 74, batch loss: 32.6625862121582\n",
      "epoch: 75, batch loss: 32.335596561431885\n",
      "epoch: 76, batch loss: 31.787224610646565\n",
      "epoch: 77, batch loss: 32.02515967686971\n",
      "epoch: 78, batch loss: 32.5201891263326\n",
      "epoch: 79, batch loss: 30.98898474375407\n",
      "epoch: 80, batch loss: 31.584408283233643\n",
      "epoch: 81, batch loss: 31.183472792307537\n",
      "epoch: 82, batch loss: 30.666743278503418\n",
      "epoch: 83, batch loss: 30.574089527130127\n",
      "epoch: 84, batch loss: 30.31614049275716\n",
      "epoch: 85, batch loss: 32.02153968811035\n",
      "epoch: 86, batch loss: 30.692107677459717\n",
      "epoch: 87, batch loss: 30.470090707143147\n",
      "epoch: 88, batch loss: 30.11043389638265\n",
      "epoch: 89, batch loss: 30.172127723693848\n",
      "epoch: 90, batch loss: 29.328460534413654\n",
      "epoch: 91, batch loss: 29.662343978881836\n",
      "epoch: 92, batch loss: 29.307023366292317\n",
      "epoch: 93, batch loss: 29.22096331914266\n",
      "epoch: 94, batch loss: 28.24985345204671\n",
      "epoch: 95, batch loss: 28.59214226404826\n",
      "epoch: 96, batch loss: 27.96772559483846\n",
      "epoch: 97, batch loss: 27.902179718017578\n",
      "epoch: 98, batch loss: 28.462038199106853\n",
      "epoch: 99, batch loss: 28.32860787709554\n",
      "epoch: 100, batch loss: 27.94650411605835\n",
      "epoch: 101, batch loss: 28.058327515920002\n",
      "epoch: 102, batch loss: 28.35136620203654\n",
      "epoch: 103, batch loss: 27.374032338460285\n",
      "epoch: 104, batch loss: 27.25504207611084\n",
      "epoch: 105, batch loss: 26.7588693300883\n",
      "epoch: 106, batch loss: 26.897355715433758\n",
      "epoch: 107, batch loss: 27.896939595540363\n",
      "epoch: 108, batch loss: 26.61705271402995\n",
      "epoch: 109, batch loss: 26.873170852661133\n",
      "epoch: 110, batch loss: 27.239096800486248\n",
      "epoch: 111, batch loss: 27.29793341954549\n",
      "epoch: 112, batch loss: 26.60398546854655\n",
      "epoch: 113, batch loss: 27.104073524475098\n",
      "epoch: 114, batch loss: 28.526472250620525\n",
      "epoch: 115, batch loss: 25.990881125132244\n",
      "epoch: 116, batch loss: 26.122385025024414\n",
      "epoch: 117, batch loss: 26.256866296132404\n",
      "epoch: 118, batch loss: 26.12758159637451\n",
      "epoch: 119, batch loss: 25.565878868103027\n",
      "epoch: 120, batch loss: 25.57070016860962\n",
      "epoch: 121, batch loss: 25.336641311645508\n",
      "epoch: 122, batch loss: 26.155407746632893\n",
      "epoch: 123, batch loss: 25.70936107635498\n",
      "epoch: 124, batch loss: 25.992852846781414\n",
      "epoch: 125, batch loss: 24.952282190322876\n",
      "epoch: 126, batch loss: 25.85200595855713\n",
      "epoch: 127, batch loss: 25.2804692586263\n",
      "epoch: 128, batch loss: 25.277394771575928\n",
      "epoch: 129, batch loss: 24.85968526204427\n",
      "epoch: 130, batch loss: 24.612539927164715\n",
      "epoch: 131, batch loss: 25.15252431233724\n",
      "epoch: 132, batch loss: 25.00135040283203\n",
      "epoch: 133, batch loss: 25.033886909484863\n",
      "epoch: 134, batch loss: 24.259870847066242\n",
      "epoch: 135, batch loss: 24.932414372762043\n",
      "epoch: 136, batch loss: 25.00589195887248\n",
      "epoch: 137, batch loss: 24.50609763463338\n",
      "epoch: 138, batch loss: 24.04962221781413\n",
      "epoch: 139, batch loss: 24.631787459055584\n",
      "epoch: 140, batch loss: 24.56490643819173\n",
      "epoch: 141, batch loss: 24.558752695719402\n",
      "epoch: 142, batch loss: 24.20741335550944\n",
      "epoch: 143, batch loss: 25.716588815053303\n",
      "epoch: 144, batch loss: 24.163675785064697\n",
      "epoch: 145, batch loss: 24.966758728027344\n",
      "epoch: 146, batch loss: 24.50576416651408\n",
      "epoch: 147, batch loss: 24.273877064387005\n",
      "epoch: 148, batch loss: 23.509218613306682\n",
      "epoch: 149, batch loss: 24.18306064605713\n",
      "epoch: 150, batch loss: 24.15691375732422\n",
      "epoch: 151, batch loss: 23.68175395329793\n",
      "epoch: 152, batch loss: 23.644690831502277\n",
      "epoch: 153, batch loss: 24.578637917836506\n",
      "epoch: 154, batch loss: 24.81630229949951\n",
      "epoch: 155, batch loss: 23.81272792816162\n",
      "epoch: 156, batch loss: 23.003718773523968\n",
      "epoch: 157, batch loss: 24.94214145342509\n",
      "epoch: 158, batch loss: 23.86802037556966\n",
      "epoch: 159, batch loss: 23.96856466929118\n",
      "epoch: 160, batch loss: 23.053431193033855\n",
      "epoch: 161, batch loss: 23.851508776346844\n",
      "epoch: 162, batch loss: 23.34610366821289\n",
      "epoch: 163, batch loss: 23.278880437215168\n",
      "epoch: 164, batch loss: 23.11198886235555\n",
      "epoch: 165, batch loss: 22.844186782836914\n",
      "epoch: 166, batch loss: 23.912447452545166\n",
      "epoch: 167, batch loss: 23.662391980489094\n",
      "epoch: 168, batch loss: 22.83528184890747\n",
      "epoch: 169, batch loss: 23.342951854070026\n",
      "epoch: 170, batch loss: 23.50287977854411\n",
      "epoch: 171, batch loss: 23.372023582458496\n",
      "epoch: 172, batch loss: 23.42752504348755\n",
      "epoch: 173, batch loss: 22.86388635635376\n",
      "epoch: 174, batch loss: 23.08653497695923\n",
      "epoch: 175, batch loss: 23.570413907368977\n",
      "epoch: 176, batch loss: 22.388530254364014\n",
      "epoch: 177, batch loss: 22.909018675486248\n",
      "epoch: 178, batch loss: 22.87592538197835\n",
      "epoch: 179, batch loss: 22.981999397277832\n",
      "epoch: 180, batch loss: 23.049912293752033\n",
      "epoch: 181, batch loss: 22.83743953704834\n",
      "epoch: 182, batch loss: 22.500876744588215\n",
      "epoch: 183, batch loss: 22.347001632054646\n",
      "epoch: 184, batch loss: 23.00199604034424\n",
      "epoch: 185, batch loss: 22.119713226954143\n",
      "epoch: 186, batch loss: 23.117706775665283\n",
      "epoch: 187, batch loss: 22.116900205612183\n",
      "epoch: 188, batch loss: 22.28415560722351\n",
      "epoch: 189, batch loss: 22.356901327768963\n",
      "epoch: 190, batch loss: 22.648571809132893\n",
      "epoch: 191, batch loss: 22.20135490099589\n",
      "epoch: 192, batch loss: 23.0285906791687\n",
      "epoch: 193, batch loss: 22.209672609965008\n",
      "epoch: 194, batch loss: 22.580451011657715\n",
      "epoch: 195, batch loss: 22.714940388997395\n",
      "epoch: 196, batch loss: 22.929178714752197\n",
      "epoch: 197, batch loss: 22.44452889760335\n",
      "epoch: 198, batch loss: 21.959914922714233\n",
      "epoch: 199, batch loss: 21.833427270253498\n",
      "epoch: 200, batch loss: 22.101946989695232\n",
      "epoch: 201, batch loss: 21.810436010360718\n",
      "epoch: 202, batch loss: 22.106707255045574\n",
      "epoch: 203, batch loss: 22.34869686762492\n",
      "epoch: 204, batch loss: 22.271468957265217\n",
      "epoch: 205, batch loss: 22.392614364624023\n",
      "epoch: 206, batch loss: 21.34860102335612\n",
      "epoch: 207, batch loss: 22.147656122843426\n",
      "epoch: 208, batch loss: 22.29364800453186\n",
      "epoch: 209, batch loss: 22.981130599975586\n",
      "epoch: 210, batch loss: 21.766896565755207\n",
      "epoch: 211, batch loss: 22.85564724604289\n",
      "epoch: 212, batch loss: 21.67179576555888\n",
      "epoch: 213, batch loss: 21.451162656148274\n",
      "epoch: 214, batch loss: 22.42523431777954\n",
      "epoch: 215, batch loss: 21.56792465845744\n",
      "epoch: 216, batch loss: 22.029074986775715\n",
      "epoch: 217, batch loss: 22.236277898152668\n",
      "epoch: 218, batch loss: 21.514336903889973\n",
      "epoch: 219, batch loss: 21.713961283365887\n",
      "epoch: 220, batch loss: 21.430464267730713\n",
      "epoch: 221, batch loss: 22.168833255767822\n",
      "epoch: 222, batch loss: 22.083744208017986\n",
      "epoch: 223, batch loss: 22.172061761220295\n",
      "epoch: 224, batch loss: 21.57895835240682\n",
      "epoch: 225, batch loss: 22.178804715474445\n",
      "epoch: 226, batch loss: 21.49865420659383\n",
      "epoch: 227, batch loss: 21.449867407480877\n",
      "epoch: 228, batch loss: 21.45005210240682\n",
      "epoch: 229, batch loss: 21.67244561513265\n",
      "epoch: 230, batch loss: 21.090569575627644\n",
      "epoch: 231, batch loss: 21.592169761657715\n",
      "epoch: 232, batch loss: 21.67525593439738\n",
      "epoch: 233, batch loss: 22.441665331522625\n",
      "epoch: 234, batch loss: 21.43917425473531\n",
      "epoch: 235, batch loss: 21.130391756693523\n",
      "epoch: 236, batch loss: 21.518635590871174\n",
      "epoch: 237, batch loss: 21.399535655975342\n",
      "epoch: 238, batch loss: 21.935062726338703\n",
      "epoch: 239, batch loss: 21.29343303044637\n",
      "epoch: 240, batch loss: 21.711699803670246\n",
      "epoch: 241, batch loss: 21.482409954071045\n",
      "epoch: 242, batch loss: 22.373246908187866\n",
      "epoch: 243, batch loss: 22.009404182434082\n",
      "epoch: 244, batch loss: 21.477650960286457\n",
      "epoch: 245, batch loss: 21.707273721694946\n",
      "epoch: 246, batch loss: 21.312134583791096\n",
      "epoch: 247, batch loss: 21.342756748199463\n",
      "epoch: 248, batch loss: 20.95055866241455\n",
      "epoch: 249, batch loss: 21.056579033533733\n",
      "epoch: 250, batch loss: 21.451776107152302\n",
      "epoch: 251, batch loss: 21.600371678670246\n",
      "epoch: 252, batch loss: 21.34118413925171\n",
      "epoch: 253, batch loss: 21.03859829902649\n",
      "epoch: 254, batch loss: 21.438711802164715\n",
      "epoch: 255, batch loss: 21.251956065495808\n",
      "epoch: 256, batch loss: 21.261531352996826\n",
      "epoch: 257, batch loss: 22.144081513086956\n",
      "epoch: 258, batch loss: 21.511259873708088\n",
      "epoch: 259, batch loss: 21.512950261433918\n",
      "epoch: 260, batch loss: 21.556470155715942\n",
      "epoch: 261, batch loss: 21.778241475423176\n",
      "epoch: 262, batch loss: 21.028326590855915\n",
      "epoch: 263, batch loss: 23.486147085825603\n",
      "epoch: 264, batch loss: 21.14039119084676\n",
      "epoch: 265, batch loss: 21.80198876063029\n",
      "epoch: 266, batch loss: 21.834286212921143\n",
      "epoch: 267, batch loss: 21.167020320892334\n",
      "epoch: 268, batch loss: 21.205973784128826\n",
      "epoch: 269, batch loss: 20.75436814626058\n",
      "epoch: 270, batch loss: 22.644180138905842\n",
      "epoch: 271, batch loss: 21.26704676946004\n",
      "epoch: 272, batch loss: 20.90014187494914\n",
      "epoch: 273, batch loss: 21.46127478281657\n",
      "epoch: 274, batch loss: 21.020992120107014\n",
      "epoch: 275, batch loss: 21.69282563527425\n",
      "epoch: 276, batch loss: 20.843498865763348\n",
      "epoch: 277, batch loss: 21.855696121851604\n",
      "epoch: 278, batch loss: 20.81330410639445\n",
      "epoch: 279, batch loss: 21.52894075711568\n",
      "epoch: 280, batch loss: 20.526525894800823\n",
      "epoch: 281, batch loss: 20.713173389434814\n",
      "epoch: 282, batch loss: 20.953181505203247\n",
      "epoch: 283, batch loss: 21.286530176798504\n",
      "epoch: 284, batch loss: 21.42998393376668\n",
      "epoch: 285, batch loss: 21.172335942586262\n",
      "epoch: 286, batch loss: 20.579038619995117\n",
      "epoch: 287, batch loss: 20.848783175150555\n",
      "epoch: 288, batch loss: 20.915255228678387\n",
      "epoch: 289, batch loss: 21.115603526433308\n",
      "epoch: 290, batch loss: 20.647317965825398\n",
      "epoch: 291, batch loss: 20.70214255650838\n",
      "epoch: 292, batch loss: 20.721038579940796\n",
      "epoch: 293, batch loss: 20.35042142868042\n",
      "epoch: 294, batch loss: 20.58377679189046\n",
      "epoch: 295, batch loss: 21.643398761749268\n",
      "epoch: 296, batch loss: 20.838860670725506\n",
      "epoch: 297, batch loss: 20.400290648142498\n",
      "epoch: 298, batch loss: 20.790894508361816\n",
      "epoch: 299, batch loss: 21.26551898320516\n",
      "epoch: 300, batch loss: 21.441195170084637\n",
      "epoch: 1, batch loss: 790.2891540527344\n",
      "epoch: 2, batch loss: 786.0444793701172\n",
      "epoch: 3, batch loss: 787.1546579996744\n",
      "epoch: 4, batch loss: 777.2295379638672\n",
      "epoch: 5, batch loss: 763.4047902425131\n",
      "epoch: 6, batch loss: 754.9290059407552\n",
      "epoch: 7, batch loss: 742.2246907552084\n",
      "epoch: 8, batch loss: 728.2235005696615\n",
      "epoch: 9, batch loss: 705.5135345458984\n",
      "epoch: 10, batch loss: 677.3243509928385\n",
      "epoch: 11, batch loss: 641.5684407552084\n",
      "epoch: 12, batch loss: 600.7310485839844\n",
      "epoch: 13, batch loss: 552.0800577799479\n",
      "epoch: 14, batch loss: 495.82274119059247\n",
      "epoch: 15, batch loss: 428.0362040201823\n",
      "epoch: 16, batch loss: 363.7832514444987\n",
      "epoch: 17, batch loss: 302.9638608296712\n",
      "epoch: 18, batch loss: 248.7110252380371\n",
      "epoch: 19, batch loss: 203.09595108032227\n",
      "epoch: 20, batch loss: 161.34549776713052\n",
      "epoch: 21, batch loss: 135.54500198364258\n",
      "epoch: 22, batch loss: 111.16190973917644\n",
      "epoch: 23, batch loss: 91.40359020233154\n",
      "epoch: 24, batch loss: 78.18074766794841\n",
      "epoch: 25, batch loss: 68.92122300465901\n",
      "epoch: 26, batch loss: 62.10988680521647\n",
      "epoch: 27, batch loss: 58.307750384012856\n",
      "epoch: 28, batch loss: 53.76343504587809\n",
      "epoch: 29, batch loss: 50.736391385396324\n",
      "epoch: 30, batch loss: 50.13762537638346\n",
      "epoch: 31, batch loss: 49.16850105921427\n",
      "epoch: 32, batch loss: 46.13706318537394\n",
      "epoch: 33, batch loss: 46.185374895731606\n",
      "epoch: 34, batch loss: 45.14662059148153\n",
      "epoch: 35, batch loss: 43.8070863087972\n",
      "epoch: 36, batch loss: 43.39189704259237\n",
      "epoch: 37, batch loss: 41.11706701914469\n",
      "epoch: 38, batch loss: 40.982296784718834\n",
      "epoch: 39, batch loss: 41.226770083109535\n",
      "epoch: 40, batch loss: 40.606417973836265\n",
      "epoch: 41, batch loss: 39.540084997812905\n",
      "epoch: 42, batch loss: 39.312607288360596\n",
      "epoch: 43, batch loss: 38.50180514653524\n",
      "epoch: 44, batch loss: 41.12879991531372\n",
      "epoch: 45, batch loss: 38.335034211476646\n",
      "epoch: 46, batch loss: 39.08500226338705\n",
      "epoch: 47, batch loss: 38.912562211354576\n",
      "epoch: 48, batch loss: 38.15922546386719\n",
      "epoch: 49, batch loss: 37.76033544540405\n",
      "epoch: 50, batch loss: 36.96821101506551\n",
      "epoch: 51, batch loss: 35.314948081970215\n",
      "epoch: 52, batch loss: 35.228033224741615\n",
      "epoch: 53, batch loss: 35.894885222117104\n",
      "epoch: 54, batch loss: 34.80454476674398\n",
      "epoch: 55, batch loss: 34.16406408945719\n",
      "epoch: 56, batch loss: 34.16792996724447\n",
      "epoch: 57, batch loss: 33.334579149881996\n",
      "epoch: 58, batch loss: 34.791998545328774\n",
      "epoch: 59, batch loss: 33.16321484247843\n",
      "epoch: 60, batch loss: 33.57529830932617\n",
      "epoch: 61, batch loss: 32.931612491607666\n",
      "epoch: 62, batch loss: 33.07504367828369\n",
      "epoch: 63, batch loss: 31.346484820048016\n",
      "epoch: 64, batch loss: 31.410945892333984\n",
      "epoch: 65, batch loss: 31.85619004567464\n",
      "epoch: 66, batch loss: 32.26639906565348\n",
      "epoch: 67, batch loss: 31.415623982747395\n",
      "epoch: 68, batch loss: 30.87824583053589\n",
      "epoch: 69, batch loss: 31.855580012003582\n",
      "epoch: 70, batch loss: 31.357830842336018\n",
      "epoch: 71, batch loss: 31.462039788564045\n",
      "epoch: 72, batch loss: 30.062140146891277\n",
      "epoch: 73, batch loss: 31.418163299560547\n",
      "epoch: 74, batch loss: 29.551339467366535\n",
      "epoch: 75, batch loss: 29.56124258041382\n",
      "epoch: 76, batch loss: 30.05801296234131\n",
      "epoch: 77, batch loss: 29.63396120071411\n",
      "epoch: 78, batch loss: 30.092706044514973\n",
      "epoch: 79, batch loss: 28.977230072021484\n",
      "epoch: 80, batch loss: 29.915821234385174\n",
      "epoch: 81, batch loss: 28.5326665242513\n",
      "epoch: 82, batch loss: 28.611953099568684\n",
      "epoch: 83, batch loss: 28.418880462646484\n",
      "epoch: 84, batch loss: 29.455779711405437\n",
      "epoch: 85, batch loss: 29.16686757405599\n",
      "epoch: 86, batch loss: 28.884822527567547\n",
      "epoch: 87, batch loss: 27.517176230748493\n",
      "epoch: 88, batch loss: 27.790841738382976\n",
      "epoch: 89, batch loss: 27.426010767618816\n",
      "epoch: 90, batch loss: 28.469881375630695\n",
      "epoch: 91, batch loss: 27.190510590871174\n",
      "epoch: 92, batch loss: 27.0486634572347\n",
      "epoch: 93, batch loss: 27.3035470644633\n",
      "epoch: 94, batch loss: 26.822851022084553\n",
      "epoch: 95, batch loss: 27.881880442301433\n",
      "epoch: 96, batch loss: 26.822713534037273\n",
      "epoch: 97, batch loss: 27.17155869801839\n",
      "epoch: 98, batch loss: 27.18916877110799\n",
      "epoch: 99, batch loss: 26.549843788146973\n",
      "epoch: 100, batch loss: 27.208558559417725\n",
      "epoch: 101, batch loss: 26.330660184224445\n",
      "epoch: 102, batch loss: 26.44190565745036\n",
      "epoch: 103, batch loss: 26.692352294921875\n",
      "epoch: 104, batch loss: 26.15505091349284\n",
      "epoch: 105, batch loss: 26.342540899912517\n",
      "epoch: 106, batch loss: 26.39654318491618\n",
      "epoch: 107, batch loss: 27.228763103485107\n",
      "epoch: 108, batch loss: 27.271173159281414\n",
      "epoch: 109, batch loss: 25.79551108678182\n",
      "epoch: 110, batch loss: 26.251168092091877\n",
      "epoch: 111, batch loss: 25.5923007329305\n",
      "epoch: 112, batch loss: 25.439524173736572\n",
      "epoch: 113, batch loss: 25.240032990773518\n",
      "epoch: 114, batch loss: 25.83276430765788\n",
      "epoch: 115, batch loss: 25.1463885307312\n",
      "epoch: 116, batch loss: 26.933168411254883\n",
      "epoch: 117, batch loss: 25.25513521830241\n",
      "epoch: 118, batch loss: 25.12976837158203\n",
      "epoch: 119, batch loss: 26.137623469034832\n",
      "epoch: 120, batch loss: 26.286986509958904\n",
      "epoch: 121, batch loss: 25.140435059865315\n",
      "epoch: 122, batch loss: 25.12386131286621\n",
      "epoch: 123, batch loss: 24.733212153116863\n",
      "epoch: 124, batch loss: 24.94733254114787\n",
      "epoch: 125, batch loss: 25.138883749643963\n",
      "epoch: 126, batch loss: 25.017669677734375\n",
      "epoch: 127, batch loss: 25.881988366444904\n",
      "epoch: 128, batch loss: 24.652963161468506\n",
      "epoch: 129, batch loss: 24.663883368174236\n",
      "epoch: 130, batch loss: 24.109015941619873\n",
      "epoch: 131, batch loss: 24.221367994944256\n",
      "epoch: 132, batch loss: 24.19846534729004\n",
      "epoch: 133, batch loss: 24.944406191507976\n",
      "epoch: 134, batch loss: 25.20784107844035\n",
      "epoch: 135, batch loss: 24.08198833465576\n",
      "epoch: 136, batch loss: 24.199114004770916\n",
      "epoch: 137, batch loss: 24.581376393636067\n",
      "epoch: 138, batch loss: 23.937972863515217\n",
      "epoch: 139, batch loss: 24.174336115519207\n",
      "epoch: 140, batch loss: 24.425621191660564\n",
      "epoch: 141, batch loss: 24.27585967381795\n",
      "epoch: 142, batch loss: 25.19183365503947\n",
      "epoch: 143, batch loss: 24.908066908518474\n",
      "epoch: 144, batch loss: 23.485356330871582\n",
      "epoch: 145, batch loss: 23.579299370447796\n",
      "epoch: 146, batch loss: 23.06928237279256\n",
      "epoch: 147, batch loss: 24.01073916753133\n",
      "epoch: 148, batch loss: 24.150668462117512\n",
      "epoch: 149, batch loss: 23.529837767283123\n",
      "epoch: 150, batch loss: 23.453325510025024\n",
      "epoch: 151, batch loss: 23.508164087931316\n",
      "epoch: 152, batch loss: 23.319286823272705\n",
      "epoch: 153, batch loss: 23.12235673268636\n",
      "epoch: 154, batch loss: 23.661883115768433\n",
      "epoch: 155, batch loss: 23.649308999379475\n",
      "epoch: 156, batch loss: 22.951558430989582\n",
      "epoch: 157, batch loss: 23.09947919845581\n",
      "epoch: 158, batch loss: 24.082163016001385\n",
      "epoch: 159, batch loss: 23.395922501881916\n",
      "epoch: 160, batch loss: 23.033963203430176\n",
      "epoch: 161, batch loss: 23.003745714823406\n",
      "epoch: 162, batch loss: 24.91749159495036\n",
      "epoch: 163, batch loss: 23.491164366404217\n",
      "epoch: 164, batch loss: 23.303232351938885\n",
      "epoch: 165, batch loss: 23.08722798029582\n",
      "epoch: 166, batch loss: 22.955416202545166\n",
      "epoch: 167, batch loss: 23.943889300028484\n",
      "epoch: 168, batch loss: 23.889494736989338\n",
      "epoch: 169, batch loss: 23.308669090270996\n",
      "epoch: 170, batch loss: 23.273580074310303\n",
      "epoch: 171, batch loss: 23.78097979227702\n",
      "epoch: 172, batch loss: 22.729265689849854\n",
      "epoch: 173, batch loss: 22.447045008341473\n",
      "epoch: 174, batch loss: 22.768190066019695\n",
      "epoch: 175, batch loss: 23.343191146850586\n",
      "epoch: 176, batch loss: 22.89652903874715\n",
      "epoch: 177, batch loss: 23.0054833094279\n",
      "epoch: 178, batch loss: 23.1142676671346\n",
      "epoch: 179, batch loss: 22.86226447423299\n",
      "epoch: 180, batch loss: 22.728606780370075\n",
      "epoch: 181, batch loss: 22.915771007537842\n",
      "epoch: 182, batch loss: 22.605693181355793\n",
      "epoch: 183, batch loss: 23.02138344446818\n",
      "epoch: 184, batch loss: 23.11936314900716\n",
      "epoch: 185, batch loss: 22.701354106267292\n",
      "epoch: 186, batch loss: 23.21884822845459\n",
      "epoch: 187, batch loss: 23.01245053609212\n",
      "epoch: 188, batch loss: 22.261683464050293\n",
      "epoch: 189, batch loss: 22.47511641184489\n",
      "epoch: 190, batch loss: 23.28962294260661\n",
      "epoch: 191, batch loss: 22.769869804382324\n",
      "epoch: 192, batch loss: 21.879834254582722\n",
      "epoch: 193, batch loss: 23.06463082631429\n",
      "epoch: 194, batch loss: 22.46734658877055\n",
      "epoch: 195, batch loss: 22.936572710673016\n",
      "epoch: 196, batch loss: 22.013350248336792\n",
      "epoch: 197, batch loss: 22.088130950927734\n",
      "epoch: 198, batch loss: 22.64201323191325\n",
      "epoch: 199, batch loss: 22.17976140975952\n",
      "epoch: 200, batch loss: 22.209599018096924\n",
      "epoch: 201, batch loss: 22.51554028193156\n",
      "epoch: 202, batch loss: 22.345541954040527\n",
      "epoch: 203, batch loss: 21.949986298878986\n",
      "epoch: 204, batch loss: 22.11545991897583\n",
      "epoch: 205, batch loss: 22.120937665303547\n",
      "epoch: 206, batch loss: 22.13477357228597\n",
      "epoch: 207, batch loss: 22.238867441813152\n",
      "epoch: 208, batch loss: 23.209140459696453\n",
      "epoch: 209, batch loss: 22.75114981333415\n",
      "epoch: 210, batch loss: 21.831648031870525\n",
      "epoch: 211, batch loss: 22.676456451416016\n",
      "epoch: 212, batch loss: 22.044851620992024\n",
      "epoch: 213, batch loss: 21.628222306569416\n",
      "epoch: 214, batch loss: 22.276450792948406\n",
      "epoch: 215, batch loss: 21.67759084701538\n",
      "epoch: 216, batch loss: 21.6271710395813\n",
      "epoch: 217, batch loss: 22.063907146453857\n",
      "epoch: 218, batch loss: 21.986584107081097\n",
      "epoch: 219, batch loss: 22.169720967610676\n",
      "epoch: 220, batch loss: 21.455686569213867\n",
      "epoch: 221, batch loss: 22.16919183731079\n",
      "epoch: 222, batch loss: 21.767263412475586\n",
      "epoch: 223, batch loss: 21.856615861256916\n",
      "epoch: 224, batch loss: 21.48253297805786\n",
      "epoch: 225, batch loss: 21.698965231577557\n",
      "epoch: 226, batch loss: 22.62348445256551\n",
      "epoch: 227, batch loss: 21.702072620391846\n",
      "epoch: 228, batch loss: 22.09232457478841\n",
      "epoch: 229, batch loss: 21.426793177922566\n",
      "epoch: 230, batch loss: 22.221827507019043\n",
      "epoch: 231, batch loss: 21.807019869486492\n",
      "epoch: 232, batch loss: 22.08974266052246\n",
      "epoch: 233, batch loss: 21.875319639841717\n",
      "epoch: 234, batch loss: 22.599631627400715\n",
      "epoch: 235, batch loss: 22.337480068206787\n",
      "epoch: 236, batch loss: 21.609206199645996\n",
      "epoch: 237, batch loss: 21.21681483586629\n",
      "epoch: 238, batch loss: 21.82065773010254\n",
      "epoch: 239, batch loss: 21.511402289072674\n",
      "epoch: 240, batch loss: 21.924133618672688\n",
      "epoch: 241, batch loss: 22.08174737294515\n",
      "epoch: 242, batch loss: 22.01584808031718\n",
      "epoch: 243, batch loss: 21.616448720296223\n",
      "epoch: 244, batch loss: 21.405213276545208\n",
      "epoch: 245, batch loss: 21.320907433827717\n",
      "epoch: 246, batch loss: 20.882012844085693\n",
      "epoch: 247, batch loss: 21.4877667427063\n",
      "epoch: 248, batch loss: 21.621913989384968\n",
      "epoch: 249, batch loss: 22.540722688039143\n",
      "epoch: 250, batch loss: 21.825738350550335\n",
      "epoch: 251, batch loss: 21.98584493001302\n",
      "epoch: 252, batch loss: 21.795564730962116\n",
      "epoch: 253, batch loss: 21.51453097661336\n",
      "epoch: 254, batch loss: 21.71039581298828\n",
      "epoch: 255, batch loss: 21.663156032562256\n",
      "epoch: 256, batch loss: 20.876826127370197\n",
      "epoch: 257, batch loss: 22.488813241322834\n",
      "epoch: 258, batch loss: 21.38855465253194\n",
      "epoch: 259, batch loss: 21.605287551879883\n",
      "epoch: 260, batch loss: 20.844000260035198\n",
      "epoch: 261, batch loss: 21.800455570220947\n",
      "epoch: 262, batch loss: 21.566283067067463\n",
      "epoch: 263, batch loss: 21.673200607299805\n",
      "epoch: 264, batch loss: 21.984416723251343\n",
      "epoch: 265, batch loss: 22.02132749557495\n",
      "epoch: 266, batch loss: 20.990332921346027\n",
      "epoch: 267, batch loss: 21.69488525390625\n",
      "epoch: 268, batch loss: 21.226829449335735\n",
      "epoch: 269, batch loss: 20.882917404174805\n",
      "epoch: 270, batch loss: 21.17241891225179\n",
      "epoch: 271, batch loss: 21.02747130393982\n",
      "epoch: 272, batch loss: 20.95356559753418\n",
      "epoch: 273, batch loss: 21.31723141670227\n",
      "epoch: 274, batch loss: 20.858012437820435\n",
      "epoch: 275, batch loss: 21.04127009709676\n",
      "epoch: 276, batch loss: 21.65815234184265\n",
      "epoch: 277, batch loss: 20.76155996322632\n",
      "epoch: 278, batch loss: 21.014734586079914\n",
      "epoch: 279, batch loss: 20.72361906369527\n",
      "epoch: 280, batch loss: 21.2409930229187\n",
      "epoch: 281, batch loss: 20.641386032104492\n",
      "epoch: 282, batch loss: 20.818230470021565\n",
      "epoch: 283, batch loss: 21.108305136362713\n",
      "epoch: 284, batch loss: 21.61149326960246\n",
      "epoch: 285, batch loss: 20.97478175163269\n",
      "epoch: 286, batch loss: 20.684021313985188\n",
      "epoch: 287, batch loss: 21.382389624913532\n",
      "epoch: 288, batch loss: 20.68180815378825\n",
      "epoch: 289, batch loss: 20.492713928222656\n",
      "epoch: 290, batch loss: 20.686044295628864\n",
      "epoch: 291, batch loss: 20.58519760767619\n",
      "epoch: 292, batch loss: 21.60945240656535\n",
      "epoch: 293, batch loss: 20.626551787058514\n",
      "epoch: 294, batch loss: 21.024845759073894\n",
      "epoch: 295, batch loss: 21.33986234664917\n",
      "epoch: 296, batch loss: 20.499793608983357\n",
      "epoch: 297, batch loss: 20.985200087229412\n",
      "epoch: 298, batch loss: 21.49837636947632\n",
      "epoch: 299, batch loss: 21.852509021759033\n",
      "epoch: 300, batch loss: 21.13963524500529\n",
      "epoch: 1, batch loss: 788.3149159749349\n",
      "epoch: 2, batch loss: 784.8096516927084\n",
      "epoch: 3, batch loss: 777.7678476969401\n",
      "epoch: 4, batch loss: 773.7618916829427\n",
      "epoch: 5, batch loss: 771.1554056803385\n",
      "epoch: 6, batch loss: 764.398427327474\n",
      "epoch: 7, batch loss: 758.0757242838541\n",
      "epoch: 8, batch loss: 734.4182535807291\n",
      "epoch: 9, batch loss: 718.1661834716797\n",
      "epoch: 10, batch loss: 696.9470774332682\n",
      "epoch: 11, batch loss: 665.9485677083334\n",
      "epoch: 12, batch loss: 625.6800689697266\n",
      "epoch: 13, batch loss: 589.7306925455729\n",
      "epoch: 14, batch loss: 538.5440673828125\n",
      "epoch: 15, batch loss: 476.25200907389325\n",
      "epoch: 16, batch loss: 416.49059804280597\n",
      "epoch: 17, batch loss: 346.03505452473956\n",
      "epoch: 18, batch loss: 282.3684768676758\n",
      "epoch: 19, batch loss: 225.09707514444986\n",
      "epoch: 20, batch loss: 180.99879201253256\n",
      "epoch: 21, batch loss: 141.30698458353677\n",
      "epoch: 22, batch loss: 112.53826077779134\n",
      "epoch: 23, batch loss: 89.97139135996501\n",
      "epoch: 24, batch loss: 74.17489655812581\n",
      "epoch: 25, batch loss: 65.98232142130534\n",
      "epoch: 26, batch loss: 58.891249656677246\n",
      "epoch: 27, batch loss: 54.790151278177895\n",
      "epoch: 28, batch loss: 53.33468087514242\n",
      "epoch: 29, batch loss: 50.20422776540121\n",
      "epoch: 30, batch loss: 49.03890450795492\n",
      "epoch: 31, batch loss: 45.8928796450297\n",
      "epoch: 32, batch loss: 46.05750306447347\n",
      "epoch: 33, batch loss: 45.11047871907552\n",
      "epoch: 34, batch loss: 43.619935035705566\n",
      "epoch: 35, batch loss: 43.917589823404946\n",
      "epoch: 36, batch loss: 43.91667207082113\n",
      "epoch: 37, batch loss: 42.60315450032552\n",
      "epoch: 38, batch loss: 42.11634063720703\n",
      "epoch: 39, batch loss: 41.548890113830566\n",
      "epoch: 40, batch loss: 41.365236600240074\n",
      "epoch: 41, batch loss: 41.29374885559082\n",
      "epoch: 42, batch loss: 39.54408121109009\n",
      "epoch: 43, batch loss: 39.926793575286865\n",
      "epoch: 44, batch loss: 37.96588961283366\n",
      "epoch: 45, batch loss: 38.40479056040446\n",
      "epoch: 46, batch loss: 38.71006043752035\n",
      "epoch: 47, batch loss: 38.02552191416422\n",
      "epoch: 48, batch loss: 37.589807192484535\n",
      "epoch: 49, batch loss: 36.76709874471029\n",
      "epoch: 50, batch loss: 37.1533834139506\n",
      "epoch: 51, batch loss: 36.550431410471596\n",
      "epoch: 52, batch loss: 37.40839417775472\n",
      "epoch: 53, batch loss: 35.38188568751017\n",
      "epoch: 54, batch loss: 35.650111039479576\n",
      "epoch: 55, batch loss: 34.262248833974205\n",
      "epoch: 56, batch loss: 34.82825390497843\n",
      "epoch: 57, batch loss: 35.13747898737589\n",
      "epoch: 58, batch loss: 34.131244818369545\n",
      "epoch: 59, batch loss: 34.15025838216146\n",
      "epoch: 60, batch loss: 32.958393255869545\n",
      "epoch: 61, batch loss: 33.376176834106445\n",
      "epoch: 62, batch loss: 33.35574324925741\n",
      "epoch: 63, batch loss: 32.912333170572914\n",
      "epoch: 64, batch loss: 33.147786140441895\n",
      "epoch: 65, batch loss: 33.32957967122396\n",
      "epoch: 66, batch loss: 33.78749831517538\n",
      "epoch: 67, batch loss: 32.850550492604576\n",
      "epoch: 68, batch loss: 31.994996070861816\n",
      "epoch: 69, batch loss: 31.267780780792236\n",
      "epoch: 70, batch loss: 31.496888478597004\n",
      "epoch: 71, batch loss: 30.826919396718342\n",
      "epoch: 72, batch loss: 30.73237482706706\n",
      "epoch: 73, batch loss: 30.826955954233807\n",
      "epoch: 74, batch loss: 30.10934813817342\n",
      "epoch: 75, batch loss: 30.152283509572346\n",
      "epoch: 76, batch loss: 29.989702860514324\n",
      "epoch: 77, batch loss: 30.004686673482258\n",
      "epoch: 78, batch loss: 29.407796223958332\n",
      "epoch: 79, batch loss: 29.55710204442342\n",
      "epoch: 80, batch loss: 30.15199677149455\n",
      "epoch: 81, batch loss: 29.62810754776001\n",
      "epoch: 82, batch loss: 28.539046843846638\n",
      "epoch: 83, batch loss: 28.668005625406902\n",
      "epoch: 84, batch loss: 28.7068084081014\n",
      "epoch: 85, batch loss: 28.428361892700195\n",
      "epoch: 86, batch loss: 28.35233990351359\n",
      "epoch: 87, batch loss: 28.34266185760498\n",
      "epoch: 88, batch loss: 29.103201548258465\n",
      "epoch: 89, batch loss: 27.88174851735433\n",
      "epoch: 90, batch loss: 28.680397987365723\n",
      "epoch: 91, batch loss: 27.802619457244873\n",
      "epoch: 92, batch loss: 27.808522701263428\n",
      "epoch: 93, batch loss: 29.24979082743327\n",
      "epoch: 94, batch loss: 27.382429599761963\n",
      "epoch: 95, batch loss: 27.488003889719646\n",
      "epoch: 96, batch loss: 26.88591957092285\n",
      "epoch: 97, batch loss: 27.56187915802002\n",
      "epoch: 98, batch loss: 28.35281753540039\n",
      "epoch: 99, batch loss: 27.324085076649983\n",
      "epoch: 100, batch loss: 27.64523220062256\n",
      "epoch: 101, batch loss: 27.08783547083537\n",
      "epoch: 102, batch loss: 26.378026326497395\n",
      "epoch: 103, batch loss: 26.802353064219158\n",
      "epoch: 104, batch loss: 26.74107599258423\n",
      "epoch: 105, batch loss: 25.83130606015523\n",
      "epoch: 106, batch loss: 26.520591100056965\n",
      "epoch: 107, batch loss: 26.819672266642254\n",
      "epoch: 108, batch loss: 26.582414786020916\n",
      "epoch: 109, batch loss: 26.260698795318604\n",
      "epoch: 110, batch loss: 26.23545789718628\n",
      "epoch: 111, batch loss: 26.62060006459554\n",
      "epoch: 112, batch loss: 25.97976891199748\n",
      "epoch: 113, batch loss: 26.35377899805705\n",
      "epoch: 114, batch loss: 25.742115179697674\n",
      "epoch: 115, batch loss: 25.61666266123454\n",
      "epoch: 116, batch loss: 25.121249675750732\n",
      "epoch: 117, batch loss: 25.50867207845052\n",
      "epoch: 118, batch loss: 26.0298589070638\n",
      "epoch: 119, batch loss: 25.266074975331623\n",
      "epoch: 120, batch loss: 24.49359353383382\n",
      "epoch: 121, batch loss: 25.680113792419434\n",
      "epoch: 122, batch loss: 24.990648587544758\n",
      "epoch: 123, batch loss: 25.386949221293133\n",
      "epoch: 124, batch loss: 24.686843713124592\n",
      "epoch: 125, batch loss: 24.875168323516846\n",
      "epoch: 126, batch loss: 24.98200575510661\n",
      "epoch: 127, batch loss: 25.388133843739826\n",
      "epoch: 128, batch loss: 24.31750504175822\n",
      "epoch: 129, batch loss: 25.256905873616535\n",
      "epoch: 130, batch loss: 24.035259167353313\n",
      "epoch: 131, batch loss: 24.844971815745037\n",
      "epoch: 132, batch loss: 24.419936180114746\n",
      "epoch: 133, batch loss: 25.010087649027508\n",
      "epoch: 134, batch loss: 25.7516827583313\n",
      "epoch: 135, batch loss: 24.773334821065266\n",
      "epoch: 136, batch loss: 24.16763432820638\n",
      "epoch: 137, batch loss: 24.96536048253377\n",
      "epoch: 138, batch loss: 24.404901663462322\n",
      "epoch: 139, batch loss: 24.260499954223633\n",
      "epoch: 140, batch loss: 24.86822970708211\n",
      "epoch: 141, batch loss: 24.857721487681072\n",
      "epoch: 142, batch loss: 25.31903298695882\n",
      "epoch: 143, batch loss: 24.283422629038494\n",
      "epoch: 144, batch loss: 23.7835791905721\n",
      "epoch: 145, batch loss: 23.67858600616455\n",
      "epoch: 146, batch loss: 24.450241883595783\n",
      "epoch: 147, batch loss: 23.955604473749798\n",
      "epoch: 148, batch loss: 23.64823865890503\n",
      "epoch: 149, batch loss: 24.36979866027832\n",
      "epoch: 150, batch loss: 23.874361832936604\n",
      "epoch: 151, batch loss: 23.636526743570965\n",
      "epoch: 152, batch loss: 23.63990084330241\n",
      "epoch: 153, batch loss: 23.86536169052124\n",
      "epoch: 154, batch loss: 23.185940742492676\n",
      "epoch: 155, batch loss: 23.112937529881794\n",
      "epoch: 156, batch loss: 23.85745032628377\n",
      "epoch: 157, batch loss: 23.143930912017822\n",
      "epoch: 158, batch loss: 23.712459087371826\n",
      "epoch: 159, batch loss: 23.280370076497395\n",
      "epoch: 160, batch loss: 23.15194050470988\n",
      "epoch: 161, batch loss: 22.78453000386556\n",
      "epoch: 162, batch loss: 23.195278644561768\n",
      "epoch: 163, batch loss: 23.93360201517741\n",
      "epoch: 164, batch loss: 23.93071635564168\n",
      "epoch: 165, batch loss: 23.051769892374676\n",
      "epoch: 166, batch loss: 23.26093928019206\n",
      "epoch: 167, batch loss: 22.9523827234904\n",
      "epoch: 168, batch loss: 22.86048396428426\n",
      "epoch: 169, batch loss: 22.459269762039185\n",
      "epoch: 170, batch loss: 22.89030408859253\n",
      "epoch: 171, batch loss: 25.492302258809406\n",
      "epoch: 172, batch loss: 22.967465082804363\n",
      "epoch: 173, batch loss: 23.78222894668579\n",
      "epoch: 174, batch loss: 23.042776425679524\n",
      "epoch: 175, batch loss: 23.40055235226949\n",
      "epoch: 176, batch loss: 22.807414531707764\n",
      "epoch: 177, batch loss: 22.893360137939453\n",
      "epoch: 178, batch loss: 24.33432976404826\n",
      "epoch: 179, batch loss: 23.608088811238606\n",
      "epoch: 180, batch loss: 22.30893087387085\n",
      "epoch: 181, batch loss: 22.457074960072834\n",
      "epoch: 182, batch loss: 22.507956822713215\n",
      "epoch: 183, batch loss: 22.959142605463665\n",
      "epoch: 184, batch loss: 22.440682411193848\n",
      "epoch: 185, batch loss: 23.510760466257732\n",
      "epoch: 186, batch loss: 23.169175306955974\n",
      "epoch: 187, batch loss: 22.00391411781311\n",
      "epoch: 188, batch loss: 22.951404571533203\n",
      "epoch: 189, batch loss: 22.82823117574056\n",
      "epoch: 190, batch loss: 22.17254114151001\n",
      "epoch: 191, batch loss: 23.060922463734943\n",
      "epoch: 192, batch loss: 22.821399370829266\n",
      "epoch: 193, batch loss: 23.550658226013184\n",
      "epoch: 194, batch loss: 23.413734277089436\n",
      "epoch: 195, batch loss: 22.58153994878133\n",
      "epoch: 196, batch loss: 22.814608891805012\n",
      "epoch: 197, batch loss: 22.212178389231365\n",
      "epoch: 198, batch loss: 22.025845686594646\n",
      "epoch: 199, batch loss: 22.1164763768514\n",
      "epoch: 200, batch loss: 22.07202895482381\n",
      "epoch: 201, batch loss: 21.795126914978027\n",
      "epoch: 202, batch loss: 21.98551368713379\n",
      "epoch: 203, batch loss: 21.767229000727337\n",
      "epoch: 204, batch loss: 22.512731075286865\n",
      "epoch: 205, batch loss: 22.365676561991375\n",
      "epoch: 206, batch loss: 22.13657784461975\n",
      "epoch: 207, batch loss: 22.281640847524006\n",
      "epoch: 208, batch loss: 21.630184014638264\n",
      "epoch: 209, batch loss: 22.45546547571818\n",
      "epoch: 210, batch loss: 22.353004455566406\n",
      "epoch: 211, batch loss: 21.912726879119873\n",
      "epoch: 212, batch loss: 22.321492195129395\n",
      "epoch: 213, batch loss: 22.96158194541931\n",
      "epoch: 214, batch loss: 22.549498955408733\n",
      "epoch: 215, batch loss: 22.944748401641846\n",
      "epoch: 216, batch loss: 21.43544348080953\n",
      "epoch: 217, batch loss: 21.17876895268758\n",
      "epoch: 218, batch loss: 21.63833522796631\n",
      "epoch: 219, batch loss: 21.283477862675984\n",
      "epoch: 220, batch loss: 22.255665461222332\n",
      "epoch: 221, batch loss: 22.066020488739014\n",
      "epoch: 222, batch loss: 21.058820803960163\n",
      "epoch: 223, batch loss: 21.167808532714844\n",
      "epoch: 224, batch loss: 22.0912082195282\n",
      "epoch: 225, batch loss: 21.37389389673869\n",
      "epoch: 226, batch loss: 21.640509287516277\n",
      "epoch: 227, batch loss: 21.428531646728516\n",
      "epoch: 228, batch loss: 21.101900498072307\n",
      "epoch: 229, batch loss: 21.469170411427815\n",
      "epoch: 230, batch loss: 21.49961558977763\n",
      "epoch: 231, batch loss: 21.25335733095805\n",
      "epoch: 232, batch loss: 22.772138277689617\n",
      "epoch: 233, batch loss: 22.16266743342082\n",
      "epoch: 234, batch loss: 21.480242411295574\n",
      "epoch: 235, batch loss: 21.40698178609212\n",
      "epoch: 236, batch loss: 21.823506673177082\n",
      "epoch: 237, batch loss: 21.234125932057697\n",
      "epoch: 238, batch loss: 21.404534180959065\n",
      "epoch: 239, batch loss: 21.067678689956665\n",
      "epoch: 240, batch loss: 21.28124698003133\n",
      "epoch: 241, batch loss: 21.614224274953205\n",
      "epoch: 242, batch loss: 21.576823552449543\n",
      "epoch: 243, batch loss: 21.6173357963562\n",
      "epoch: 244, batch loss: 21.632551193237305\n",
      "epoch: 245, batch loss: 21.14420159657796\n",
      "epoch: 246, batch loss: 20.860413153966267\n",
      "epoch: 247, batch loss: 21.196702480316162\n",
      "epoch: 248, batch loss: 21.37338201204936\n",
      "epoch: 249, batch loss: 21.175761858622234\n",
      "epoch: 250, batch loss: 21.5139004389445\n",
      "epoch: 251, batch loss: 21.051361719767254\n",
      "epoch: 252, batch loss: 21.174457550048828\n",
      "epoch: 253, batch loss: 21.08904258410136\n",
      "epoch: 254, batch loss: 21.282543500264484\n",
      "epoch: 255, batch loss: 21.07899284362793\n",
      "epoch: 256, batch loss: 22.526407798131306\n",
      "epoch: 257, batch loss: 21.515149275461834\n",
      "epoch: 258, batch loss: 21.252528429031372\n",
      "epoch: 259, batch loss: 20.757641712824505\n",
      "epoch: 260, batch loss: 22.041288375854492\n",
      "epoch: 261, batch loss: 21.190904299418133\n",
      "epoch: 262, batch loss: 21.623240391413372\n",
      "epoch: 263, batch loss: 21.34490442276001\n",
      "epoch: 264, batch loss: 21.236441373825073\n",
      "epoch: 265, batch loss: 21.59197457631429\n",
      "epoch: 266, batch loss: 21.07252248128255\n",
      "epoch: 267, batch loss: 23.13394037882487\n",
      "epoch: 268, batch loss: 20.881810506184895\n",
      "epoch: 269, batch loss: 20.70723009109497\n",
      "epoch: 270, batch loss: 21.460795640945435\n",
      "epoch: 271, batch loss: 20.771870056788128\n",
      "epoch: 272, batch loss: 20.655768473943073\n",
      "epoch: 273, batch loss: 21.360340277353924\n",
      "epoch: 274, batch loss: 20.762557983398438\n",
      "epoch: 275, batch loss: 21.700101216634113\n",
      "epoch: 276, batch loss: 22.348775068918865\n",
      "epoch: 277, batch loss: 20.52787685394287\n",
      "epoch: 278, batch loss: 22.052673021952312\n",
      "epoch: 279, batch loss: 21.110403537750244\n",
      "epoch: 280, batch loss: 21.823043664296467\n",
      "epoch: 281, batch loss: 20.759114901224773\n",
      "epoch: 282, batch loss: 20.563050667444866\n",
      "epoch: 283, batch loss: 21.100693146387737\n",
      "epoch: 284, batch loss: 20.529678265253704\n",
      "epoch: 285, batch loss: 20.995638132095337\n",
      "epoch: 286, batch loss: 20.76246722539266\n",
      "epoch: 287, batch loss: 20.917797247568767\n",
      "epoch: 288, batch loss: 20.90324894587199\n",
      "epoch: 289, batch loss: 20.520928541819256\n",
      "epoch: 290, batch loss: 20.457712570826214\n",
      "epoch: 291, batch loss: 21.237499634424847\n",
      "epoch: 292, batch loss: 20.774466037750244\n",
      "epoch: 293, batch loss: 20.458531538645428\n",
      "epoch: 294, batch loss: 20.614280382792156\n",
      "epoch: 295, batch loss: 20.678958654403687\n",
      "epoch: 296, batch loss: 20.854904254277546\n",
      "epoch: 297, batch loss: 21.138935963312786\n",
      "epoch: 298, batch loss: 21.039853811264038\n",
      "epoch: 299, batch loss: 21.600969950358074\n",
      "epoch: 300, batch loss: 20.748689889907837\n",
      "epoch: 1, batch loss: 791.9594268798828\n",
      "epoch: 2, batch loss: 789.3787282307943\n",
      "epoch: 3, batch loss: 782.2838897705078\n",
      "epoch: 4, batch loss: 781.7695465087891\n",
      "epoch: 5, batch loss: 775.0783996582031\n",
      "epoch: 6, batch loss: 768.8283894856771\n",
      "epoch: 7, batch loss: 754.4323933919271\n",
      "epoch: 8, batch loss: 743.0680999755859\n",
      "epoch: 9, batch loss: 727.8211517333984\n",
      "epoch: 10, batch loss: 716.6548156738281\n",
      "epoch: 11, batch loss: 683.9410552978516\n",
      "epoch: 12, batch loss: 649.8566233317057\n",
      "epoch: 13, batch loss: 612.7200419108073\n",
      "epoch: 14, batch loss: 568.9561920166016\n",
      "epoch: 15, batch loss: 504.9284032185872\n",
      "epoch: 16, batch loss: 445.2758382161458\n",
      "epoch: 17, batch loss: 382.66080474853516\n",
      "epoch: 18, batch loss: 314.32091522216797\n",
      "epoch: 19, batch loss: 255.83577728271484\n",
      "epoch: 20, batch loss: 205.41061147054037\n",
      "epoch: 21, batch loss: 170.0706024169922\n",
      "epoch: 22, batch loss: 139.5439135233561\n",
      "epoch: 23, batch loss: 118.01561164855957\n",
      "epoch: 24, batch loss: 98.4565912882487\n",
      "epoch: 25, batch loss: 87.25473912556966\n",
      "epoch: 26, batch loss: 80.82825088500977\n",
      "epoch: 27, batch loss: 66.47875181833903\n",
      "epoch: 28, batch loss: 63.38906701405843\n",
      "epoch: 29, batch loss: 59.61390813191732\n",
      "epoch: 30, batch loss: 53.60235786437988\n",
      "epoch: 31, batch loss: 50.61380863189697\n",
      "epoch: 32, batch loss: 49.10911846160889\n",
      "epoch: 33, batch loss: 47.919864654541016\n",
      "epoch: 34, batch loss: 46.02758757273356\n",
      "epoch: 35, batch loss: 45.173120180765785\n",
      "epoch: 36, batch loss: 44.5194517771403\n",
      "epoch: 37, batch loss: 42.62290604909261\n",
      "epoch: 38, batch loss: 44.24797614415487\n",
      "epoch: 39, batch loss: 41.74476877848307\n",
      "epoch: 40, batch loss: 41.10277779897054\n",
      "epoch: 41, batch loss: 40.814662297566734\n",
      "epoch: 42, batch loss: 40.10605732599894\n",
      "epoch: 43, batch loss: 39.9696691830953\n",
      "epoch: 44, batch loss: 39.44213533401489\n",
      "epoch: 45, batch loss: 39.8024705251058\n",
      "epoch: 46, batch loss: 39.72971757253011\n",
      "epoch: 47, batch loss: 37.98568948109945\n",
      "epoch: 48, batch loss: 37.8640030225118\n",
      "epoch: 49, batch loss: 37.822728951772056\n",
      "epoch: 50, batch loss: 37.63167460759481\n",
      "epoch: 51, batch loss: 36.33292865753174\n",
      "epoch: 52, batch loss: 36.45195531845093\n",
      "epoch: 53, batch loss: 38.45436557133993\n",
      "epoch: 54, batch loss: 36.30626885096232\n",
      "epoch: 55, batch loss: 37.73051357269287\n",
      "epoch: 56, batch loss: 36.82318369547526\n",
      "epoch: 57, batch loss: 35.183593114217125\n",
      "epoch: 58, batch loss: 35.56407721837362\n",
      "epoch: 59, batch loss: 35.069012800852455\n",
      "epoch: 60, batch loss: 34.21069447199503\n",
      "epoch: 61, batch loss: 34.149847984313965\n",
      "epoch: 62, batch loss: 33.376070181528725\n",
      "epoch: 63, batch loss: 33.14797401428223\n",
      "epoch: 64, batch loss: 33.349581241607666\n",
      "epoch: 65, batch loss: 32.41133721669515\n",
      "epoch: 66, batch loss: 32.479386965433754\n",
      "epoch: 67, batch loss: 35.00984811782837\n",
      "epoch: 68, batch loss: 32.31463511784872\n",
      "epoch: 69, batch loss: 31.490546544392902\n",
      "epoch: 70, batch loss: 31.731569290161133\n",
      "epoch: 71, batch loss: 32.752851804097496\n",
      "epoch: 72, batch loss: 31.134151458740234\n",
      "epoch: 73, batch loss: 31.399308681488037\n",
      "epoch: 74, batch loss: 31.015894571940105\n",
      "epoch: 75, batch loss: 31.494717915852863\n",
      "epoch: 76, batch loss: 30.841225465138752\n",
      "epoch: 77, batch loss: 32.16933012008667\n",
      "epoch: 78, batch loss: 30.137160619099934\n",
      "epoch: 79, batch loss: 30.98610480626424\n",
      "epoch: 80, batch loss: 30.09197235107422\n",
      "epoch: 81, batch loss: 30.472180366516113\n",
      "epoch: 82, batch loss: 30.182758172353108\n",
      "epoch: 83, batch loss: 30.45642360051473\n",
      "epoch: 84, batch loss: 29.63403590520223\n",
      "epoch: 85, batch loss: 29.61841328938802\n",
      "epoch: 86, batch loss: 29.1678565343221\n",
      "epoch: 87, batch loss: 30.44280735651652\n",
      "epoch: 88, batch loss: 29.031956672668457\n",
      "epoch: 89, batch loss: 28.792383035024006\n",
      "epoch: 90, batch loss: 29.07691462834676\n",
      "epoch: 91, batch loss: 28.5257085164388\n",
      "epoch: 92, batch loss: 28.03713099161784\n",
      "epoch: 93, batch loss: 28.13764238357544\n",
      "epoch: 94, batch loss: 29.061513264973957\n",
      "epoch: 95, batch loss: 28.694081783294678\n",
      "epoch: 96, batch loss: 27.683524926503498\n",
      "epoch: 97, batch loss: 27.659587860107422\n",
      "epoch: 98, batch loss: 28.13682460784912\n",
      "epoch: 99, batch loss: 27.24379285176595\n",
      "epoch: 100, batch loss: 29.136510213216145\n",
      "epoch: 101, batch loss: 27.857585589090984\n",
      "epoch: 102, batch loss: 27.025171597798664\n",
      "epoch: 103, batch loss: 27.82376305262248\n",
      "epoch: 104, batch loss: 27.610190868377686\n",
      "epoch: 105, batch loss: 27.03325653076172\n",
      "epoch: 106, batch loss: 26.94628079732259\n",
      "epoch: 107, batch loss: 26.37032739321391\n",
      "epoch: 108, batch loss: 27.990811983744305\n",
      "epoch: 109, batch loss: 26.401134808858234\n",
      "epoch: 110, batch loss: 26.079798936843872\n",
      "epoch: 111, batch loss: 26.968433539072674\n",
      "epoch: 112, batch loss: 26.024065017700195\n",
      "epoch: 113, batch loss: 25.823025544484455\n",
      "epoch: 114, batch loss: 26.240931669871014\n",
      "epoch: 115, batch loss: 26.506264368693035\n",
      "epoch: 116, batch loss: 27.114394187927246\n",
      "epoch: 117, batch loss: 25.74764935175578\n",
      "epoch: 118, batch loss: 25.629159927368164\n",
      "epoch: 119, batch loss: 26.88148069381714\n",
      "epoch: 120, batch loss: 25.999022960662842\n",
      "epoch: 121, batch loss: 25.547886689503986\n",
      "epoch: 122, batch loss: 25.56388568878174\n",
      "epoch: 123, batch loss: 25.787142753601074\n",
      "epoch: 124, batch loss: 25.866775512695312\n",
      "epoch: 125, batch loss: 26.075303475062054\n",
      "epoch: 126, batch loss: 25.721121946970623\n",
      "epoch: 127, batch loss: 25.409115473429363\n",
      "epoch: 128, batch loss: 25.434396107991535\n",
      "epoch: 129, batch loss: 24.932557741800945\n",
      "epoch: 130, batch loss: 25.07693640391032\n",
      "epoch: 131, batch loss: 25.814543565114338\n",
      "epoch: 132, batch loss: 24.94579792022705\n",
      "epoch: 133, batch loss: 25.6159348487854\n",
      "epoch: 134, batch loss: 24.844159603118896\n",
      "epoch: 135, batch loss: 24.11842481295268\n",
      "epoch: 136, batch loss: 24.449159145355225\n",
      "epoch: 137, batch loss: 24.0210755666097\n",
      "epoch: 138, batch loss: 24.298204104105633\n",
      "epoch: 139, batch loss: 24.52147976557414\n",
      "epoch: 140, batch loss: 24.509365876515705\n",
      "epoch: 141, batch loss: 24.55359109242757\n",
      "epoch: 142, batch loss: 24.040326436360676\n",
      "epoch: 143, batch loss: 24.036041418711346\n",
      "epoch: 144, batch loss: 25.19690736134847\n",
      "epoch: 145, batch loss: 25.273115475972492\n",
      "epoch: 146, batch loss: 24.81585756937663\n",
      "epoch: 147, batch loss: 24.476741790771484\n",
      "epoch: 148, batch loss: 23.86936394373576\n",
      "epoch: 149, batch loss: 23.69439458847046\n",
      "epoch: 150, batch loss: 23.85495964686076\n",
      "epoch: 151, batch loss: 23.773796717325848\n",
      "epoch: 152, batch loss: 23.43400812149048\n",
      "epoch: 153, batch loss: 24.2118026415507\n",
      "epoch: 154, batch loss: 23.75573492050171\n",
      "epoch: 155, batch loss: 23.624174912770588\n",
      "epoch: 156, batch loss: 24.77958567937215\n",
      "epoch: 157, batch loss: 23.858552932739258\n",
      "epoch: 158, batch loss: 23.27561044692993\n",
      "epoch: 159, batch loss: 23.50010546048482\n",
      "epoch: 160, batch loss: 23.47449763615926\n",
      "epoch: 161, batch loss: 23.482428550720215\n",
      "epoch: 162, batch loss: 23.37039128939311\n",
      "epoch: 163, batch loss: 23.50435733795166\n",
      "epoch: 164, batch loss: 24.410818576812744\n",
      "epoch: 165, batch loss: 23.351181030273438\n",
      "epoch: 166, batch loss: 24.06384563446045\n",
      "epoch: 167, batch loss: 23.101696650187176\n",
      "epoch: 168, batch loss: 23.96248944600423\n",
      "epoch: 169, batch loss: 23.53633403778076\n",
      "epoch: 170, batch loss: 23.11390558878581\n",
      "epoch: 171, batch loss: 23.191675027211506\n",
      "epoch: 172, batch loss: 23.18732253710429\n",
      "epoch: 173, batch loss: 22.779897689819336\n",
      "epoch: 174, batch loss: 23.642770449320476\n",
      "epoch: 175, batch loss: 22.62913179397583\n",
      "epoch: 176, batch loss: 23.337801933288574\n",
      "epoch: 177, batch loss: 22.943267504374187\n",
      "epoch: 178, batch loss: 22.420494000116985\n",
      "epoch: 179, batch loss: 22.77390480041504\n",
      "epoch: 180, batch loss: 22.88193106651306\n",
      "epoch: 181, batch loss: 22.50086323420207\n",
      "epoch: 182, batch loss: 22.664092381795246\n",
      "epoch: 183, batch loss: 22.65868838628133\n",
      "epoch: 184, batch loss: 22.47693411509196\n",
      "epoch: 185, batch loss: 22.488383054733276\n",
      "epoch: 186, batch loss: 22.32323757807414\n",
      "epoch: 187, batch loss: 22.773571809132893\n",
      "epoch: 188, batch loss: 22.235435326894123\n",
      "epoch: 189, batch loss: 22.004051208496094\n",
      "epoch: 190, batch loss: 23.27097749710083\n",
      "epoch: 191, batch loss: 22.78527593612671\n",
      "epoch: 192, batch loss: 22.64098318417867\n",
      "epoch: 193, batch loss: 22.240901470184326\n",
      "epoch: 194, batch loss: 22.565372625986736\n",
      "epoch: 195, batch loss: 22.28808530171712\n",
      "epoch: 196, batch loss: 22.427592833836872\n",
      "epoch: 197, batch loss: 22.29353864987691\n",
      "epoch: 198, batch loss: 22.60348606109619\n",
      "epoch: 199, batch loss: 21.816624244054157\n",
      "epoch: 200, batch loss: 22.669975916544598\n",
      "epoch: 201, batch loss: 21.902887026468914\n",
      "epoch: 202, batch loss: 22.155821800231934\n",
      "epoch: 203, batch loss: 21.929202159245808\n",
      "epoch: 204, batch loss: 22.13731861114502\n",
      "epoch: 205, batch loss: 22.91976499557495\n",
      "epoch: 206, batch loss: 22.60738690694173\n",
      "epoch: 207, batch loss: 22.112499554951984\n",
      "epoch: 208, batch loss: 22.047696590423584\n",
      "epoch: 209, batch loss: 22.314189275105793\n",
      "epoch: 210, batch loss: 21.99281358718872\n",
      "epoch: 211, batch loss: 21.4436678091685\n",
      "epoch: 212, batch loss: 22.574023882548016\n",
      "epoch: 213, batch loss: 22.18048365910848\n",
      "epoch: 214, batch loss: 22.39207712809245\n",
      "epoch: 215, batch loss: 21.99523671468099\n",
      "epoch: 216, batch loss: 22.40052890777588\n",
      "epoch: 217, batch loss: 21.7172638575236\n",
      "epoch: 218, batch loss: 22.35070474942525\n",
      "epoch: 219, batch loss: 21.838467915852863\n",
      "epoch: 220, batch loss: 22.15346686045329\n",
      "epoch: 221, batch loss: 21.527931451797485\n",
      "epoch: 222, batch loss: 21.75859236717224\n",
      "epoch: 223, batch loss: 22.712650616963703\n",
      "epoch: 224, batch loss: 22.049028078715008\n",
      "epoch: 225, batch loss: 21.85290749867757\n",
      "epoch: 226, batch loss: 21.296526511510212\n",
      "epoch: 227, batch loss: 22.338461955388386\n",
      "epoch: 228, batch loss: 21.557011127471924\n",
      "epoch: 229, batch loss: 21.736788193384807\n",
      "epoch: 230, batch loss: 21.772891124089558\n",
      "epoch: 231, batch loss: 21.54530183474223\n",
      "epoch: 232, batch loss: 21.477512915929157\n",
      "epoch: 233, batch loss: 22.74412965774536\n",
      "epoch: 234, batch loss: 21.691223462422688\n",
      "epoch: 235, batch loss: 21.38339837392171\n",
      "epoch: 236, batch loss: 21.598159790039062\n",
      "epoch: 237, batch loss: 21.73931320508321\n",
      "epoch: 238, batch loss: 21.15165678660075\n",
      "epoch: 239, batch loss: 21.275723854700725\n",
      "epoch: 240, batch loss: 21.334630409876507\n",
      "epoch: 241, batch loss: 21.60615873336792\n",
      "epoch: 242, batch loss: 21.794989426930744\n",
      "epoch: 243, batch loss: 22.647583484649658\n",
      "epoch: 244, batch loss: 21.635383288065594\n",
      "epoch: 245, batch loss: 20.96904158592224\n",
      "epoch: 246, batch loss: 21.296836614608765\n",
      "epoch: 247, batch loss: 21.590072234471638\n",
      "epoch: 248, batch loss: 21.40472682317098\n",
      "epoch: 249, batch loss: 21.16299025217692\n",
      "epoch: 250, batch loss: 21.9181805451711\n",
      "epoch: 251, batch loss: 21.226028362909954\n",
      "epoch: 252, batch loss: 21.610777695973713\n",
      "epoch: 253, batch loss: 20.972224553426106\n",
      "epoch: 254, batch loss: 21.76358429590861\n",
      "epoch: 255, batch loss: 21.904027144114178\n",
      "epoch: 256, batch loss: 21.50872500737508\n",
      "epoch: 257, batch loss: 21.66617163022359\n",
      "epoch: 258, batch loss: 21.143847544987995\n",
      "epoch: 259, batch loss: 21.135223388671875\n",
      "epoch: 260, batch loss: 21.23428241411845\n",
      "epoch: 261, batch loss: 21.403298536936443\n",
      "epoch: 262, batch loss: 21.523401578267414\n",
      "epoch: 263, batch loss: 21.812918663024902\n",
      "epoch: 264, batch loss: 20.958492596944172\n",
      "epoch: 265, batch loss: 21.16712299982707\n",
      "epoch: 266, batch loss: 20.91859531402588\n",
      "epoch: 267, batch loss: 21.777366320292156\n",
      "epoch: 268, batch loss: 22.3969570795695\n",
      "epoch: 269, batch loss: 21.43619505564372\n",
      "epoch: 270, batch loss: 21.94018252690633\n",
      "epoch: 271, batch loss: 21.490094582239788\n",
      "epoch: 272, batch loss: 21.028136491775513\n",
      "epoch: 273, batch loss: 21.266497770945232\n",
      "epoch: 274, batch loss: 21.099892059961956\n",
      "epoch: 275, batch loss: 21.212687412897747\n",
      "epoch: 276, batch loss: 20.80376633008321\n",
      "epoch: 277, batch loss: 20.780128002166748\n",
      "epoch: 278, batch loss: 21.398590485254925\n",
      "epoch: 279, batch loss: 20.850087642669678\n",
      "epoch: 280, batch loss: 20.918009996414185\n",
      "epoch: 281, batch loss: 20.640833695729572\n",
      "epoch: 282, batch loss: 21.629625876744587\n",
      "epoch: 283, batch loss: 20.68612313270569\n",
      "epoch: 284, batch loss: 21.345723867416382\n",
      "epoch: 285, batch loss: 21.168723583221436\n",
      "epoch: 286, batch loss: 21.249303023020428\n",
      "epoch: 287, batch loss: 21.527535915374756\n",
      "epoch: 288, batch loss: 20.624243021011353\n",
      "epoch: 289, batch loss: 21.035187880198162\n",
      "epoch: 290, batch loss: 20.647589842478435\n",
      "epoch: 291, batch loss: 20.855931758880615\n",
      "epoch: 292, batch loss: 20.718480984369915\n",
      "epoch: 293, batch loss: 21.047171592712402\n",
      "epoch: 294, batch loss: 20.48250397046407\n",
      "epoch: 295, batch loss: 20.6245919863383\n",
      "epoch: 296, batch loss: 21.397979736328125\n",
      "epoch: 297, batch loss: 21.433754046758015\n",
      "epoch: 298, batch loss: 21.120347181955974\n",
      "epoch: 299, batch loss: 20.632895469665527\n",
      "epoch: 300, batch loss: 20.78047513961792\n",
      "epoch: 1, batch loss: 771.3618214925131\n",
      "epoch: 2, batch loss: 769.3767954508463\n",
      "epoch: 3, batch loss: 763.2168121337891\n",
      "epoch: 4, batch loss: 763.0800628662109\n",
      "epoch: 5, batch loss: 758.9236297607422\n",
      "epoch: 6, batch loss: 749.1377970377604\n",
      "epoch: 7, batch loss: 742.8188120524088\n",
      "epoch: 8, batch loss: 741.2380523681641\n",
      "epoch: 9, batch loss: 727.2433573404948\n",
      "epoch: 10, batch loss: 710.8419698079427\n",
      "epoch: 11, batch loss: 695.9834187825521\n",
      "epoch: 12, batch loss: 663.1643015543619\n",
      "epoch: 13, batch loss: 633.1570281982422\n",
      "epoch: 14, batch loss: 591.3526763916016\n",
      "epoch: 15, batch loss: 535.2589009602865\n",
      "epoch: 16, batch loss: 471.45843505859375\n",
      "epoch: 17, batch loss: 403.3279724121094\n",
      "epoch: 18, batch loss: 334.5305582682292\n",
      "epoch: 19, batch loss: 258.14418029785156\n",
      "epoch: 20, batch loss: 201.5286661783854\n",
      "epoch: 21, batch loss: 157.47989336649576\n",
      "epoch: 22, batch loss: 128.72029050191244\n",
      "epoch: 23, batch loss: 106.88713836669922\n",
      "epoch: 24, batch loss: 92.69311014811198\n",
      "epoch: 25, batch loss: 80.15671412150066\n",
      "epoch: 26, batch loss: 71.84644889831543\n",
      "epoch: 27, batch loss: 65.14920075734456\n",
      "epoch: 28, batch loss: 60.6098747253418\n",
      "epoch: 29, batch loss: 58.501532554626465\n",
      "epoch: 30, batch loss: 55.08570639292399\n",
      "epoch: 31, batch loss: 51.91588274637858\n",
      "epoch: 32, batch loss: 49.778444608052574\n",
      "epoch: 33, batch loss: 47.62243445714315\n",
      "epoch: 34, batch loss: 49.362781842549644\n",
      "epoch: 35, batch loss: 49.474568684895836\n",
      "epoch: 36, batch loss: 45.82843748728434\n",
      "epoch: 37, batch loss: 45.6974991162618\n",
      "epoch: 38, batch loss: 45.684866428375244\n",
      "epoch: 39, batch loss: 42.985528310139976\n",
      "epoch: 40, batch loss: 42.46179914474487\n",
      "epoch: 41, batch loss: 43.46401357650757\n",
      "epoch: 42, batch loss: 42.720006148020424\n",
      "epoch: 43, batch loss: 40.359906355539955\n",
      "epoch: 44, batch loss: 39.23718277613322\n",
      "epoch: 45, batch loss: 38.94369697570801\n",
      "epoch: 46, batch loss: 38.743077437082924\n",
      "epoch: 47, batch loss: 37.4699281056722\n",
      "epoch: 48, batch loss: 39.4523541132609\n",
      "epoch: 49, batch loss: 36.823188940684\n",
      "epoch: 50, batch loss: 35.74882793426514\n",
      "epoch: 51, batch loss: 37.54224491119385\n",
      "epoch: 52, batch loss: 36.48189194997152\n",
      "epoch: 53, batch loss: 36.73523775736491\n",
      "epoch: 54, batch loss: 35.28022384643555\n",
      "epoch: 55, batch loss: 34.9281374613444\n",
      "epoch: 56, batch loss: 34.5916215578715\n",
      "epoch: 57, batch loss: 33.942492643992104\n",
      "epoch: 58, batch loss: 33.14936828613281\n",
      "epoch: 59, batch loss: 32.45676199595133\n",
      "epoch: 60, batch loss: 33.73607317606608\n",
      "epoch: 61, batch loss: 33.779930432637535\n",
      "epoch: 62, batch loss: 32.246835708618164\n",
      "epoch: 63, batch loss: 33.2223858833313\n",
      "epoch: 64, batch loss: 31.52172040939331\n",
      "epoch: 65, batch loss: 32.20298449198405\n",
      "epoch: 66, batch loss: 32.49351088205973\n",
      "epoch: 67, batch loss: 31.059706528981526\n",
      "epoch: 68, batch loss: 31.764978249867756\n",
      "epoch: 69, batch loss: 30.42410119374593\n",
      "epoch: 70, batch loss: 32.469136555989586\n",
      "epoch: 71, batch loss: 31.390089829762776\n",
      "epoch: 72, batch loss: 30.084177017211914\n",
      "epoch: 73, batch loss: 30.11703284581502\n",
      "epoch: 74, batch loss: 29.76668882369995\n",
      "epoch: 75, batch loss: 29.949251015981037\n",
      "epoch: 76, batch loss: 29.529489199320476\n",
      "epoch: 77, batch loss: 29.285542329152424\n",
      "epoch: 78, batch loss: 29.357175985972088\n",
      "epoch: 79, batch loss: 29.70889647801717\n",
      "epoch: 80, batch loss: 29.193251132965088\n",
      "epoch: 81, batch loss: 29.49769926071167\n",
      "epoch: 82, batch loss: 30.225110054016113\n",
      "epoch: 83, batch loss: 29.412631193796795\n",
      "epoch: 84, batch loss: 29.068989912668865\n",
      "epoch: 85, batch loss: 28.88359022140503\n",
      "epoch: 86, batch loss: 28.557487964630127\n",
      "epoch: 87, batch loss: 29.187607606252033\n",
      "epoch: 88, batch loss: 28.05122184753418\n",
      "epoch: 89, batch loss: 27.949036757151287\n",
      "epoch: 90, batch loss: 28.156807899475098\n",
      "epoch: 91, batch loss: 29.07538366317749\n",
      "epoch: 92, batch loss: 27.78667974472046\n",
      "epoch: 93, batch loss: 27.0644211769104\n",
      "epoch: 94, batch loss: 27.933500448862713\n",
      "epoch: 95, batch loss: 27.438244819641113\n",
      "epoch: 96, batch loss: 26.89357344309489\n",
      "epoch: 97, batch loss: 27.402194182078045\n",
      "epoch: 98, batch loss: 27.625792026519775\n",
      "epoch: 99, batch loss: 26.551219304402668\n",
      "epoch: 100, batch loss: 27.412911891937256\n",
      "epoch: 101, batch loss: 26.75472927093506\n",
      "epoch: 102, batch loss: 28.131277243296307\n",
      "epoch: 103, batch loss: 27.577005704243977\n",
      "epoch: 104, batch loss: 26.500403086344402\n",
      "epoch: 105, batch loss: 26.541840076446533\n",
      "epoch: 106, batch loss: 25.890315373738606\n",
      "epoch: 107, batch loss: 26.230781714121502\n",
      "epoch: 108, batch loss: 26.28825108210246\n",
      "epoch: 109, batch loss: 25.836551825205486\n",
      "epoch: 110, batch loss: 26.036166667938232\n",
      "epoch: 111, batch loss: 26.767922083536785\n",
      "epoch: 112, batch loss: 25.82668463389079\n",
      "epoch: 113, batch loss: 25.475098609924316\n",
      "epoch: 114, batch loss: 25.946783701578777\n",
      "epoch: 115, batch loss: 26.56117304166158\n",
      "epoch: 116, batch loss: 25.152282158533733\n",
      "epoch: 117, batch loss: 25.441438515981037\n",
      "epoch: 118, batch loss: 25.645616213480633\n",
      "epoch: 119, batch loss: 25.76283009847005\n",
      "epoch: 120, batch loss: 25.014077027638752\n",
      "epoch: 121, batch loss: 26.11773935953776\n",
      "epoch: 122, batch loss: 25.62894042332967\n",
      "epoch: 123, batch loss: 25.283734798431396\n",
      "epoch: 124, batch loss: 25.207433064778645\n",
      "epoch: 125, batch loss: 25.23535378774007\n",
      "epoch: 126, batch loss: 24.853387355804443\n",
      "epoch: 127, batch loss: 24.97504146893819\n",
      "epoch: 128, batch loss: 24.70283015569051\n",
      "epoch: 129, batch loss: 24.443382740020752\n",
      "epoch: 130, batch loss: 24.977636496225994\n",
      "epoch: 131, batch loss: 25.203296502431233\n",
      "epoch: 132, batch loss: 24.454363505045574\n",
      "epoch: 133, batch loss: 24.313266277313232\n",
      "epoch: 134, batch loss: 24.37165101369222\n",
      "epoch: 135, batch loss: 24.62181266148885\n",
      "epoch: 136, batch loss: 24.42323382695516\n",
      "epoch: 137, batch loss: 23.76258333524068\n",
      "epoch: 138, batch loss: 24.768213192621868\n",
      "epoch: 139, batch loss: 24.395614624023438\n",
      "epoch: 140, batch loss: 23.79669491449992\n",
      "epoch: 141, batch loss: 24.154168446858723\n",
      "epoch: 142, batch loss: 24.184370517730713\n",
      "epoch: 143, batch loss: 24.32361078262329\n",
      "epoch: 144, batch loss: 23.895326773325603\n",
      "epoch: 145, batch loss: 24.486077467600506\n",
      "epoch: 146, batch loss: 23.41543523470561\n",
      "epoch: 147, batch loss: 23.62896426518758\n",
      "epoch: 148, batch loss: 23.958144187927246\n",
      "epoch: 149, batch loss: 23.55085563659668\n",
      "epoch: 150, batch loss: 24.506476720174152\n",
      "epoch: 151, batch loss: 23.030630588531494\n",
      "epoch: 152, batch loss: 23.855225086212158\n",
      "epoch: 153, batch loss: 23.2231601079305\n",
      "epoch: 154, batch loss: 23.511172930399578\n",
      "epoch: 155, batch loss: 23.62406078974406\n",
      "epoch: 156, batch loss: 23.007399320602417\n",
      "epoch: 157, batch loss: 23.768908659617107\n",
      "epoch: 158, batch loss: 23.013107776641846\n",
      "epoch: 159, batch loss: 23.106210708618164\n",
      "epoch: 160, batch loss: 23.802941004435223\n",
      "epoch: 161, batch loss: 23.047417481740315\n",
      "epoch: 162, batch loss: 22.923115730285645\n",
      "epoch: 163, batch loss: 23.586896578470867\n",
      "epoch: 164, batch loss: 23.349249521891277\n",
      "epoch: 165, batch loss: 23.63830828666687\n",
      "epoch: 166, batch loss: 23.591318289438885\n",
      "epoch: 167, batch loss: 22.898855050404865\n",
      "epoch: 168, batch loss: 22.261016527811687\n",
      "epoch: 169, batch loss: 22.727121194203693\n",
      "epoch: 170, batch loss: 22.71546443303426\n",
      "epoch: 171, batch loss: 22.379037936528523\n",
      "epoch: 172, batch loss: 23.183228015899658\n",
      "epoch: 173, batch loss: 22.87812312444051\n",
      "epoch: 174, batch loss: 22.672338167826336\n",
      "epoch: 175, batch loss: 23.05867338180542\n",
      "epoch: 176, batch loss: 22.28919045130412\n",
      "epoch: 177, batch loss: 22.642778237660725\n",
      "epoch: 178, batch loss: 22.493846734364826\n",
      "epoch: 179, batch loss: 22.53348970413208\n",
      "epoch: 180, batch loss: 23.06867202123006\n",
      "epoch: 181, batch loss: 22.27911901473999\n",
      "epoch: 182, batch loss: 23.455865065256756\n",
      "epoch: 183, batch loss: 23.403377930323284\n",
      "epoch: 184, batch loss: 22.05458990732829\n",
      "epoch: 185, batch loss: 21.993656476338703\n",
      "epoch: 186, batch loss: 22.300174236297607\n",
      "epoch: 187, batch loss: 22.237518469492596\n",
      "epoch: 188, batch loss: 22.786973635355633\n",
      "epoch: 189, batch loss: 22.703429063161213\n",
      "epoch: 190, batch loss: 22.316585699717205\n",
      "epoch: 191, batch loss: 22.07838733990987\n",
      "epoch: 192, batch loss: 22.073355674743652\n",
      "epoch: 193, batch loss: 22.230129957199097\n",
      "epoch: 194, batch loss: 22.870264689127605\n",
      "epoch: 195, batch loss: 23.59566593170166\n",
      "epoch: 196, batch loss: 22.472870190938313\n",
      "epoch: 197, batch loss: 22.826974789301556\n",
      "epoch: 198, batch loss: 22.718987782796223\n",
      "epoch: 199, batch loss: 21.967334906260174\n",
      "epoch: 200, batch loss: 22.41752290725708\n",
      "epoch: 201, batch loss: 22.234311898549397\n",
      "epoch: 202, batch loss: 22.606030305226643\n",
      "epoch: 203, batch loss: 22.31886927286784\n",
      "epoch: 204, batch loss: 21.307228962580364\n",
      "epoch: 205, batch loss: 21.480460087458294\n",
      "epoch: 206, batch loss: 22.164418697357178\n",
      "epoch: 207, batch loss: 21.859053134918213\n",
      "epoch: 208, batch loss: 21.729819774627686\n",
      "epoch: 209, batch loss: 21.970574061075848\n",
      "epoch: 210, batch loss: 21.631051540374756\n",
      "epoch: 211, batch loss: 22.10734287897746\n",
      "epoch: 212, batch loss: 22.022000233332317\n",
      "epoch: 213, batch loss: 21.544649442036945\n",
      "epoch: 214, batch loss: 22.159870862960815\n",
      "epoch: 215, batch loss: 21.709425052007038\n",
      "epoch: 216, batch loss: 22.312719186147053\n",
      "epoch: 217, batch loss: 21.308307965596516\n",
      "epoch: 218, batch loss: 21.345656156539917\n",
      "epoch: 219, batch loss: 22.5975817044576\n",
      "epoch: 220, batch loss: 21.51830267906189\n",
      "epoch: 221, batch loss: 21.91550079981486\n",
      "epoch: 222, batch loss: 21.871249675750732\n",
      "epoch: 223, batch loss: 21.887202739715576\n",
      "epoch: 224, batch loss: 21.44837299982707\n",
      "epoch: 225, batch loss: 22.360330661137898\n",
      "epoch: 226, batch loss: 21.637619256973267\n",
      "epoch: 227, batch loss: 21.740715662638348\n",
      "epoch: 228, batch loss: 21.911408583323162\n",
      "epoch: 229, batch loss: 21.22796320915222\n",
      "epoch: 230, batch loss: 22.627031644185383\n",
      "epoch: 231, batch loss: 21.95450560251872\n",
      "epoch: 232, batch loss: 21.572450240453083\n",
      "epoch: 233, batch loss: 22.009095986684162\n",
      "epoch: 234, batch loss: 23.273611545562744\n",
      "epoch: 235, batch loss: 22.612533966700237\n",
      "epoch: 236, batch loss: 21.441161394119263\n",
      "epoch: 237, batch loss: 21.977428913116455\n",
      "epoch: 238, batch loss: 22.74554991722107\n",
      "epoch: 239, batch loss: 21.464915990829468\n",
      "epoch: 240, batch loss: 21.572051684061687\n",
      "epoch: 241, batch loss: 22.17566434542338\n",
      "epoch: 242, batch loss: 21.106502930323284\n",
      "epoch: 243, batch loss: 21.61067295074463\n",
      "epoch: 244, batch loss: 21.289672374725342\n",
      "epoch: 245, batch loss: 21.24005659421285\n",
      "epoch: 246, batch loss: 21.44424343109131\n",
      "epoch: 247, batch loss: 21.207300027211506\n",
      "epoch: 248, batch loss: 21.64058796564738\n",
      "epoch: 249, batch loss: 21.82100184758504\n",
      "epoch: 250, batch loss: 20.964177052179974\n",
      "epoch: 251, batch loss: 22.245917717615765\n",
      "epoch: 252, batch loss: 21.066535393397015\n",
      "epoch: 253, batch loss: 20.794387340545654\n",
      "epoch: 254, batch loss: 20.903238375981648\n",
      "epoch: 255, batch loss: 21.58551836013794\n",
      "epoch: 256, batch loss: 21.56018352508545\n",
      "epoch: 257, batch loss: 21.03681969642639\n",
      "epoch: 258, batch loss: 21.48667828241984\n",
      "epoch: 259, batch loss: 21.85980995496114\n",
      "epoch: 260, batch loss: 21.545059045155842\n",
      "epoch: 261, batch loss: 20.67554585138957\n",
      "epoch: 262, batch loss: 21.358209292093914\n",
      "epoch: 263, batch loss: 20.626401503880818\n",
      "epoch: 264, batch loss: 21.11182975769043\n",
      "epoch: 265, batch loss: 21.10216522216797\n",
      "epoch: 266, batch loss: 21.648505846659344\n",
      "epoch: 267, batch loss: 22.52830417950948\n",
      "epoch: 268, batch loss: 20.622499863306682\n",
      "epoch: 269, batch loss: 22.080362558364868\n",
      "epoch: 270, batch loss: 21.698232491811115\n",
      "epoch: 271, batch loss: 20.782488266626995\n",
      "epoch: 272, batch loss: 21.890938599904377\n",
      "epoch: 273, batch loss: 20.88375488917033\n",
      "epoch: 274, batch loss: 20.42923331260681\n",
      "epoch: 275, batch loss: 20.988354365030926\n",
      "epoch: 276, batch loss: 21.50927988688151\n",
      "epoch: 277, batch loss: 21.375361442565918\n",
      "epoch: 278, batch loss: 21.032349745432537\n",
      "epoch: 279, batch loss: 21.727211316426594\n",
      "epoch: 280, batch loss: 20.779518127441406\n",
      "epoch: 281, batch loss: 21.82839798927307\n",
      "epoch: 282, batch loss: 20.87601113319397\n",
      "epoch: 283, batch loss: 22.084391434987385\n",
      "epoch: 284, batch loss: 20.60702673594157\n",
      "epoch: 285, batch loss: 20.651658693949383\n",
      "epoch: 286, batch loss: 21.28245210647583\n",
      "epoch: 287, batch loss: 21.094528039296467\n",
      "epoch: 288, batch loss: 21.61660846074422\n",
      "epoch: 289, batch loss: 20.83493177096049\n",
      "epoch: 290, batch loss: 21.726767619450886\n",
      "epoch: 291, batch loss: 20.41214942932129\n",
      "epoch: 292, batch loss: 21.5630038579305\n",
      "epoch: 293, batch loss: 21.30397590001424\n",
      "epoch: 294, batch loss: 20.877041180928547\n",
      "epoch: 295, batch loss: 20.310865481694538\n",
      "epoch: 296, batch loss: 21.0940682888031\n",
      "epoch: 297, batch loss: 20.53418167432149\n",
      "epoch: 298, batch loss: 20.65948232014974\n",
      "epoch: 299, batch loss: 20.570001284281414\n",
      "epoch: 300, batch loss: 21.14228614171346\n",
      "epoch: 1, batch loss: 783.9028676350912\n",
      "epoch: 2, batch loss: 765.4088897705078\n",
      "epoch: 3, batch loss: 765.0615539550781\n",
      "epoch: 4, batch loss: 767.7300262451172\n",
      "epoch: 5, batch loss: 752.0062357584635\n",
      "epoch: 6, batch loss: 748.6295674641927\n",
      "epoch: 7, batch loss: 727.3429616292318\n",
      "epoch: 8, batch loss: 716.0572255452474\n",
      "epoch: 9, batch loss: 692.8682556152344\n",
      "epoch: 10, batch loss: 662.9543507893881\n",
      "epoch: 11, batch loss: 637.4283192952474\n",
      "epoch: 12, batch loss: 587.8682963053385\n",
      "epoch: 13, batch loss: 539.9970016479492\n",
      "epoch: 14, batch loss: 474.6490300496419\n",
      "epoch: 15, batch loss: 414.3333206176758\n",
      "epoch: 16, batch loss: 348.84953053792316\n",
      "epoch: 17, batch loss: 282.65343856811523\n",
      "epoch: 18, batch loss: 224.3855234781901\n",
      "epoch: 19, batch loss: 176.98556900024414\n",
      "epoch: 20, batch loss: 142.11664644877115\n",
      "epoch: 21, batch loss: 118.47814114888509\n",
      "epoch: 22, batch loss: 101.12547238667806\n",
      "epoch: 23, batch loss: 87.12556902567546\n",
      "epoch: 24, batch loss: 86.08636887868245\n",
      "epoch: 25, batch loss: 68.66114711761475\n",
      "epoch: 26, batch loss: 63.402444203694664\n",
      "epoch: 27, batch loss: 59.979475339253746\n",
      "epoch: 28, batch loss: 53.91479778289795\n",
      "epoch: 29, batch loss: 50.02347151438395\n",
      "epoch: 30, batch loss: 51.57217884063721\n",
      "epoch: 31, batch loss: 46.52118333180746\n",
      "epoch: 32, batch loss: 45.056467056274414\n",
      "epoch: 33, batch loss: 44.819104512532554\n",
      "epoch: 34, batch loss: 42.728538831075035\n",
      "epoch: 35, batch loss: 42.620102882385254\n",
      "epoch: 36, batch loss: 42.772820472717285\n",
      "epoch: 37, batch loss: 41.137370904286705\n",
      "epoch: 38, batch loss: 40.30912160873413\n",
      "epoch: 39, batch loss: 40.061235427856445\n",
      "epoch: 40, batch loss: 39.56522560119629\n",
      "epoch: 41, batch loss: 39.72285350163778\n",
      "epoch: 42, batch loss: 38.42800855636597\n",
      "epoch: 43, batch loss: 37.888461112976074\n",
      "epoch: 44, batch loss: 38.784469286600746\n",
      "epoch: 45, batch loss: 37.01935736338297\n",
      "epoch: 46, batch loss: 39.06965525945028\n",
      "epoch: 47, batch loss: 37.475091298421226\n",
      "epoch: 48, batch loss: 36.3841495513916\n",
      "epoch: 49, batch loss: 36.356457551320396\n",
      "epoch: 50, batch loss: 36.71251265207926\n",
      "epoch: 51, batch loss: 35.56560134887695\n",
      "epoch: 52, batch loss: 34.65488243103027\n",
      "epoch: 53, batch loss: 35.13860273361206\n",
      "epoch: 54, batch loss: 36.11676820119222\n",
      "epoch: 55, batch loss: 34.502254803975426\n",
      "epoch: 56, batch loss: 34.45698897043864\n",
      "epoch: 57, batch loss: 34.072525342305504\n",
      "epoch: 58, batch loss: 35.115039030710854\n",
      "epoch: 59, batch loss: 33.35195827484131\n",
      "epoch: 60, batch loss: 32.3883646329244\n",
      "epoch: 61, batch loss: 32.32358376185099\n",
      "epoch: 62, batch loss: 32.584729512532554\n",
      "epoch: 63, batch loss: 31.851985454559326\n",
      "epoch: 64, batch loss: 33.08947738011678\n",
      "epoch: 65, batch loss: 31.62538941701253\n",
      "epoch: 66, batch loss: 31.941233158111572\n",
      "epoch: 67, batch loss: 31.686002413431805\n",
      "epoch: 68, batch loss: 31.391664346059162\n",
      "epoch: 69, batch loss: 31.25706911087036\n",
      "epoch: 70, batch loss: 31.265909671783447\n",
      "epoch: 71, batch loss: 30.54588747024536\n",
      "epoch: 72, batch loss: 30.000401973724365\n",
      "epoch: 73, batch loss: 30.802783648173016\n",
      "epoch: 74, batch loss: 29.722085158030193\n",
      "epoch: 75, batch loss: 29.852850278218586\n",
      "epoch: 76, batch loss: 29.980526288350422\n",
      "epoch: 77, batch loss: 29.627450784047443\n",
      "epoch: 78, batch loss: 29.591501394907635\n",
      "epoch: 79, batch loss: 30.40607722600301\n",
      "epoch: 80, batch loss: 28.625085194905598\n",
      "epoch: 81, batch loss: 29.77422348658244\n",
      "epoch: 82, batch loss: 29.59654649098714\n",
      "epoch: 83, batch loss: 28.75725221633911\n",
      "epoch: 84, batch loss: 28.107229073842365\n",
      "epoch: 85, batch loss: 28.67919381459554\n",
      "epoch: 86, batch loss: 29.257029056549072\n",
      "epoch: 87, batch loss: 30.0381121635437\n",
      "epoch: 88, batch loss: 27.89065980911255\n",
      "epoch: 89, batch loss: 27.665067195892334\n",
      "epoch: 90, batch loss: 28.747197310129803\n",
      "epoch: 91, batch loss: 28.26239474614461\n",
      "epoch: 92, batch loss: 27.67956240971883\n",
      "epoch: 93, batch loss: 27.620054562886555\n",
      "epoch: 94, batch loss: 28.62529746691386\n",
      "epoch: 95, batch loss: 27.68037287394206\n",
      "epoch: 96, batch loss: 28.566895484924316\n",
      "epoch: 97, batch loss: 26.759456157684326\n",
      "epoch: 98, batch loss: 26.707509835561115\n",
      "epoch: 99, batch loss: 26.739150524139404\n",
      "epoch: 100, batch loss: 28.748244285583496\n",
      "epoch: 101, batch loss: 26.33693790435791\n",
      "epoch: 102, batch loss: 26.539079189300537\n",
      "epoch: 103, batch loss: 26.4248046875\n",
      "epoch: 104, batch loss: 27.577609380086262\n",
      "epoch: 105, batch loss: 26.530979951222736\n",
      "epoch: 106, batch loss: 25.847798188527424\n",
      "epoch: 107, batch loss: 25.774118582407635\n",
      "epoch: 108, batch loss: 25.640206178029377\n",
      "epoch: 109, batch loss: 25.79430357615153\n",
      "epoch: 110, batch loss: 26.10104576746623\n",
      "epoch: 111, batch loss: 26.034300963083904\n",
      "epoch: 112, batch loss: 25.469149589538574\n",
      "epoch: 113, batch loss: 26.048414548238117\n",
      "epoch: 114, batch loss: 25.438146114349365\n",
      "epoch: 115, batch loss: 25.9365496635437\n",
      "epoch: 116, batch loss: 25.572559356689453\n",
      "epoch: 117, batch loss: 25.462721188863117\n",
      "epoch: 118, batch loss: 25.0568683942159\n",
      "epoch: 119, batch loss: 25.78773085276286\n",
      "epoch: 120, batch loss: 24.81776237487793\n",
      "epoch: 121, batch loss: 26.039493719736736\n",
      "epoch: 122, batch loss: 25.316481908162434\n",
      "epoch: 123, batch loss: 25.72777819633484\n",
      "epoch: 124, batch loss: 25.121013800303142\n",
      "epoch: 125, batch loss: 24.9107567469279\n",
      "epoch: 126, batch loss: 25.49440924326579\n",
      "epoch: 127, batch loss: 24.32494870821635\n",
      "epoch: 128, batch loss: 25.940241018931072\n",
      "epoch: 129, batch loss: 24.87925370534261\n",
      "epoch: 130, batch loss: 25.19559971491496\n",
      "epoch: 131, batch loss: 24.175605932871502\n",
      "epoch: 132, batch loss: 24.89511760075887\n",
      "epoch: 133, batch loss: 24.07931089401245\n",
      "epoch: 134, batch loss: 26.33866850535075\n",
      "epoch: 135, batch loss: 24.345855394999187\n",
      "epoch: 136, batch loss: 23.917134284973145\n",
      "epoch: 137, batch loss: 23.88300593694051\n",
      "epoch: 138, batch loss: 25.16256284713745\n",
      "epoch: 139, batch loss: 23.819366772969563\n",
      "epoch: 140, batch loss: 24.73758888244629\n",
      "epoch: 141, batch loss: 23.67225694656372\n",
      "epoch: 142, batch loss: 23.511892239252727\n",
      "epoch: 143, batch loss: 23.707351843516033\n",
      "epoch: 144, batch loss: 23.662218411763508\n",
      "epoch: 145, batch loss: 23.528736750284832\n",
      "epoch: 146, batch loss: 23.62992509206136\n",
      "epoch: 147, batch loss: 23.812116622924805\n",
      "epoch: 148, batch loss: 23.18364667892456\n",
      "epoch: 149, batch loss: 23.592094262440998\n",
      "epoch: 150, batch loss: 23.401448567708332\n",
      "epoch: 151, batch loss: 23.334946791330974\n",
      "epoch: 152, batch loss: 23.099876642227173\n",
      "epoch: 153, batch loss: 23.134423573811848\n",
      "epoch: 154, batch loss: 22.98624070485433\n",
      "epoch: 155, batch loss: 24.23475933074951\n",
      "epoch: 156, batch loss: 23.945895115534466\n",
      "epoch: 157, batch loss: 22.887961228688557\n",
      "epoch: 158, batch loss: 23.20833396911621\n",
      "epoch: 159, batch loss: 23.388532479604084\n",
      "epoch: 160, batch loss: 23.01456085840861\n",
      "epoch: 161, batch loss: 23.579355875651043\n",
      "epoch: 162, batch loss: 23.204104900360107\n",
      "epoch: 163, batch loss: 22.618781566619873\n",
      "epoch: 164, batch loss: 22.817273139953613\n",
      "epoch: 165, batch loss: 23.081658363342285\n",
      "epoch: 166, batch loss: 22.510968446731567\n",
      "epoch: 167, batch loss: 23.067870140075684\n",
      "epoch: 168, batch loss: 22.810592492421467\n",
      "epoch: 169, batch loss: 23.22714837392171\n",
      "epoch: 170, batch loss: 23.9569034576416\n",
      "epoch: 171, batch loss: 22.40654452641805\n",
      "epoch: 172, batch loss: 22.966276963551838\n",
      "epoch: 173, batch loss: 22.558464845021565\n",
      "epoch: 174, batch loss: 22.223069588343304\n",
      "epoch: 175, batch loss: 22.249318202336628\n",
      "epoch: 176, batch loss: 22.391330242156982\n",
      "epoch: 177, batch loss: 22.84710152943929\n",
      "epoch: 178, batch loss: 22.276429096857708\n",
      "epoch: 179, batch loss: 22.75215021769206\n",
      "epoch: 180, batch loss: 22.168559392293293\n",
      "epoch: 181, batch loss: 22.61448335647583\n",
      "epoch: 182, batch loss: 23.65600307782491\n",
      "epoch: 183, batch loss: 22.34430726369222\n",
      "epoch: 184, batch loss: 22.382593075434368\n",
      "epoch: 185, batch loss: 23.0487486521403\n",
      "epoch: 186, batch loss: 22.051777680714924\n",
      "epoch: 187, batch loss: 22.974573771158855\n",
      "epoch: 188, batch loss: 22.869027455647785\n",
      "epoch: 189, batch loss: 22.036530176798504\n",
      "epoch: 190, batch loss: 22.74701182047526\n",
      "epoch: 191, batch loss: 22.580763737360638\n",
      "epoch: 192, batch loss: 21.86013960838318\n",
      "epoch: 193, batch loss: 22.62728150685628\n",
      "epoch: 194, batch loss: 22.434416611989338\n",
      "epoch: 195, batch loss: 21.564265092213947\n",
      "epoch: 196, batch loss: 22.103134473164875\n",
      "epoch: 197, batch loss: 23.369874318440754\n",
      "epoch: 198, batch loss: 21.76405382156372\n",
      "epoch: 199, batch loss: 23.084945678710938\n",
      "epoch: 200, batch loss: 21.966349601745605\n",
      "epoch: 201, batch loss: 21.868191480636597\n",
      "epoch: 202, batch loss: 22.178606510162354\n",
      "epoch: 203, batch loss: 21.84358024597168\n",
      "epoch: 204, batch loss: 22.10357141494751\n",
      "epoch: 205, batch loss: 21.869729200998943\n",
      "epoch: 206, batch loss: 22.346988519032795\n",
      "epoch: 207, batch loss: 21.40287510553996\n",
      "epoch: 208, batch loss: 21.634828408559162\n",
      "epoch: 209, batch loss: 22.643336455027264\n",
      "epoch: 210, batch loss: 21.927339553833008\n",
      "epoch: 211, batch loss: 23.031389792760212\n",
      "epoch: 212, batch loss: 22.508641242980957\n",
      "epoch: 213, batch loss: 22.21641143163045\n",
      "epoch: 214, batch loss: 22.144222418467205\n",
      "epoch: 215, batch loss: 21.61653272310893\n",
      "epoch: 216, batch loss: 21.53906551996867\n",
      "epoch: 217, batch loss: 22.366915464401245\n",
      "epoch: 218, batch loss: 21.579118887583416\n",
      "epoch: 219, batch loss: 21.497614860534668\n",
      "epoch: 220, batch loss: 21.31636881828308\n",
      "epoch: 221, batch loss: 22.019659916559856\n",
      "epoch: 222, batch loss: 21.884536902109783\n",
      "epoch: 223, batch loss: 21.454140504201252\n",
      "epoch: 224, batch loss: 22.401992956797283\n",
      "epoch: 225, batch loss: 21.13294283548991\n",
      "epoch: 226, batch loss: 22.187517166137695\n",
      "epoch: 227, batch loss: 21.21580219268799\n",
      "epoch: 228, batch loss: 21.7391410668691\n",
      "epoch: 229, batch loss: 21.111177762349445\n",
      "epoch: 230, batch loss: 20.77868906656901\n",
      "epoch: 231, batch loss: 21.29970375696818\n",
      "epoch: 232, batch loss: 21.676406542460125\n",
      "epoch: 233, batch loss: 22.01938470204671\n",
      "epoch: 234, batch loss: 20.97450304031372\n",
      "epoch: 235, batch loss: 21.249056657155354\n",
      "epoch: 236, batch loss: 20.9643398920695\n",
      "epoch: 237, batch loss: 21.07642126083374\n",
      "epoch: 238, batch loss: 21.117867469787598\n",
      "epoch: 239, batch loss: 21.98417870203654\n",
      "epoch: 240, batch loss: 20.81700611114502\n",
      "epoch: 241, batch loss: 22.168885628382366\n",
      "epoch: 242, batch loss: 21.260045925776165\n",
      "epoch: 243, batch loss: 21.281271537144978\n",
      "epoch: 244, batch loss: 21.509243329366047\n",
      "epoch: 245, batch loss: 20.71762712796529\n",
      "epoch: 246, batch loss: 21.35012372334798\n",
      "epoch: 247, batch loss: 21.316265265146892\n",
      "epoch: 248, batch loss: 21.352778911590576\n",
      "epoch: 249, batch loss: 21.510238965352375\n",
      "epoch: 250, batch loss: 21.305813471476238\n",
      "epoch: 251, batch loss: 20.946932951609295\n",
      "epoch: 252, batch loss: 20.89443286259969\n",
      "epoch: 253, batch loss: 21.045598189036053\n",
      "epoch: 254, batch loss: 20.737162748972576\n",
      "epoch: 255, batch loss: 21.145429929097492\n",
      "epoch: 256, batch loss: 20.939974943796795\n",
      "epoch: 257, batch loss: 21.650383631388348\n",
      "epoch: 258, batch loss: 20.95283778508504\n",
      "epoch: 259, batch loss: 21.289273103078205\n",
      "epoch: 260, batch loss: 21.111334244410198\n",
      "epoch: 261, batch loss: 20.93271239598592\n",
      "epoch: 262, batch loss: 20.581968943277996\n",
      "epoch: 263, batch loss: 21.522952636082966\n",
      "epoch: 264, batch loss: 20.792200565338135\n",
      "epoch: 265, batch loss: 21.10878626505534\n",
      "epoch: 266, batch loss: 20.748725573221844\n",
      "epoch: 267, batch loss: 20.589855670928955\n",
      "epoch: 268, batch loss: 21.24086920420329\n",
      "epoch: 269, batch loss: 20.587392727533977\n",
      "epoch: 270, batch loss: 20.580241362253826\n",
      "epoch: 271, batch loss: 20.190977255503338\n",
      "epoch: 272, batch loss: 20.261948664983112\n",
      "epoch: 273, batch loss: 21.342813968658447\n",
      "epoch: 274, batch loss: 21.06917150815328\n",
      "epoch: 275, batch loss: 20.619211196899414\n",
      "epoch: 276, batch loss: 20.75176731745402\n",
      "epoch: 277, batch loss: 20.26893703142802\n",
      "epoch: 278, batch loss: 20.631709893544514\n",
      "epoch: 279, batch loss: 21.173481782277424\n",
      "epoch: 280, batch loss: 20.75465154647827\n",
      "epoch: 281, batch loss: 20.674800793329876\n",
      "epoch: 282, batch loss: 20.659441312154133\n",
      "epoch: 283, batch loss: 20.1164554754893\n",
      "epoch: 284, batch loss: 21.04466183980306\n",
      "epoch: 285, batch loss: 21.509052356084187\n",
      "epoch: 286, batch loss: 20.92477337519328\n",
      "epoch: 287, batch loss: 20.391624371210735\n",
      "epoch: 288, batch loss: 20.479036966959637\n",
      "epoch: 289, batch loss: 20.282529592514038\n",
      "epoch: 290, batch loss: 21.44254477818807\n",
      "epoch: 291, batch loss: 20.371603965759277\n",
      "epoch: 292, batch loss: 20.842328945795696\n",
      "epoch: 293, batch loss: 20.1803556283315\n",
      "epoch: 294, batch loss: 20.239819208780926\n",
      "epoch: 295, batch loss: 20.309789180755615\n",
      "epoch: 296, batch loss: 20.2028493086497\n",
      "epoch: 297, batch loss: 20.481080849965412\n",
      "epoch: 298, batch loss: 20.37409496307373\n",
      "epoch: 299, batch loss: 20.195798953374226\n",
      "epoch: 300, batch loss: 20.574302832285564\n",
      "epoch: 1, batch loss: 773.4139048258463\n",
      "epoch: 2, batch loss: 771.6771901448568\n",
      "epoch: 3, batch loss: 772.8517913818359\n",
      "epoch: 4, batch loss: 764.5731099446615\n",
      "epoch: 5, batch loss: 753.8866577148438\n",
      "epoch: 6, batch loss: 755.2098490397135\n",
      "epoch: 7, batch loss: 738.5423278808594\n",
      "epoch: 8, batch loss: 727.029042561849\n",
      "epoch: 9, batch loss: 704.0482737223307\n",
      "epoch: 10, batch loss: 678.5304667154948\n",
      "epoch: 11, batch loss: 642.3328399658203\n",
      "epoch: 12, batch loss: 611.9644470214844\n",
      "epoch: 13, batch loss: 562.9241129557291\n",
      "epoch: 14, batch loss: 507.5378011067708\n",
      "epoch: 15, batch loss: 439.5828679402669\n",
      "epoch: 16, batch loss: 372.732666015625\n",
      "epoch: 17, batch loss: 310.7928873697917\n",
      "epoch: 18, batch loss: 244.7509282430013\n",
      "epoch: 19, batch loss: 195.48909632364908\n",
      "epoch: 20, batch loss: 152.12704594930014\n",
      "epoch: 21, batch loss: 120.16758155822754\n",
      "epoch: 22, batch loss: 100.34913571675618\n",
      "epoch: 23, batch loss: 88.77123260498047\n",
      "epoch: 24, batch loss: 77.09539890289307\n",
      "epoch: 25, batch loss: 72.37117513020833\n",
      "epoch: 26, batch loss: 67.0483627319336\n",
      "epoch: 27, batch loss: 66.10834407806396\n",
      "epoch: 28, batch loss: 67.74044577280681\n",
      "epoch: 29, batch loss: 59.66242059071859\n",
      "epoch: 30, batch loss: 59.356950441996254\n",
      "epoch: 31, batch loss: 56.10395526885986\n",
      "epoch: 32, batch loss: 54.612930615743004\n",
      "epoch: 33, batch loss: 53.82839838663737\n",
      "epoch: 34, batch loss: 52.371066093444824\n",
      "epoch: 35, batch loss: 51.56920369466146\n",
      "epoch: 36, batch loss: 50.87472375233968\n",
      "epoch: 37, batch loss: 49.957071940104164\n",
      "epoch: 38, batch loss: 49.9552640914917\n",
      "epoch: 39, batch loss: 48.2643035252889\n",
      "epoch: 40, batch loss: 45.83200136820475\n",
      "epoch: 41, batch loss: 45.29358180363973\n",
      "epoch: 42, batch loss: 45.46560859680176\n",
      "epoch: 43, batch loss: 45.32661819458008\n",
      "epoch: 44, batch loss: 44.24163659413656\n",
      "epoch: 45, batch loss: 44.08955446879069\n",
      "epoch: 46, batch loss: 43.60097519556681\n",
      "epoch: 47, batch loss: 45.79036966959635\n",
      "epoch: 48, batch loss: 43.95857000350952\n",
      "epoch: 49, batch loss: 41.57896280288696\n",
      "epoch: 50, batch loss: 40.49016237258911\n",
      "epoch: 51, batch loss: 40.06031211217245\n",
      "epoch: 52, batch loss: 39.50713920593262\n",
      "epoch: 53, batch loss: 40.04326311747233\n",
      "epoch: 54, batch loss: 40.19270102183024\n",
      "epoch: 55, batch loss: 38.90496714909872\n",
      "epoch: 56, batch loss: 38.99843740463257\n",
      "epoch: 57, batch loss: 38.37995926539103\n",
      "epoch: 58, batch loss: 38.48380088806152\n",
      "epoch: 59, batch loss: 36.90036265055338\n",
      "epoch: 60, batch loss: 36.70916493733724\n",
      "epoch: 61, batch loss: 35.64100742340088\n",
      "epoch: 62, batch loss: 35.638389905293785\n",
      "epoch: 63, batch loss: 35.273138682047524\n",
      "epoch: 64, batch loss: 35.06708780924479\n",
      "epoch: 65, batch loss: 37.4588106473287\n",
      "epoch: 66, batch loss: 34.04072666168213\n",
      "epoch: 67, batch loss: 34.34879223505656\n",
      "epoch: 68, batch loss: 34.527453581492104\n",
      "epoch: 69, batch loss: 33.715248107910156\n",
      "epoch: 70, batch loss: 32.83919429779053\n",
      "epoch: 71, batch loss: 33.115098317464195\n",
      "epoch: 72, batch loss: 34.301716804504395\n",
      "epoch: 73, batch loss: 32.17675590515137\n",
      "epoch: 74, batch loss: 33.641064961751304\n",
      "epoch: 75, batch loss: 32.65570036570231\n",
      "epoch: 76, batch loss: 31.404482046763103\n",
      "epoch: 77, batch loss: 31.315141201019287\n",
      "epoch: 78, batch loss: 30.64407555262248\n",
      "epoch: 79, batch loss: 32.386729876200356\n",
      "epoch: 80, batch loss: 31.526918093363445\n",
      "epoch: 81, batch loss: 30.61975606282552\n",
      "epoch: 82, batch loss: 30.456846396128338\n",
      "epoch: 83, batch loss: 29.497671763102215\n",
      "epoch: 84, batch loss: 29.17706799507141\n",
      "epoch: 85, batch loss: 29.491479873657227\n",
      "epoch: 86, batch loss: 28.986952145894367\n",
      "epoch: 87, batch loss: 29.200509548187256\n",
      "epoch: 88, batch loss: 30.268314361572266\n",
      "epoch: 89, batch loss: 30.010260264078777\n",
      "epoch: 90, batch loss: 28.526654720306396\n",
      "epoch: 91, batch loss: 28.249181906382244\n",
      "epoch: 92, batch loss: 28.43020550409953\n",
      "epoch: 93, batch loss: 27.997679869333904\n",
      "epoch: 94, batch loss: 27.86794439951579\n",
      "epoch: 95, batch loss: 28.614227135976154\n",
      "epoch: 96, batch loss: 28.956478436787922\n",
      "epoch: 97, batch loss: 28.226056734720867\n",
      "epoch: 98, batch loss: 27.079618612925213\n",
      "epoch: 99, batch loss: 27.78605016072591\n",
      "epoch: 100, batch loss: 26.916404247283936\n",
      "epoch: 101, batch loss: 29.20614258448283\n",
      "epoch: 102, batch loss: 27.38078037897746\n",
      "epoch: 103, batch loss: 27.365588823954266\n",
      "epoch: 104, batch loss: 27.56084458033244\n",
      "epoch: 105, batch loss: 26.683799743652344\n",
      "epoch: 106, batch loss: 27.31178903579712\n",
      "epoch: 107, batch loss: 25.908644835154217\n",
      "epoch: 108, batch loss: 26.016451517740887\n",
      "epoch: 109, batch loss: 26.297595500946045\n",
      "epoch: 110, batch loss: 26.969537893931072\n",
      "epoch: 111, batch loss: 25.91174538930257\n",
      "epoch: 112, batch loss: 26.277268250783283\n",
      "epoch: 113, batch loss: 25.442563772201538\n",
      "epoch: 114, batch loss: 26.399338563283283\n",
      "epoch: 115, batch loss: 26.091533819834392\n",
      "epoch: 116, batch loss: 25.36679490407308\n",
      "epoch: 117, batch loss: 26.082610607147217\n",
      "epoch: 118, batch loss: 25.632659594217937\n",
      "epoch: 119, batch loss: 25.062906742095947\n",
      "epoch: 120, batch loss: 25.32433255513509\n",
      "epoch: 121, batch loss: 25.444356123606365\n",
      "epoch: 122, batch loss: 27.2940092086792\n",
      "epoch: 123, batch loss: 25.289676666259766\n",
      "epoch: 124, batch loss: 24.419935544331867\n",
      "epoch: 125, batch loss: 24.742686589558918\n",
      "epoch: 126, batch loss: 24.491579055786133\n",
      "epoch: 127, batch loss: 24.419433116912842\n",
      "epoch: 128, batch loss: 25.703603267669678\n",
      "epoch: 129, batch loss: 24.55968205134074\n",
      "epoch: 130, batch loss: 24.723432064056396\n",
      "epoch: 131, batch loss: 25.126625696818035\n",
      "epoch: 132, batch loss: 24.745240688323975\n",
      "epoch: 133, batch loss: 24.613747119903564\n",
      "epoch: 134, batch loss: 24.52678680419922\n",
      "epoch: 135, batch loss: 24.50528113047282\n",
      "epoch: 136, batch loss: 23.541707197825115\n",
      "epoch: 137, batch loss: 24.416527271270752\n",
      "epoch: 138, batch loss: 24.268768310546875\n",
      "epoch: 139, batch loss: 24.475319862365723\n",
      "epoch: 140, batch loss: 23.92130486170451\n",
      "epoch: 141, batch loss: 23.65380350748698\n",
      "epoch: 142, batch loss: 23.448049704233807\n",
      "epoch: 143, batch loss: 24.135700225830078\n",
      "epoch: 144, batch loss: 24.95880937576294\n",
      "epoch: 145, batch loss: 23.365338643391926\n",
      "epoch: 146, batch loss: 23.818119366963703\n",
      "epoch: 147, batch loss: 23.173710823059082\n",
      "epoch: 148, batch loss: 23.241223335266113\n",
      "epoch: 149, batch loss: 22.722479899724323\n",
      "epoch: 150, batch loss: 23.06009562810262\n",
      "epoch: 151, batch loss: 22.820350011189777\n",
      "epoch: 152, batch loss: 23.377986590067547\n",
      "epoch: 153, batch loss: 23.012500127156574\n",
      "epoch: 154, batch loss: 23.167027791341145\n",
      "epoch: 155, batch loss: 22.766924381256104\n",
      "epoch: 156, batch loss: 22.625324726104736\n",
      "epoch: 157, batch loss: 23.29252854983012\n",
      "epoch: 158, batch loss: 22.366520166397095\n",
      "epoch: 159, batch loss: 23.69950803120931\n",
      "epoch: 160, batch loss: 22.527297655741375\n",
      "epoch: 161, batch loss: 22.43869400024414\n",
      "epoch: 162, batch loss: 22.582639694213867\n",
      "epoch: 163, batch loss: 23.036094347635906\n",
      "epoch: 164, batch loss: 22.67548926671346\n",
      "epoch: 165, batch loss: 23.846759001413982\n",
      "epoch: 166, batch loss: 22.947895844777424\n",
      "epoch: 167, batch loss: 22.430324872334797\n",
      "epoch: 168, batch loss: 21.95376451810201\n",
      "epoch: 169, batch loss: 23.124319553375244\n",
      "epoch: 170, batch loss: 21.842029492060345\n",
      "epoch: 171, batch loss: 23.090837081273396\n",
      "epoch: 172, batch loss: 22.567808151245117\n",
      "epoch: 173, batch loss: 22.50758949915568\n",
      "epoch: 174, batch loss: 22.17854642868042\n",
      "epoch: 175, batch loss: 22.651636282602947\n",
      "epoch: 176, batch loss: 21.93955659866333\n",
      "epoch: 177, batch loss: 22.177218596140545\n",
      "epoch: 178, batch loss: 22.466754754384358\n",
      "epoch: 179, batch loss: 22.211904684702557\n",
      "epoch: 180, batch loss: 22.657624800999958\n",
      "epoch: 181, batch loss: 21.641145547231037\n",
      "epoch: 182, batch loss: 22.175698121388752\n",
      "epoch: 183, batch loss: 21.917002995808918\n",
      "epoch: 184, batch loss: 22.008555332819622\n",
      "epoch: 185, batch loss: 21.88428481419881\n",
      "epoch: 186, batch loss: 22.18149201075236\n",
      "epoch: 187, batch loss: 21.959761063257854\n",
      "epoch: 188, batch loss: 21.84443251291911\n",
      "epoch: 189, batch loss: 21.489556392033894\n",
      "epoch: 190, batch loss: 22.271936575571697\n",
      "epoch: 191, batch loss: 22.438798745473225\n",
      "epoch: 192, batch loss: 21.62396987279256\n",
      "epoch: 193, batch loss: 21.4386727809906\n",
      "epoch: 194, batch loss: 21.91285999615987\n",
      "epoch: 195, batch loss: 21.5335480372111\n",
      "epoch: 196, batch loss: 21.325353145599365\n",
      "epoch: 197, batch loss: 21.950817267100017\n",
      "epoch: 198, batch loss: 21.447181701660156\n",
      "epoch: 199, batch loss: 22.174686272939045\n",
      "epoch: 200, batch loss: 21.88385597864787\n",
      "epoch: 201, batch loss: 21.395421504974365\n",
      "epoch: 202, batch loss: 21.25932741165161\n",
      "epoch: 203, batch loss: 21.752629280090332\n",
      "epoch: 204, batch loss: 21.486599445343018\n",
      "epoch: 205, batch loss: 21.231650431950886\n",
      "epoch: 206, batch loss: 22.191885073979694\n",
      "epoch: 207, batch loss: 21.642966588338215\n",
      "epoch: 208, batch loss: 21.10002859433492\n",
      "epoch: 209, batch loss: 21.80900494257609\n",
      "epoch: 210, batch loss: 21.297272364298504\n",
      "epoch: 211, batch loss: 21.235848506291706\n",
      "epoch: 212, batch loss: 20.95501446723938\n",
      "epoch: 213, batch loss: 21.397961139678955\n",
      "epoch: 214, batch loss: 22.144702275594074\n",
      "epoch: 215, batch loss: 21.4423508644104\n",
      "epoch: 216, batch loss: 21.783047596613567\n",
      "epoch: 217, batch loss: 21.310681343078613\n",
      "epoch: 218, batch loss: 21.056092103322346\n",
      "epoch: 219, batch loss: 22.251291433970135\n",
      "epoch: 220, batch loss: 21.100621382395428\n",
      "epoch: 221, batch loss: 21.25489377975464\n",
      "epoch: 222, batch loss: 22.134628852208454\n",
      "epoch: 223, batch loss: 21.229180971781414\n",
      "epoch: 224, batch loss: 21.391048669815063\n",
      "epoch: 225, batch loss: 21.109562397003174\n",
      "epoch: 226, batch loss: 21.461944262186687\n",
      "epoch: 227, batch loss: 21.31274724006653\n",
      "epoch: 228, batch loss: 20.605977455774944\n",
      "epoch: 229, batch loss: 21.028306643168133\n",
      "epoch: 230, batch loss: 21.201401074727375\n",
      "epoch: 231, batch loss: 21.67577250798543\n",
      "epoch: 232, batch loss: 20.803720951080322\n",
      "epoch: 233, batch loss: 20.794394493103027\n",
      "epoch: 234, batch loss: 22.202467918395996\n",
      "epoch: 235, batch loss: 21.261505285898846\n",
      "epoch: 236, batch loss: 21.04948314030965\n",
      "epoch: 237, batch loss: 20.536949237187702\n",
      "epoch: 238, batch loss: 21.064558108647663\n",
      "epoch: 239, batch loss: 21.159650961558025\n",
      "epoch: 240, batch loss: 20.717637300491333\n",
      "epoch: 241, batch loss: 20.871994972229004\n",
      "epoch: 242, batch loss: 20.839677492777508\n",
      "epoch: 243, batch loss: 20.689268906911213\n",
      "epoch: 244, batch loss: 20.933390299479168\n",
      "epoch: 245, batch loss: 20.602707386016846\n",
      "epoch: 246, batch loss: 20.965970039367676\n",
      "epoch: 247, batch loss: 20.910905838012695\n",
      "epoch: 248, batch loss: 20.870567162831623\n",
      "epoch: 249, batch loss: 21.09687900543213\n",
      "epoch: 250, batch loss: 20.33265773455302\n",
      "epoch: 251, batch loss: 20.8883003393809\n",
      "epoch: 252, batch loss: 20.965724150339764\n",
      "epoch: 253, batch loss: 20.820013840993244\n",
      "epoch: 254, batch loss: 21.063055515289307\n",
      "epoch: 255, batch loss: 21.42083517710368\n",
      "epoch: 256, batch loss: 20.8599747021993\n",
      "epoch: 257, batch loss: 20.303250948588055\n",
      "epoch: 258, batch loss: 21.06036599477132\n",
      "epoch: 259, batch loss: 20.969508330027264\n",
      "epoch: 260, batch loss: 20.645827293395996\n",
      "epoch: 261, batch loss: 20.725177605946858\n",
      "epoch: 262, batch loss: 20.645368417104084\n",
      "epoch: 263, batch loss: 20.322825113932293\n",
      "epoch: 264, batch loss: 20.37979046503703\n",
      "epoch: 265, batch loss: 20.799174626668293\n",
      "epoch: 266, batch loss: 20.478084484736126\n",
      "epoch: 267, batch loss: 20.270081599553425\n",
      "epoch: 268, batch loss: 20.271555026372273\n",
      "epoch: 269, batch loss: 20.494982878367107\n",
      "epoch: 270, batch loss: 20.54946517944336\n",
      "epoch: 271, batch loss: 21.15268357594808\n",
      "epoch: 272, batch loss: 20.090541044871014\n",
      "epoch: 273, batch loss: 20.85839859644572\n",
      "epoch: 274, batch loss: 20.371882279713947\n",
      "epoch: 275, batch loss: 20.366021156311035\n",
      "epoch: 276, batch loss: 20.527597268422443\n",
      "epoch: 277, batch loss: 20.5805451075236\n",
      "epoch: 278, batch loss: 20.97375710805257\n",
      "epoch: 279, batch loss: 20.609697182973225\n",
      "epoch: 280, batch loss: 21.20218261082967\n",
      "epoch: 281, batch loss: 20.473507563273113\n",
      "epoch: 282, batch loss: 20.402183691660564\n",
      "epoch: 283, batch loss: 19.94721245765686\n",
      "epoch: 284, batch loss: 20.745694716771443\n",
      "epoch: 285, batch loss: 20.301506519317627\n",
      "epoch: 286, batch loss: 21.52185360590617\n",
      "epoch: 287, batch loss: 20.65120283762614\n",
      "epoch: 288, batch loss: 20.190157334009807\n",
      "epoch: 289, batch loss: 20.616838932037354\n",
      "epoch: 290, batch loss: 20.26948134104411\n",
      "epoch: 291, batch loss: 20.018291155497234\n",
      "epoch: 292, batch loss: 20.34252182642619\n",
      "epoch: 293, batch loss: 20.20954656600952\n",
      "epoch: 294, batch loss: 20.918487230936687\n",
      "epoch: 295, batch loss: 20.078463474909466\n",
      "epoch: 296, batch loss: 19.849838654200237\n",
      "epoch: 297, batch loss: 19.88107379277547\n",
      "epoch: 298, batch loss: 19.76815915107727\n",
      "epoch: 299, batch loss: 20.38382665316264\n",
      "epoch: 300, batch loss: 21.882676601409912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22.74191952326789"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss = []\n",
    "for i in range(10):\n",
    "    test_loss.append(train_sbm())\n",
    "np.array(test_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, batch loss: 782.9950968424479\n",
      "epoch: 2, batch loss: 786.8428904215494\n",
      "epoch: 3, batch loss: 778.0479227701823\n",
      "epoch: 4, batch loss: 774.3045298258463\n",
      "epoch: 5, batch loss: 776.7972157796224\n",
      "epoch: 6, batch loss: 767.0718078613281\n",
      "epoch: 7, batch loss: 765.1491038004557\n",
      "epoch: 8, batch loss: 766.7789204915365\n",
      "epoch: 9, batch loss: 759.8102162679037\n",
      "epoch: 10, batch loss: 751.0029907226562\n",
      "epoch: 11, batch loss: 744.6490427652994\n",
      "epoch: 12, batch loss: 735.577891031901\n",
      "epoch: 13, batch loss: 723.8629862467448\n",
      "epoch: 14, batch loss: 715.0005391438802\n",
      "epoch: 15, batch loss: 696.0958607991537\n",
      "epoch: 16, batch loss: 680.8677164713541\n",
      "epoch: 17, batch loss: 657.8819122314453\n",
      "epoch: 18, batch loss: 626.6165873209635\n",
      "epoch: 19, batch loss: 596.5623474121094\n",
      "epoch: 20, batch loss: 562.5684534708658\n",
      "epoch: 21, batch loss: 531.9283981323242\n",
      "epoch: 22, batch loss: 485.682378133138\n",
      "epoch: 23, batch loss: 451.1513392130534\n",
      "epoch: 24, batch loss: 404.78247833251953\n",
      "epoch: 25, batch loss: 364.9516372680664\n",
      "epoch: 26, batch loss: 325.4882227579753\n",
      "epoch: 27, batch loss: 291.8824412027995\n",
      "epoch: 28, batch loss: 262.173033396403\n",
      "epoch: 29, batch loss: 231.89560826619467\n",
      "epoch: 30, batch loss: 207.72805150349936\n",
      "epoch: 31, batch loss: 185.43552907307944\n",
      "epoch: 32, batch loss: 166.61377716064453\n",
      "epoch: 33, batch loss: 147.79762649536133\n",
      "epoch: 34, batch loss: 135.97201919555664\n",
      "epoch: 35, batch loss: 121.75287755330403\n",
      "epoch: 36, batch loss: 113.22672462463379\n",
      "epoch: 37, batch loss: 101.75836118062337\n",
      "epoch: 38, batch loss: 94.98320007324219\n",
      "epoch: 39, batch loss: 86.24899164835612\n",
      "epoch: 40, batch loss: 82.5900084177653\n",
      "epoch: 41, batch loss: 75.99471982320149\n",
      "epoch: 42, batch loss: 70.85775534311931\n",
      "epoch: 43, batch loss: 69.28511174519856\n",
      "epoch: 44, batch loss: 65.52659543355306\n",
      "epoch: 45, batch loss: 62.81547705332438\n",
      "epoch: 46, batch loss: 59.416763623555504\n",
      "epoch: 47, batch loss: 56.93015480041504\n",
      "epoch: 48, batch loss: 56.43711598714193\n",
      "epoch: 49, batch loss: 52.035212675730385\n",
      "epoch: 50, batch loss: 50.39929803212484\n",
      "epoch: 51, batch loss: 51.8007820447286\n",
      "epoch: 52, batch loss: 50.12693436940511\n",
      "epoch: 53, batch loss: 47.74338754018148\n",
      "epoch: 54, batch loss: 46.91043567657471\n",
      "epoch: 55, batch loss: 45.55684153238932\n",
      "epoch: 56, batch loss: 44.96555360158285\n",
      "epoch: 57, batch loss: 43.56480693817139\n",
      "epoch: 58, batch loss: 43.10315767923991\n",
      "epoch: 59, batch loss: 42.1152089436849\n",
      "epoch: 60, batch loss: 41.912927309672035\n",
      "epoch: 61, batch loss: 41.49592272440592\n",
      "epoch: 62, batch loss: 41.56769243876139\n",
      "epoch: 63, batch loss: 41.17468770345052\n",
      "epoch: 64, batch loss: 41.423348903656006\n",
      "epoch: 65, batch loss: 38.477942943573\n",
      "epoch: 66, batch loss: 39.20022932688395\n",
      "epoch: 67, batch loss: 37.83900817235311\n",
      "epoch: 68, batch loss: 39.19283358256022\n",
      "epoch: 69, batch loss: 37.77850023905436\n",
      "epoch: 70, batch loss: 39.095282554626465\n",
      "epoch: 71, batch loss: 35.804535230000816\n",
      "epoch: 72, batch loss: 36.3939216931661\n",
      "epoch: 73, batch loss: 35.87867943445841\n",
      "epoch: 74, batch loss: 35.727263768514\n",
      "epoch: 75, batch loss: 35.71466890970866\n",
      "epoch: 76, batch loss: 35.563093980153404\n",
      "epoch: 77, batch loss: 35.67486985524496\n",
      "epoch: 78, batch loss: 35.410794734954834\n",
      "epoch: 79, batch loss: 33.82773097356161\n",
      "epoch: 80, batch loss: 34.045431772867836\n",
      "epoch: 81, batch loss: 34.917012214660645\n",
      "epoch: 82, batch loss: 32.991427421569824\n",
      "epoch: 83, batch loss: 32.9728487332662\n",
      "epoch: 84, batch loss: 32.15303262074789\n",
      "epoch: 85, batch loss: 33.85942920049032\n",
      "epoch: 86, batch loss: 34.4523229598999\n",
      "epoch: 87, batch loss: 32.180590311686196\n",
      "epoch: 88, batch loss: 32.88112926483154\n",
      "epoch: 89, batch loss: 32.475734074910484\n",
      "epoch: 90, batch loss: 31.76508315404256\n",
      "epoch: 91, batch loss: 31.747631867726643\n",
      "epoch: 92, batch loss: 30.708661397298176\n",
      "epoch: 93, batch loss: 31.817033608754475\n",
      "epoch: 94, batch loss: 30.550124486287434\n",
      "epoch: 95, batch loss: 30.543165683746338\n",
      "epoch: 96, batch loss: 30.63230832417806\n",
      "epoch: 97, batch loss: 30.132573286692303\n",
      "epoch: 98, batch loss: 32.56767304738363\n",
      "epoch: 99, batch loss: 30.857022285461426\n",
      "epoch: 100, batch loss: 29.836786429087322\n",
      "epoch: 101, batch loss: 29.82440646489461\n",
      "epoch: 102, batch loss: 29.35297616322835\n",
      "epoch: 103, batch loss: 29.68447208404541\n",
      "epoch: 104, batch loss: 29.916215896606445\n",
      "epoch: 105, batch loss: 28.84025526046753\n",
      "epoch: 106, batch loss: 28.702147483825684\n",
      "epoch: 107, batch loss: 28.938847223917644\n",
      "epoch: 108, batch loss: 29.052016258239746\n",
      "epoch: 109, batch loss: 28.273640791575115\n",
      "epoch: 110, batch loss: 28.42339817682902\n",
      "epoch: 111, batch loss: 27.933497269948322\n",
      "epoch: 112, batch loss: 27.753828684488933\n",
      "epoch: 113, batch loss: 28.505571365356445\n",
      "epoch: 114, batch loss: 28.221797943115234\n",
      "epoch: 115, batch loss: 29.211263020833332\n",
      "epoch: 116, batch loss: 28.1103614171346\n",
      "epoch: 117, batch loss: 27.204556783040363\n",
      "epoch: 118, batch loss: 27.862102031707764\n",
      "epoch: 119, batch loss: 27.85919189453125\n",
      "epoch: 120, batch loss: 28.54731829961141\n",
      "epoch: 121, batch loss: 27.65146255493164\n",
      "epoch: 122, batch loss: 27.14763577779134\n",
      "epoch: 123, batch loss: 27.16035556793213\n",
      "epoch: 124, batch loss: 27.567462921142578\n",
      "epoch: 125, batch loss: 26.571330547332764\n",
      "epoch: 126, batch loss: 26.057966868082683\n",
      "epoch: 127, batch loss: 27.0392009417216\n",
      "epoch: 128, batch loss: 27.330332438151043\n",
      "epoch: 129, batch loss: 26.514950434366863\n",
      "epoch: 130, batch loss: 26.68148374557495\n",
      "epoch: 131, batch loss: 27.830329577128094\n",
      "epoch: 132, batch loss: 26.701302528381348\n",
      "epoch: 133, batch loss: 26.05889320373535\n",
      "epoch: 134, batch loss: 26.427289644877117\n",
      "epoch: 135, batch loss: 25.70167414347331\n",
      "epoch: 136, batch loss: 26.251753012339275\n",
      "epoch: 137, batch loss: 26.12774149576823\n",
      "epoch: 138, batch loss: 27.142693996429443\n",
      "epoch: 139, batch loss: 25.219809532165527\n",
      "epoch: 140, batch loss: 25.294873237609863\n",
      "epoch: 141, batch loss: 26.048529783884685\n",
      "epoch: 142, batch loss: 25.610961596171062\n",
      "epoch: 143, batch loss: 25.3720285097758\n",
      "epoch: 144, batch loss: 26.491631666819256\n",
      "epoch: 145, batch loss: 24.88557767868042\n",
      "epoch: 146, batch loss: 25.02021074295044\n",
      "epoch: 147, batch loss: 25.987258434295654\n",
      "epoch: 148, batch loss: 25.613025188446045\n",
      "epoch: 149, batch loss: 24.814945062001545\n",
      "epoch: 150, batch loss: 24.638906002044678\n",
      "epoch: 151, batch loss: 24.6620610555013\n",
      "epoch: 152, batch loss: 24.385971705118816\n",
      "epoch: 153, batch loss: 24.45624017715454\n",
      "epoch: 154, batch loss: 24.586517175038654\n",
      "epoch: 155, batch loss: 24.532891909281414\n",
      "epoch: 156, batch loss: 24.844861189524334\n",
      "epoch: 157, batch loss: 24.865341822306316\n",
      "epoch: 158, batch loss: 25.994410435358684\n",
      "epoch: 159, batch loss: 23.71088695526123\n",
      "epoch: 160, batch loss: 23.927073001861572\n",
      "epoch: 161, batch loss: 24.734428246815998\n",
      "epoch: 162, batch loss: 24.104716459910076\n",
      "epoch: 163, batch loss: 24.209603150685627\n",
      "epoch: 164, batch loss: 23.941833178202312\n",
      "epoch: 165, batch loss: 23.99873924255371\n",
      "epoch: 166, batch loss: 23.575069586435955\n",
      "epoch: 167, batch loss: 23.640299479166668\n",
      "epoch: 168, batch loss: 23.805743376413982\n",
      "epoch: 169, batch loss: 23.396350224812824\n",
      "epoch: 170, batch loss: 24.949265638987224\n",
      "epoch: 171, batch loss: 23.317781448364258\n",
      "epoch: 172, batch loss: 22.921315749486286\n",
      "epoch: 173, batch loss: 23.099125226338703\n",
      "epoch: 174, batch loss: 23.474284172058105\n",
      "epoch: 175, batch loss: 22.81044864654541\n",
      "epoch: 176, batch loss: 23.747915585835774\n",
      "epoch: 177, batch loss: 22.418047507603962\n",
      "epoch: 178, batch loss: 23.978182156880695\n",
      "epoch: 179, batch loss: 23.184139887491863\n",
      "epoch: 180, batch loss: 23.029077132542927\n",
      "epoch: 181, batch loss: 23.19846796989441\n",
      "epoch: 182, batch loss: 23.084145387013752\n",
      "epoch: 183, batch loss: 22.874236424763996\n",
      "epoch: 184, batch loss: 22.74257739384969\n",
      "epoch: 185, batch loss: 22.659624099731445\n",
      "epoch: 186, batch loss: 22.38788851102193\n",
      "epoch: 187, batch loss: 23.036682605743408\n",
      "epoch: 188, batch loss: 22.27235730489095\n",
      "epoch: 189, batch loss: 22.6968199412028\n",
      "epoch: 190, batch loss: 22.319642384847004\n",
      "epoch: 191, batch loss: 22.276392777760822\n",
      "epoch: 192, batch loss: 22.544934193293255\n",
      "epoch: 193, batch loss: 23.428852399190266\n",
      "epoch: 194, batch loss: 22.774794101715088\n",
      "epoch: 195, batch loss: 21.985035339991253\n",
      "epoch: 196, batch loss: 23.558900833129883\n",
      "epoch: 197, batch loss: 21.694166978200276\n",
      "epoch: 198, batch loss: 22.13290349642436\n",
      "epoch: 199, batch loss: 22.12354342142741\n",
      "epoch: 200, batch loss: 21.807213068008423\n",
      "epoch: 201, batch loss: 22.38357337315877\n",
      "epoch: 202, batch loss: 21.957655429840088\n",
      "epoch: 203, batch loss: 22.11344575881958\n",
      "epoch: 204, batch loss: 22.458503405253094\n",
      "epoch: 205, batch loss: 22.643791516621906\n",
      "epoch: 206, batch loss: 21.766554196675617\n",
      "epoch: 207, batch loss: 22.427433649698894\n",
      "epoch: 208, batch loss: 21.30372428894043\n",
      "epoch: 209, batch loss: 21.71054967244466\n",
      "epoch: 210, batch loss: 22.250155846277874\n",
      "epoch: 211, batch loss: 21.80499537785848\n",
      "epoch: 212, batch loss: 21.322049220403034\n",
      "epoch: 213, batch loss: 21.61315409342448\n",
      "epoch: 214, batch loss: 21.79951000213623\n",
      "epoch: 215, batch loss: 21.99009831746419\n",
      "epoch: 216, batch loss: 21.175939639409382\n",
      "epoch: 217, batch loss: 21.307707627614338\n",
      "epoch: 218, batch loss: 21.778348445892334\n",
      "epoch: 219, batch loss: 20.90111295382182\n",
      "epoch: 220, batch loss: 21.431506156921387\n",
      "epoch: 221, batch loss: 21.493760744730633\n",
      "epoch: 222, batch loss: 20.871079842249554\n",
      "epoch: 223, batch loss: 20.802569548288982\n",
      "epoch: 224, batch loss: 21.19294277826945\n",
      "epoch: 225, batch loss: 20.896981716156006\n",
      "epoch: 226, batch loss: 20.70375959078471\n",
      "epoch: 227, batch loss: 21.107155005137127\n",
      "epoch: 228, batch loss: 20.971151272455852\n",
      "epoch: 229, batch loss: 21.412012418111164\n",
      "epoch: 230, batch loss: 21.900534470876057\n",
      "epoch: 231, batch loss: 20.96197493871053\n",
      "epoch: 232, batch loss: 21.143019437789917\n",
      "epoch: 233, batch loss: 20.87804977099101\n",
      "epoch: 234, batch loss: 21.263798236846924\n",
      "epoch: 235, batch loss: 21.66542959213257\n",
      "epoch: 236, batch loss: 20.920219977696735\n",
      "epoch: 237, batch loss: 20.47545099258423\n",
      "epoch: 238, batch loss: 21.448287804921467\n",
      "epoch: 239, batch loss: 20.415371894836426\n",
      "epoch: 240, batch loss: 20.746703624725342\n",
      "epoch: 241, batch loss: 21.230731805165608\n",
      "epoch: 242, batch loss: 20.421034653981526\n",
      "epoch: 243, batch loss: 21.221760749816895\n",
      "epoch: 244, batch loss: 21.044982353846233\n",
      "epoch: 245, batch loss: 20.146511793136597\n",
      "epoch: 246, batch loss: 20.365914821624756\n",
      "epoch: 247, batch loss: 20.383601903915405\n",
      "epoch: 248, batch loss: 20.81140152613322\n",
      "epoch: 249, batch loss: 20.84278718630473\n",
      "epoch: 250, batch loss: 20.181600491205852\n",
      "epoch: 251, batch loss: 20.452051162719727\n",
      "epoch: 252, batch loss: 20.73493456840515\n",
      "epoch: 253, batch loss: 20.53820212682088\n",
      "epoch: 254, batch loss: 20.49450985590617\n",
      "epoch: 255, batch loss: 19.962868213653564\n",
      "epoch: 256, batch loss: 21.110029856363933\n",
      "epoch: 257, batch loss: 20.42607323328654\n",
      "epoch: 258, batch loss: 21.185871521631878\n",
      "epoch: 259, batch loss: 20.343885103861492\n",
      "epoch: 260, batch loss: 20.24174737930298\n",
      "epoch: 261, batch loss: 20.29839833577474\n",
      "epoch: 262, batch loss: 20.644802888234455\n",
      "epoch: 263, batch loss: 20.86915334065755\n",
      "epoch: 264, batch loss: 20.73135169347127\n",
      "epoch: 265, batch loss: 21.371834357579548\n",
      "epoch: 266, batch loss: 21.19587739308675\n",
      "epoch: 267, batch loss: 20.41015402475993\n",
      "epoch: 268, batch loss: 20.307864983876545\n",
      "epoch: 269, batch loss: 20.627689520517986\n",
      "epoch: 270, batch loss: 20.589774131774902\n",
      "epoch: 271, batch loss: 20.5144046942393\n",
      "epoch: 272, batch loss: 20.017837127049763\n",
      "epoch: 273, batch loss: 20.288134415944416\n",
      "epoch: 274, batch loss: 20.00095788637797\n",
      "epoch: 275, batch loss: 19.56033245722453\n",
      "epoch: 276, batch loss: 20.283135414123535\n",
      "epoch: 277, batch loss: 19.83081038792928\n",
      "epoch: 278, batch loss: 19.855211575826008\n",
      "epoch: 279, batch loss: 20.605982303619385\n",
      "epoch: 280, batch loss: 20.625922282536823\n",
      "epoch: 281, batch loss: 19.97424515088399\n",
      "epoch: 282, batch loss: 20.675191799799602\n",
      "epoch: 283, batch loss: 20.671566168467205\n",
      "epoch: 284, batch loss: 20.03840684890747\n",
      "epoch: 285, batch loss: 19.95589542388916\n",
      "epoch: 286, batch loss: 20.397471745808918\n",
      "epoch: 287, batch loss: 19.959504922231037\n",
      "epoch: 288, batch loss: 20.126065572102863\n",
      "epoch: 289, batch loss: 19.57545232772827\n",
      "epoch: 290, batch loss: 19.825159390767414\n",
      "epoch: 291, batch loss: 20.259573856989544\n",
      "epoch: 292, batch loss: 19.508565584818523\n",
      "epoch: 293, batch loss: 19.453360557556152\n",
      "epoch: 294, batch loss: 19.873204390207928\n",
      "epoch: 295, batch loss: 20.23132610321045\n",
      "epoch: 296, batch loss: 20.18838667869568\n",
      "epoch: 297, batch loss: 19.938300450642902\n",
      "epoch: 298, batch loss: 20.21178364753723\n",
      "epoch: 299, batch loss: 20.152770280838013\n",
      "epoch: 300, batch loss: 19.855085134506226\n",
      "epoch: 1, batch loss: 784.2441660563151\n",
      "epoch: 2, batch loss: 782.8372395833334\n",
      "epoch: 3, batch loss: 777.2279713948568\n",
      "epoch: 4, batch loss: 778.2420450846354\n",
      "epoch: 5, batch loss: 780.6093088785807\n",
      "epoch: 6, batch loss: 782.1968434651693\n",
      "epoch: 7, batch loss: 767.9363505045573\n",
      "epoch: 8, batch loss: 761.9826711018881\n",
      "epoch: 9, batch loss: 760.1423695882162\n",
      "epoch: 10, batch loss: 760.5512644449869\n",
      "epoch: 11, batch loss: 752.4075113932291\n",
      "epoch: 12, batch loss: 746.8538157145182\n",
      "epoch: 13, batch loss: 729.0446523030599\n",
      "epoch: 14, batch loss: 720.8265991210938\n",
      "epoch: 15, batch loss: 710.1542409261068\n",
      "epoch: 16, batch loss: 686.7173563639323\n",
      "epoch: 17, batch loss: 664.3669789632162\n",
      "epoch: 18, batch loss: 650.9170735677084\n",
      "epoch: 19, batch loss: 623.2050730387369\n",
      "epoch: 20, batch loss: 598.3486175537109\n",
      "epoch: 21, batch loss: 561.715576171875\n",
      "epoch: 22, batch loss: 533.6596883138021\n",
      "epoch: 23, batch loss: 491.08824157714844\n",
      "epoch: 24, batch loss: 457.2329355875651\n",
      "epoch: 25, batch loss: 417.20789591471356\n",
      "epoch: 26, batch loss: 384.891357421875\n",
      "epoch: 27, batch loss: 352.7776819864909\n",
      "epoch: 28, batch loss: 318.7216974894206\n",
      "epoch: 29, batch loss: 288.48197682698566\n",
      "epoch: 30, batch loss: 264.7043825785319\n",
      "epoch: 31, batch loss: 241.49026743570963\n",
      "epoch: 32, batch loss: 226.80089696248373\n",
      "epoch: 33, batch loss: 205.93054072062174\n",
      "epoch: 34, batch loss: 189.50867970784506\n",
      "epoch: 35, batch loss: 177.5607884724935\n",
      "epoch: 36, batch loss: 165.93716176350912\n",
      "epoch: 37, batch loss: 157.88887151082358\n",
      "epoch: 38, batch loss: 141.57406425476074\n",
      "epoch: 39, batch loss: 134.28414154052734\n",
      "epoch: 40, batch loss: 125.30436833699544\n",
      "epoch: 41, batch loss: 116.12994321187337\n",
      "epoch: 42, batch loss: 109.84980010986328\n",
      "epoch: 43, batch loss: 101.70787111918132\n",
      "epoch: 44, batch loss: 95.0053768157959\n",
      "epoch: 45, batch loss: 90.37653477986653\n",
      "epoch: 46, batch loss: 85.09146372477214\n",
      "epoch: 47, batch loss: 81.75014305114746\n",
      "epoch: 48, batch loss: 81.50866413116455\n",
      "epoch: 49, batch loss: 76.89089584350586\n",
      "epoch: 50, batch loss: 71.7274792989095\n",
      "epoch: 51, batch loss: 69.94855531056722\n",
      "epoch: 52, batch loss: 66.70198822021484\n",
      "epoch: 53, batch loss: 66.24501101175944\n",
      "epoch: 54, batch loss: 65.82962862650554\n",
      "epoch: 55, batch loss: 63.26237964630127\n",
      "epoch: 56, batch loss: 60.58485794067383\n",
      "epoch: 57, batch loss: 59.168491999308266\n",
      "epoch: 58, batch loss: 57.08471711476644\n",
      "epoch: 59, batch loss: 56.439789136250816\n",
      "epoch: 60, batch loss: 54.70298099517822\n",
      "epoch: 61, batch loss: 54.79946358998617\n",
      "epoch: 62, batch loss: 53.351316134134926\n",
      "epoch: 63, batch loss: 51.802084604899086\n",
      "epoch: 64, batch loss: 52.05908044179281\n",
      "epoch: 65, batch loss: 49.84196662902832\n",
      "epoch: 66, batch loss: 50.30895233154297\n",
      "epoch: 67, batch loss: 48.34479331970215\n",
      "epoch: 68, batch loss: 48.52015399932861\n",
      "epoch: 69, batch loss: 49.95610650380453\n",
      "epoch: 70, batch loss: 46.86714013417562\n",
      "epoch: 71, batch loss: 45.400388399759926\n",
      "epoch: 72, batch loss: 44.24748865763346\n",
      "epoch: 73, batch loss: 44.67916933695475\n",
      "epoch: 74, batch loss: 43.82986227671305\n",
      "epoch: 75, batch loss: 43.246240297953285\n",
      "epoch: 76, batch loss: 42.89684581756592\n",
      "epoch: 77, batch loss: 41.79473749796549\n",
      "epoch: 78, batch loss: 41.31095536549886\n",
      "epoch: 79, batch loss: 39.53691864013672\n",
      "epoch: 80, batch loss: 39.84312995274862\n",
      "epoch: 81, batch loss: 39.861576875050865\n",
      "epoch: 82, batch loss: 39.22356923421224\n",
      "epoch: 83, batch loss: 37.77933422724406\n",
      "epoch: 84, batch loss: 37.76240873336792\n",
      "epoch: 85, batch loss: 37.30702257156372\n",
      "epoch: 86, batch loss: 36.87839571634928\n",
      "epoch: 87, batch loss: 37.401450634002686\n",
      "epoch: 88, batch loss: 35.893041133880615\n",
      "epoch: 89, batch loss: 35.797189712524414\n",
      "epoch: 90, batch loss: 35.438324292500816\n",
      "epoch: 91, batch loss: 35.314998626708984\n",
      "epoch: 92, batch loss: 34.93006992340088\n",
      "epoch: 93, batch loss: 34.62187878290812\n",
      "epoch: 94, batch loss: 35.19464651743571\n",
      "epoch: 95, batch loss: 36.6802183787028\n",
      "epoch: 96, batch loss: 33.469178676605225\n",
      "epoch: 97, batch loss: 33.21901114781698\n",
      "epoch: 98, batch loss: 33.53378883997599\n",
      "epoch: 99, batch loss: 33.00560172398885\n",
      "epoch: 100, batch loss: 32.75425926844279\n",
      "epoch: 101, batch loss: 32.29249016443888\n",
      "epoch: 102, batch loss: 32.063896338144936\n",
      "epoch: 103, batch loss: 31.42849890391032\n",
      "epoch: 104, batch loss: 31.66062132517497\n",
      "epoch: 105, batch loss: 31.27093807856242\n",
      "epoch: 106, batch loss: 31.094340642293293\n",
      "epoch: 107, batch loss: 31.548954168955486\n",
      "epoch: 108, batch loss: 30.176061153411865\n",
      "epoch: 109, batch loss: 30.524963061014812\n",
      "epoch: 110, batch loss: 30.10203981399536\n",
      "epoch: 111, batch loss: 29.545223077138264\n",
      "epoch: 112, batch loss: 29.435626983642578\n",
      "epoch: 113, batch loss: 29.90632979075114\n",
      "epoch: 114, batch loss: 29.380680084228516\n",
      "epoch: 115, batch loss: 30.155040105183918\n",
      "epoch: 116, batch loss: 29.158267498016357\n",
      "epoch: 117, batch loss: 30.448342005411785\n",
      "epoch: 118, batch loss: 28.964245001475017\n",
      "epoch: 119, batch loss: 29.029810269673664\n",
      "epoch: 120, batch loss: 29.576972007751465\n",
      "epoch: 121, batch loss: 28.140015761057537\n",
      "epoch: 122, batch loss: 28.61927143732707\n",
      "epoch: 123, batch loss: 28.2185796101888\n",
      "epoch: 124, batch loss: 27.778276920318604\n",
      "epoch: 125, batch loss: 27.251668850580852\n",
      "epoch: 126, batch loss: 27.691806475321453\n",
      "epoch: 127, batch loss: 27.748610019683838\n",
      "epoch: 128, batch loss: 27.193931420644123\n",
      "epoch: 129, batch loss: 27.80936845143636\n",
      "epoch: 130, batch loss: 26.73983144760132\n",
      "epoch: 131, batch loss: 27.467410882314045\n",
      "epoch: 132, batch loss: 26.80481417973836\n",
      "epoch: 133, batch loss: 26.699793974558514\n",
      "epoch: 134, batch loss: 26.298949877421062\n",
      "epoch: 135, batch loss: 26.6778400739034\n",
      "epoch: 136, batch loss: 26.762189388275146\n",
      "epoch: 137, batch loss: 26.094890276590984\n",
      "epoch: 138, batch loss: 26.274652004241943\n",
      "epoch: 139, batch loss: 26.43218533198039\n",
      "epoch: 140, batch loss: 25.777341683705647\n",
      "epoch: 141, batch loss: 25.835983276367188\n",
      "epoch: 142, batch loss: 25.726492881774902\n",
      "epoch: 143, batch loss: 26.03312063217163\n",
      "epoch: 144, batch loss: 26.42520761489868\n",
      "epoch: 145, batch loss: 26.147327582041424\n",
      "epoch: 146, batch loss: 25.615611394246418\n",
      "epoch: 147, batch loss: 25.336426734924316\n",
      "epoch: 148, batch loss: 24.814086437225342\n",
      "epoch: 149, batch loss: 24.882449467976887\n",
      "epoch: 150, batch loss: 24.829796473185223\n",
      "epoch: 151, batch loss: 25.39699347813924\n",
      "epoch: 152, batch loss: 25.08022912343343\n",
      "epoch: 153, batch loss: 25.104872941970825\n",
      "epoch: 154, batch loss: 25.559226671854656\n",
      "epoch: 155, batch loss: 25.323265552520752\n",
      "epoch: 156, batch loss: 24.645630677541096\n",
      "epoch: 157, batch loss: 25.9108460744222\n",
      "epoch: 158, batch loss: 24.01923394203186\n",
      "epoch: 159, batch loss: 24.29439576466878\n",
      "epoch: 160, batch loss: 24.557738145192463\n",
      "epoch: 161, batch loss: 23.703179677327473\n",
      "epoch: 162, batch loss: 24.47403081258138\n",
      "epoch: 163, batch loss: 24.768709182739258\n",
      "epoch: 164, batch loss: 24.459436734517414\n",
      "epoch: 165, batch loss: 23.946212927500408\n",
      "epoch: 166, batch loss: 23.63217043876648\n",
      "epoch: 167, batch loss: 24.67421531677246\n",
      "epoch: 168, batch loss: 23.77665630976359\n",
      "epoch: 169, batch loss: 24.322930335998535\n",
      "epoch: 170, batch loss: 23.618130842844646\n",
      "epoch: 171, batch loss: 24.077106714248657\n",
      "epoch: 172, batch loss: 25.276443004608154\n",
      "epoch: 173, batch loss: 23.11557682355245\n",
      "epoch: 174, batch loss: 23.095332543055218\n",
      "epoch: 175, batch loss: 22.899854739507038\n",
      "epoch: 176, batch loss: 25.03018029530843\n",
      "epoch: 177, batch loss: 23.199488798777264\n",
      "epoch: 178, batch loss: 24.2314879099528\n",
      "epoch: 179, batch loss: 24.070116360982258\n",
      "epoch: 180, batch loss: 23.549617131551106\n",
      "epoch: 181, batch loss: 22.801177740097046\n",
      "epoch: 182, batch loss: 23.031002362569172\n",
      "epoch: 183, batch loss: 22.882895787556965\n",
      "epoch: 184, batch loss: 22.69860537846883\n",
      "epoch: 185, batch loss: 23.058430353800457\n",
      "epoch: 186, batch loss: 23.255009333292644\n",
      "epoch: 187, batch loss: 22.50893457730611\n",
      "epoch: 188, batch loss: 23.154000918070476\n",
      "epoch: 189, batch loss: 22.653120676676433\n",
      "epoch: 190, batch loss: 22.20743719736735\n",
      "epoch: 191, batch loss: 22.74683078130086\n",
      "epoch: 192, batch loss: 22.864914019902546\n",
      "epoch: 193, batch loss: 22.345009803771973\n",
      "epoch: 194, batch loss: 23.138916333516438\n",
      "epoch: 195, batch loss: 22.64902098973592\n",
      "epoch: 196, batch loss: 22.336057901382446\n",
      "epoch: 197, batch loss: 22.105103651682537\n",
      "epoch: 198, batch loss: 22.97320493062337\n",
      "epoch: 199, batch loss: 23.229816436767578\n",
      "epoch: 200, batch loss: 22.05094854036967\n",
      "epoch: 201, batch loss: 22.387304147084553\n",
      "epoch: 202, batch loss: 23.06588403383891\n",
      "epoch: 203, batch loss: 23.370208740234375\n",
      "epoch: 204, batch loss: 22.089918931325276\n",
      "epoch: 205, batch loss: 22.29760233561198\n",
      "epoch: 206, batch loss: 21.66310707728068\n",
      "epoch: 207, batch loss: 21.771108388900757\n",
      "epoch: 208, batch loss: 21.894647757212322\n",
      "epoch: 209, batch loss: 21.41919740041097\n",
      "epoch: 210, batch loss: 21.638215859731037\n",
      "epoch: 211, batch loss: 21.826212803522747\n",
      "epoch: 212, batch loss: 21.258243242899578\n",
      "epoch: 213, batch loss: 21.48200559616089\n",
      "epoch: 214, batch loss: 21.94159444173177\n",
      "epoch: 215, batch loss: 22.62992302576701\n",
      "epoch: 216, batch loss: 21.98830223083496\n",
      "epoch: 217, batch loss: 21.58495370546977\n",
      "epoch: 218, batch loss: 21.850073019663494\n",
      "epoch: 219, batch loss: 21.31295124689738\n",
      "epoch: 220, batch loss: 21.052339951197307\n",
      "epoch: 221, batch loss: 21.88227637608846\n",
      "epoch: 222, batch loss: 21.35814340909322\n",
      "epoch: 223, batch loss: 21.33184226353963\n",
      "epoch: 224, batch loss: 20.80372953414917\n",
      "epoch: 225, batch loss: 21.836697657903034\n",
      "epoch: 226, batch loss: 21.3849360148112\n",
      "epoch: 227, batch loss: 21.002323627471924\n",
      "epoch: 228, batch loss: 21.38628276189168\n",
      "epoch: 229, batch loss: 21.7615327835083\n",
      "epoch: 230, batch loss: 22.114437739054363\n",
      "epoch: 231, batch loss: 20.794822772343952\n",
      "epoch: 232, batch loss: 20.63830558458964\n",
      "epoch: 233, batch loss: 20.72271164258321\n",
      "epoch: 234, batch loss: 21.547109127044678\n",
      "epoch: 235, batch loss: 20.578192710876465\n",
      "epoch: 236, batch loss: 21.681700547536213\n",
      "epoch: 237, batch loss: 20.605436325073242\n",
      "epoch: 238, batch loss: 20.77698850631714\n",
      "epoch: 239, batch loss: 21.32202426592509\n",
      "epoch: 240, batch loss: 20.65095321337382\n",
      "epoch: 241, batch loss: 21.3142565091451\n",
      "epoch: 242, batch loss: 20.964306036631267\n",
      "epoch: 243, batch loss: 21.109771410624187\n",
      "epoch: 244, batch loss: 20.765671809514362\n",
      "epoch: 245, batch loss: 20.609870195388794\n",
      "epoch: 246, batch loss: 21.096055905024212\n",
      "epoch: 247, batch loss: 20.81329846382141\n",
      "epoch: 248, batch loss: 20.9471534093221\n",
      "epoch: 249, batch loss: 20.810309410095215\n",
      "epoch: 250, batch loss: 20.79559548695882\n",
      "epoch: 251, batch loss: 20.804779688517254\n",
      "epoch: 252, batch loss: 20.404284954071045\n",
      "epoch: 253, batch loss: 20.296035051345825\n",
      "epoch: 254, batch loss: 20.731311957041424\n",
      "epoch: 255, batch loss: 20.470094362894695\n",
      "epoch: 256, batch loss: 20.161518573760986\n",
      "epoch: 257, batch loss: 20.53017743428548\n",
      "epoch: 258, batch loss: 20.689505338668823\n",
      "epoch: 259, batch loss: 20.921584447224934\n",
      "epoch: 260, batch loss: 20.65632661183675\n",
      "epoch: 261, batch loss: 20.368083238601685\n",
      "epoch: 262, batch loss: 20.659772713979084\n",
      "epoch: 263, batch loss: 20.41658083597819\n",
      "epoch: 264, batch loss: 21.063779433568318\n",
      "epoch: 265, batch loss: 20.451698621114094\n",
      "epoch: 266, batch loss: 21.135133425394695\n",
      "epoch: 267, batch loss: 19.95850419998169\n",
      "epoch: 268, batch loss: 20.22172498703003\n",
      "epoch: 269, batch loss: 20.074451605478924\n",
      "epoch: 270, batch loss: 20.12498680750529\n",
      "epoch: 271, batch loss: 20.703216234842937\n",
      "epoch: 272, batch loss: 21.014529546101887\n",
      "epoch: 273, batch loss: 20.490323384602863\n",
      "epoch: 274, batch loss: 20.19042722384135\n",
      "epoch: 275, batch loss: 20.334887663523357\n",
      "epoch: 276, batch loss: 19.789115349451702\n",
      "epoch: 277, batch loss: 20.200635989507038\n",
      "epoch: 278, batch loss: 20.267871379852295\n",
      "epoch: 279, batch loss: 20.086559534072876\n",
      "epoch: 280, batch loss: 19.827582279841106\n",
      "epoch: 281, batch loss: 20.584567944208782\n",
      "epoch: 282, batch loss: 20.48510281244914\n",
      "epoch: 283, batch loss: 20.3886292775472\n",
      "epoch: 284, batch loss: 20.072731335957844\n",
      "epoch: 285, batch loss: 19.871885061264038\n",
      "epoch: 286, batch loss: 20.963653882344563\n",
      "epoch: 287, batch loss: 21.028925100962322\n",
      "epoch: 288, batch loss: 20.07885726292928\n",
      "epoch: 289, batch loss: 21.00529209772746\n",
      "epoch: 290, batch loss: 19.708658695220947\n",
      "epoch: 291, batch loss: 19.71381600697835\n",
      "epoch: 292, batch loss: 19.959264119466145\n",
      "epoch: 293, batch loss: 20.425489743550617\n",
      "epoch: 294, batch loss: 20.521703561147053\n",
      "epoch: 295, batch loss: 20.236559470494587\n",
      "epoch: 296, batch loss: 20.566630760828655\n",
      "epoch: 297, batch loss: 20.93542965253194\n",
      "epoch: 298, batch loss: 19.630571603775024\n",
      "epoch: 299, batch loss: 19.60423509279887\n",
      "epoch: 300, batch loss: 19.811955610911053\n",
      "epoch: 1, batch loss: 810.1375935872396\n",
      "epoch: 2, batch loss: 809.0566965738932\n",
      "epoch: 3, batch loss: 804.0098114013672\n",
      "epoch: 4, batch loss: 808.6702321370443\n",
      "epoch: 5, batch loss: 811.4777119954427\n",
      "epoch: 6, batch loss: 799.8810628255209\n",
      "epoch: 7, batch loss: 798.6039377848307\n",
      "epoch: 8, batch loss: 797.2419586181641\n",
      "epoch: 9, batch loss: 792.5967102050781\n",
      "epoch: 10, batch loss: 788.9094289143881\n",
      "epoch: 11, batch loss: 783.1731821695963\n",
      "epoch: 12, batch loss: 780.0793202718099\n",
      "epoch: 13, batch loss: 773.8209533691406\n",
      "epoch: 14, batch loss: 768.9908599853516\n",
      "epoch: 15, batch loss: 764.8636728922526\n",
      "epoch: 16, batch loss: 758.2477722167969\n",
      "epoch: 17, batch loss: 752.9600423177084\n",
      "epoch: 18, batch loss: 735.2357381184896\n",
      "epoch: 19, batch loss: 716.4022420247396\n",
      "epoch: 20, batch loss: 700.4504699707031\n",
      "epoch: 21, batch loss: 686.6990509033203\n",
      "epoch: 22, batch loss: 659.5586802164713\n",
      "epoch: 23, batch loss: 641.7590128580729\n",
      "epoch: 24, batch loss: 605.0941416422526\n",
      "epoch: 25, batch loss: 584.8177947998047\n",
      "epoch: 26, batch loss: 537.6215871175131\n",
      "epoch: 27, batch loss: 505.23499298095703\n",
      "epoch: 28, batch loss: 470.3766504923503\n",
      "epoch: 29, batch loss: 439.49084218343097\n",
      "epoch: 30, batch loss: 409.6935806274414\n",
      "epoch: 31, batch loss: 378.2297948201497\n",
      "epoch: 32, batch loss: 364.3687108357747\n",
      "epoch: 33, batch loss: 339.66182200113934\n",
      "epoch: 34, batch loss: 324.8151448567708\n",
      "epoch: 35, batch loss: 311.3030700683594\n",
      "epoch: 36, batch loss: 296.00880432128906\n",
      "epoch: 37, batch loss: 285.9830830891927\n",
      "epoch: 38, batch loss: 270.63327407836914\n",
      "epoch: 39, batch loss: 261.7080446879069\n",
      "epoch: 40, batch loss: 248.85311889648438\n",
      "epoch: 41, batch loss: 239.23893864949545\n",
      "epoch: 42, batch loss: 236.30537923177084\n",
      "epoch: 43, batch loss: 220.13301849365234\n",
      "epoch: 44, batch loss: 211.4727872212728\n",
      "epoch: 45, batch loss: 202.4933204650879\n",
      "epoch: 46, batch loss: 191.7833048502604\n",
      "epoch: 47, batch loss: 182.5676155090332\n",
      "epoch: 48, batch loss: 176.40804036458334\n",
      "epoch: 49, batch loss: 168.8448371887207\n",
      "epoch: 50, batch loss: 161.19760513305664\n",
      "epoch: 51, batch loss: 150.96588897705078\n",
      "epoch: 52, batch loss: 144.61078643798828\n",
      "epoch: 53, batch loss: 140.3101374308268\n",
      "epoch: 54, batch loss: 130.55842272440592\n",
      "epoch: 55, batch loss: 126.64147694905598\n",
      "epoch: 56, batch loss: 117.48740069071452\n",
      "epoch: 57, batch loss: 110.46997133890788\n",
      "epoch: 58, batch loss: 104.5251948038737\n",
      "epoch: 59, batch loss: 98.53304354349773\n",
      "epoch: 60, batch loss: 95.32289568583171\n",
      "epoch: 61, batch loss: 88.36538887023926\n",
      "epoch: 62, batch loss: 85.12732060750325\n",
      "epoch: 63, batch loss: 79.9151341120402\n",
      "epoch: 64, batch loss: 78.44783687591553\n",
      "epoch: 65, batch loss: 73.61150900522868\n",
      "epoch: 66, batch loss: 69.98917961120605\n",
      "epoch: 67, batch loss: 67.24443817138672\n",
      "epoch: 68, batch loss: 64.79909388224284\n",
      "epoch: 69, batch loss: 65.11682828267415\n",
      "epoch: 70, batch loss: 60.68431599934896\n",
      "epoch: 71, batch loss: 59.27331829071045\n",
      "epoch: 72, batch loss: 57.890350341796875\n",
      "epoch: 73, batch loss: 54.18884118398031\n",
      "epoch: 74, batch loss: 53.30415058135986\n",
      "epoch: 75, batch loss: 51.88849671681722\n",
      "epoch: 76, batch loss: 50.99731604258219\n",
      "epoch: 77, batch loss: 51.04808743794759\n",
      "epoch: 78, batch loss: 50.96112187703451\n",
      "epoch: 79, batch loss: 48.22883447011312\n",
      "epoch: 80, batch loss: 46.89995193481445\n",
      "epoch: 81, batch loss: 45.45588397979736\n",
      "epoch: 82, batch loss: 46.606239000956215\n",
      "epoch: 83, batch loss: 43.24050855636597\n",
      "epoch: 84, batch loss: 44.313206831614174\n",
      "epoch: 85, batch loss: 42.073673248291016\n",
      "epoch: 86, batch loss: 42.19452412923177\n",
      "epoch: 87, batch loss: 41.08104006449381\n",
      "epoch: 88, batch loss: 40.40552568435669\n",
      "epoch: 89, batch loss: 40.17329216003418\n",
      "epoch: 90, batch loss: 39.442135175069176\n",
      "epoch: 91, batch loss: 38.73432016372681\n",
      "epoch: 92, batch loss: 37.66166321436564\n",
      "epoch: 93, batch loss: 37.73293209075928\n",
      "epoch: 94, batch loss: 36.78770303726196\n",
      "epoch: 95, batch loss: 36.16661866505941\n",
      "epoch: 96, batch loss: 36.50020090738932\n",
      "epoch: 97, batch loss: 35.39532470703125\n",
      "epoch: 98, batch loss: 35.49535719553629\n",
      "epoch: 99, batch loss: 33.76669963200887\n",
      "epoch: 100, batch loss: 33.991992791493736\n",
      "epoch: 101, batch loss: 33.02867635091146\n",
      "epoch: 102, batch loss: 34.237670262654625\n",
      "epoch: 103, batch loss: 33.30492083231608\n",
      "epoch: 104, batch loss: 32.43630933761597\n",
      "epoch: 105, batch loss: 32.05007632573446\n",
      "epoch: 106, batch loss: 31.78604857126872\n",
      "epoch: 107, batch loss: 31.764573574066162\n",
      "epoch: 108, batch loss: 33.60963487625122\n",
      "epoch: 109, batch loss: 31.003098169962566\n",
      "epoch: 110, batch loss: 31.330105463663738\n",
      "epoch: 111, batch loss: 30.365616003672283\n",
      "epoch: 112, batch loss: 32.347451527913414\n",
      "epoch: 113, batch loss: 29.91383234659831\n",
      "epoch: 114, batch loss: 29.99653911590576\n",
      "epoch: 115, batch loss: 29.162341912587483\n",
      "epoch: 116, batch loss: 29.537994543711346\n",
      "epoch: 117, batch loss: 29.544178009033203\n",
      "epoch: 118, batch loss: 28.416292826334637\n",
      "epoch: 119, batch loss: 29.635095755259197\n",
      "epoch: 120, batch loss: 28.480009078979492\n",
      "epoch: 121, batch loss: 28.17545811335246\n",
      "epoch: 122, batch loss: 27.87694485982259\n",
      "epoch: 123, batch loss: 28.509746233622234\n",
      "epoch: 124, batch loss: 27.93607298533122\n",
      "epoch: 125, batch loss: 27.242647965749104\n",
      "epoch: 126, batch loss: 28.406298637390137\n",
      "epoch: 127, batch loss: 28.319724559783936\n",
      "epoch: 128, batch loss: 27.11725918451945\n",
      "epoch: 129, batch loss: 27.01763455073039\n",
      "epoch: 130, batch loss: 28.354344844818115\n",
      "epoch: 131, batch loss: 26.309922854105633\n",
      "epoch: 132, batch loss: 26.34576400121053\n",
      "epoch: 133, batch loss: 26.34521468480428\n",
      "epoch: 134, batch loss: 25.50861104329427\n",
      "epoch: 135, batch loss: 26.21388578414917\n",
      "epoch: 136, batch loss: 25.81842803955078\n",
      "epoch: 137, batch loss: 25.980310599009197\n",
      "epoch: 138, batch loss: 25.13390239079793\n",
      "epoch: 139, batch loss: 25.440434137980144\n",
      "epoch: 140, batch loss: 26.170122305552166\n",
      "epoch: 141, batch loss: 25.395950317382812\n",
      "epoch: 142, batch loss: 25.03060022989909\n",
      "epoch: 143, batch loss: 25.436418533325195\n",
      "epoch: 144, batch loss: 24.417867422103882\n",
      "epoch: 145, batch loss: 24.649202823638916\n",
      "epoch: 146, batch loss: 24.71366548538208\n",
      "epoch: 147, batch loss: 24.48941946029663\n",
      "epoch: 148, batch loss: 25.464921474456787\n",
      "epoch: 149, batch loss: 24.300543149312336\n",
      "epoch: 150, batch loss: 24.130241870880127\n",
      "epoch: 151, batch loss: 24.750714937845867\n",
      "epoch: 152, batch loss: 24.6305734316508\n",
      "epoch: 153, batch loss: 26.233908971150715\n",
      "epoch: 154, batch loss: 24.28578758239746\n",
      "epoch: 155, batch loss: 23.80917501449585\n",
      "epoch: 156, batch loss: 23.54963191350301\n",
      "epoch: 157, batch loss: 23.767178058624268\n",
      "epoch: 158, batch loss: 24.276578267415363\n",
      "epoch: 159, batch loss: 24.26102113723755\n",
      "epoch: 160, batch loss: 23.6689133644104\n",
      "epoch: 161, batch loss: 23.35486936569214\n",
      "epoch: 162, batch loss: 22.976123968760174\n",
      "epoch: 163, batch loss: 22.75882323582967\n",
      "epoch: 164, batch loss: 23.386255900065105\n",
      "epoch: 165, batch loss: 23.210163911183674\n",
      "epoch: 166, batch loss: 22.834222952524822\n",
      "epoch: 167, batch loss: 23.18005673090617\n",
      "epoch: 168, batch loss: 22.75943883260091\n",
      "epoch: 169, batch loss: 23.65403668085734\n",
      "epoch: 170, batch loss: 22.66780122121175\n",
      "epoch: 171, batch loss: 22.382310549418133\n",
      "epoch: 172, batch loss: 22.64157239596049\n",
      "epoch: 173, batch loss: 22.96032969156901\n",
      "epoch: 174, batch loss: 23.514641125996906\n",
      "epoch: 175, batch loss: 22.82376527786255\n",
      "epoch: 176, batch loss: 22.860891660054524\n",
      "epoch: 177, batch loss: 22.679903666178387\n",
      "epoch: 178, batch loss: 22.412162224451702\n",
      "epoch: 179, batch loss: 22.70858971277873\n",
      "epoch: 180, batch loss: 23.24050172170003\n",
      "epoch: 181, batch loss: 22.28370475769043\n",
      "epoch: 182, batch loss: 22.537102699279785\n",
      "epoch: 183, batch loss: 22.258582433064777\n",
      "epoch: 184, batch loss: 22.095428148905437\n",
      "epoch: 185, batch loss: 21.856937726338703\n",
      "epoch: 186, batch loss: 22.599535306294758\n",
      "epoch: 187, batch loss: 22.24651352564494\n",
      "epoch: 188, batch loss: 22.765151500701904\n",
      "epoch: 189, batch loss: 22.190581957499187\n",
      "epoch: 190, batch loss: 21.869385480880737\n",
      "epoch: 191, batch loss: 22.070534149805706\n",
      "epoch: 192, batch loss: 22.11663071314494\n",
      "epoch: 193, batch loss: 22.473920504252117\n",
      "epoch: 194, batch loss: 21.48599974314372\n",
      "epoch: 195, batch loss: 21.661062558492024\n",
      "epoch: 196, batch loss: 21.57000716527303\n",
      "epoch: 197, batch loss: 21.469149510065716\n",
      "epoch: 198, batch loss: 21.96294395128886\n",
      "epoch: 199, batch loss: 22.253921190897625\n",
      "epoch: 200, batch loss: 21.968149344126385\n",
      "epoch: 201, batch loss: 21.7252033551534\n",
      "epoch: 202, batch loss: 22.068374156951904\n",
      "epoch: 203, batch loss: 22.47675593694051\n",
      "epoch: 204, batch loss: 21.16461443901062\n",
      "epoch: 205, batch loss: 21.615853945414226\n",
      "epoch: 206, batch loss: 22.13997681935628\n",
      "epoch: 207, batch loss: 21.659059524536133\n",
      "epoch: 208, batch loss: 22.46700135866801\n",
      "epoch: 209, batch loss: 22.686254342397053\n",
      "epoch: 210, batch loss: 21.711177349090576\n",
      "epoch: 211, batch loss: 21.661869684855144\n",
      "epoch: 212, batch loss: 21.71937847137451\n",
      "epoch: 213, batch loss: 21.08342933654785\n",
      "epoch: 214, batch loss: 21.114482084910076\n",
      "epoch: 215, batch loss: 21.35625187555949\n",
      "epoch: 216, batch loss: 21.082762400309246\n",
      "epoch: 217, batch loss: 20.710854212443035\n",
      "epoch: 218, batch loss: 21.32114330927531\n",
      "epoch: 219, batch loss: 21.29224395751953\n",
      "epoch: 220, batch loss: 21.385254700978596\n",
      "epoch: 221, batch loss: 21.257938782374065\n",
      "epoch: 222, batch loss: 20.9103585879008\n",
      "epoch: 223, batch loss: 20.92594035466512\n",
      "epoch: 224, batch loss: 20.935001134872437\n",
      "epoch: 225, batch loss: 21.140780607859295\n",
      "epoch: 226, batch loss: 20.81689230600993\n",
      "epoch: 227, batch loss: 21.275989691416424\n",
      "epoch: 228, batch loss: 21.451056400934856\n",
      "epoch: 229, batch loss: 20.711093187332153\n",
      "epoch: 230, batch loss: 21.05080795288086\n",
      "epoch: 231, batch loss: 20.648760080337524\n",
      "epoch: 232, batch loss: 20.701606432596844\n",
      "epoch: 233, batch loss: 20.803808848063152\n",
      "epoch: 234, batch loss: 21.02446722984314\n",
      "epoch: 235, batch loss: 20.643428802490234\n",
      "epoch: 236, batch loss: 20.674157698949177\n",
      "epoch: 237, batch loss: 21.279297828674316\n",
      "epoch: 238, batch loss: 21.026558478673298\n",
      "epoch: 239, batch loss: 20.56789557139079\n",
      "epoch: 240, batch loss: 21.317091782887776\n",
      "epoch: 241, batch loss: 20.641307671864826\n",
      "epoch: 242, batch loss: 20.317740360895794\n",
      "epoch: 243, batch loss: 20.345066865285236\n",
      "epoch: 244, batch loss: 21.551047166188557\n",
      "epoch: 245, batch loss: 20.00743341445923\n",
      "epoch: 246, batch loss: 21.396227200826008\n",
      "epoch: 247, batch loss: 20.313042640686035\n",
      "epoch: 248, batch loss: 20.78377954165141\n",
      "epoch: 249, batch loss: 20.608187834421795\n",
      "epoch: 250, batch loss: 20.949605226516724\n",
      "epoch: 251, batch loss: 21.078035831451416\n",
      "epoch: 252, batch loss: 20.791582425435383\n",
      "epoch: 253, batch loss: 20.297704140345257\n",
      "epoch: 254, batch loss: 20.209646860758465\n",
      "epoch: 255, batch loss: 21.100448211034138\n",
      "epoch: 256, batch loss: 20.578105608622234\n",
      "epoch: 257, batch loss: 20.734270731608074\n",
      "epoch: 258, batch loss: 20.344050963719685\n",
      "epoch: 259, batch loss: 20.30344049135844\n",
      "epoch: 260, batch loss: 20.629093885421753\n",
      "epoch: 261, batch loss: 20.069992224375408\n",
      "epoch: 262, batch loss: 20.41246239344279\n",
      "epoch: 263, batch loss: 21.34655745824178\n",
      "epoch: 264, batch loss: 20.460181951522827\n",
      "epoch: 265, batch loss: 20.501931587855022\n",
      "epoch: 266, batch loss: 20.164122422536213\n",
      "epoch: 267, batch loss: 20.149186452229817\n",
      "epoch: 268, batch loss: 21.468223333358765\n",
      "epoch: 269, batch loss: 20.221563180287678\n",
      "epoch: 270, batch loss: 20.333024819691975\n",
      "epoch: 271, batch loss: 20.6744065284729\n",
      "epoch: 272, batch loss: 20.109771887461346\n",
      "epoch: 273, batch loss: 20.267115434010822\n",
      "epoch: 274, batch loss: 20.457687695821125\n",
      "epoch: 275, batch loss: 20.897325038909912\n",
      "epoch: 276, batch loss: 20.266737302144367\n",
      "epoch: 277, batch loss: 19.936699708302815\n",
      "epoch: 278, batch loss: 20.260735273361206\n",
      "epoch: 279, batch loss: 20.06495436032613\n",
      "epoch: 280, batch loss: 20.762127161026\n",
      "epoch: 281, batch loss: 20.2709481716156\n",
      "epoch: 282, batch loss: 20.135661045710247\n",
      "epoch: 283, batch loss: 20.3639710744222\n",
      "epoch: 284, batch loss: 19.708008845647175\n",
      "epoch: 285, batch loss: 19.57742206255595\n",
      "epoch: 286, batch loss: 19.670208136240642\n",
      "epoch: 287, batch loss: 20.099058866500854\n",
      "epoch: 288, batch loss: 20.65859357515971\n",
      "epoch: 289, batch loss: 19.917301336924236\n",
      "epoch: 290, batch loss: 20.049923181533813\n",
      "epoch: 291, batch loss: 19.55985911687215\n",
      "epoch: 292, batch loss: 19.81420048077901\n",
      "epoch: 293, batch loss: 20.064074675242107\n",
      "epoch: 294, batch loss: 19.69175934791565\n",
      "epoch: 295, batch loss: 20.18764527638753\n",
      "epoch: 296, batch loss: 21.076080958048504\n",
      "epoch: 297, batch loss: 20.629590113957722\n",
      "epoch: 298, batch loss: 19.848701238632202\n",
      "epoch: 299, batch loss: 20.46809260050456\n",
      "epoch: 300, batch loss: 20.661296367645264\n",
      "epoch: 1, batch loss: 780.3663330078125\n",
      "epoch: 2, batch loss: 777.7420654296875\n",
      "epoch: 3, batch loss: 778.7075754801432\n",
      "epoch: 4, batch loss: 772.5136260986328\n",
      "epoch: 5, batch loss: 776.456064860026\n",
      "epoch: 6, batch loss: 769.3950042724609\n",
      "epoch: 7, batch loss: 758.4904429117838\n",
      "epoch: 8, batch loss: 756.4757436116537\n",
      "epoch: 9, batch loss: 749.7336069742838\n",
      "epoch: 10, batch loss: 752.7576395670573\n",
      "epoch: 11, batch loss: 734.6125132242838\n",
      "epoch: 12, batch loss: 733.556640625\n",
      "epoch: 13, batch loss: 719.4778951009115\n",
      "epoch: 14, batch loss: 708.1088612874349\n",
      "epoch: 15, batch loss: 691.3954111735026\n",
      "epoch: 16, batch loss: 677.9656168619791\n",
      "epoch: 17, batch loss: 652.4681294759115\n",
      "epoch: 18, batch loss: 636.1686197916666\n",
      "epoch: 19, batch loss: 612.1966807047526\n",
      "epoch: 20, batch loss: 577.9611663818359\n",
      "epoch: 21, batch loss: 550.6987228393555\n",
      "epoch: 22, batch loss: 512.2179641723633\n",
      "epoch: 23, batch loss: 469.2528788248698\n",
      "epoch: 24, batch loss: 431.1576843261719\n",
      "epoch: 25, batch loss: 386.42223866780597\n",
      "epoch: 26, batch loss: 344.16907501220703\n",
      "epoch: 27, batch loss: 302.15066782633465\n",
      "epoch: 28, batch loss: 260.4463399251302\n",
      "epoch: 29, batch loss: 229.83912150065103\n",
      "epoch: 30, batch loss: 196.42291895548502\n",
      "epoch: 31, batch loss: 172.54988606770834\n",
      "epoch: 32, batch loss: 149.77940940856934\n",
      "epoch: 33, batch loss: 139.4942715962728\n",
      "epoch: 34, batch loss: 119.03541692097981\n",
      "epoch: 35, batch loss: 106.00595219930013\n",
      "epoch: 36, batch loss: 96.83662796020508\n",
      "epoch: 37, batch loss: 89.87141418457031\n",
      "epoch: 38, batch loss: 85.14985783894856\n",
      "epoch: 39, batch loss: 76.87267589569092\n",
      "epoch: 40, batch loss: 73.2453104654948\n",
      "epoch: 41, batch loss: 73.79036331176758\n",
      "epoch: 42, batch loss: 67.50396347045898\n",
      "epoch: 43, batch loss: 67.39509582519531\n",
      "epoch: 44, batch loss: 63.52504189809164\n",
      "epoch: 45, batch loss: 58.94792111714681\n",
      "epoch: 46, batch loss: 59.589016914367676\n",
      "epoch: 47, batch loss: 57.70763238271078\n",
      "epoch: 48, batch loss: 54.30866559346517\n",
      "epoch: 49, batch loss: 54.33743031819662\n",
      "epoch: 50, batch loss: 52.36978530883789\n",
      "epoch: 51, batch loss: 51.1566801071167\n",
      "epoch: 52, batch loss: 49.6051451365153\n",
      "epoch: 53, batch loss: 48.53492228190104\n",
      "epoch: 54, batch loss: 47.03614203135172\n",
      "epoch: 55, batch loss: 45.66071176528931\n",
      "epoch: 56, batch loss: 45.727609952290855\n",
      "epoch: 57, batch loss: 44.31837590535482\n",
      "epoch: 58, batch loss: 46.32541116078695\n",
      "epoch: 59, batch loss: 42.866804440816246\n",
      "epoch: 60, batch loss: 44.538790225982666\n",
      "epoch: 61, batch loss: 42.58739821116129\n",
      "epoch: 62, batch loss: 42.558290322621666\n",
      "epoch: 63, batch loss: 40.06116930643717\n",
      "epoch: 64, batch loss: 41.18937555948893\n",
      "epoch: 65, batch loss: 39.395413398742676\n",
      "epoch: 66, batch loss: 40.66994444529215\n",
      "epoch: 67, batch loss: 37.25438563028971\n",
      "epoch: 68, batch loss: 38.413431803385414\n",
      "epoch: 69, batch loss: 38.02718607584635\n",
      "epoch: 70, batch loss: 36.003305753072105\n",
      "epoch: 71, batch loss: 38.24043639500936\n",
      "epoch: 72, batch loss: 36.88086795806885\n",
      "epoch: 73, batch loss: 34.68613187472025\n",
      "epoch: 74, batch loss: 35.5426033337911\n",
      "epoch: 75, batch loss: 35.38450972239176\n",
      "epoch: 76, batch loss: 35.00188382466634\n",
      "epoch: 77, batch loss: 34.24378728866577\n",
      "epoch: 78, batch loss: 33.92605702082316\n",
      "epoch: 79, batch loss: 32.937713623046875\n",
      "epoch: 80, batch loss: 33.91034173965454\n",
      "epoch: 81, batch loss: 33.84570757548014\n",
      "epoch: 82, batch loss: 32.96340370178223\n",
      "epoch: 83, batch loss: 33.88431056340536\n",
      "epoch: 84, batch loss: 31.86813958485921\n",
      "epoch: 85, batch loss: 32.274646600087486\n",
      "epoch: 86, batch loss: 32.69593747456869\n",
      "epoch: 87, batch loss: 32.14363320668539\n",
      "epoch: 88, batch loss: 31.597872734069824\n",
      "epoch: 89, batch loss: 31.378225962320965\n",
      "epoch: 90, batch loss: 31.6693377494812\n",
      "epoch: 91, batch loss: 30.497265021006267\n",
      "epoch: 92, batch loss: 30.990181128184002\n",
      "epoch: 93, batch loss: 30.179016431172688\n",
      "epoch: 94, batch loss: 31.76838731765747\n",
      "epoch: 95, batch loss: 29.91294765472412\n",
      "epoch: 96, batch loss: 29.1854510307312\n",
      "epoch: 97, batch loss: 29.375490347544353\n",
      "epoch: 98, batch loss: 29.173927307128906\n",
      "epoch: 99, batch loss: 30.23032522201538\n",
      "epoch: 100, batch loss: 28.52030897140503\n",
      "epoch: 101, batch loss: 29.412585417429607\n",
      "epoch: 102, batch loss: 29.316999435424805\n",
      "epoch: 103, batch loss: 28.526921272277832\n",
      "epoch: 104, batch loss: 29.119263172149658\n",
      "epoch: 105, batch loss: 27.79849894841512\n",
      "epoch: 106, batch loss: 28.568052609761555\n",
      "epoch: 107, batch loss: 27.494683583577473\n",
      "epoch: 108, batch loss: 27.770656903584797\n",
      "epoch: 109, batch loss: 27.817780017852783\n",
      "epoch: 110, batch loss: 27.82448434829712\n",
      "epoch: 111, batch loss: 27.76788314183553\n",
      "epoch: 112, batch loss: 26.421313842137653\n",
      "epoch: 113, batch loss: 26.736083507537842\n",
      "epoch: 114, batch loss: 26.92586660385132\n",
      "epoch: 115, batch loss: 25.953689972559612\n",
      "epoch: 116, batch loss: 26.744718074798584\n",
      "epoch: 117, batch loss: 26.28460995356242\n",
      "epoch: 118, batch loss: 26.866551558176678\n",
      "epoch: 119, batch loss: 25.45508329073588\n",
      "epoch: 120, batch loss: 25.99289909998576\n",
      "epoch: 121, batch loss: 25.831822554270428\n",
      "epoch: 122, batch loss: 26.53147268295288\n",
      "epoch: 123, batch loss: 25.66131607691447\n",
      "epoch: 124, batch loss: 26.112247149149578\n",
      "epoch: 125, batch loss: 25.4569517771403\n",
      "epoch: 126, batch loss: 26.419813950856526\n",
      "epoch: 127, batch loss: 24.751368125279743\n",
      "epoch: 128, batch loss: 25.126636505126953\n",
      "epoch: 129, batch loss: 25.526678562164307\n",
      "epoch: 130, batch loss: 24.887003898620605\n",
      "epoch: 131, batch loss: 24.983218987782795\n",
      "epoch: 132, batch loss: 25.19283962249756\n",
      "epoch: 133, batch loss: 24.305271863937378\n",
      "epoch: 134, batch loss: 24.23425793647766\n",
      "epoch: 135, batch loss: 25.165926774342854\n",
      "epoch: 136, batch loss: 24.29657522837321\n",
      "epoch: 137, batch loss: 24.57981586456299\n",
      "epoch: 138, batch loss: 25.422549724578857\n",
      "epoch: 139, batch loss: 25.589078267415363\n",
      "epoch: 140, batch loss: 24.036000728607178\n",
      "epoch: 141, batch loss: 23.929378509521484\n",
      "epoch: 142, batch loss: 24.082460085550945\n",
      "epoch: 143, batch loss: 24.058586438496906\n",
      "epoch: 144, batch loss: 23.938331921895344\n",
      "epoch: 145, batch loss: 24.15575710932414\n",
      "epoch: 146, batch loss: 24.552927017211914\n",
      "epoch: 147, batch loss: 24.264900366465252\n",
      "epoch: 148, batch loss: 24.18557373682658\n",
      "epoch: 149, batch loss: 24.732380231221516\n",
      "epoch: 150, batch loss: 23.75873374938965\n",
      "epoch: 151, batch loss: 23.322248617808025\n",
      "epoch: 152, batch loss: 24.242645899454754\n",
      "epoch: 153, batch loss: 23.256749312082928\n",
      "epoch: 154, batch loss: 24.20018704732259\n",
      "epoch: 155, batch loss: 23.947819232940674\n",
      "epoch: 156, batch loss: 23.325093428293865\n",
      "epoch: 157, batch loss: 23.139145215352375\n",
      "epoch: 158, batch loss: 23.56345860163371\n",
      "epoch: 159, batch loss: 23.03126859664917\n",
      "epoch: 160, batch loss: 22.75883110364278\n",
      "epoch: 161, batch loss: 23.157505830128986\n",
      "epoch: 162, batch loss: 23.68848975499471\n",
      "epoch: 163, batch loss: 23.54667043685913\n",
      "epoch: 164, batch loss: 22.759655952453613\n",
      "epoch: 165, batch loss: 22.986677090326946\n",
      "epoch: 166, batch loss: 23.45394229888916\n",
      "epoch: 167, batch loss: 22.547727425893147\n",
      "epoch: 168, batch loss: 22.512273629506428\n",
      "epoch: 169, batch loss: 22.918732166290283\n",
      "epoch: 170, batch loss: 22.645175615946453\n",
      "epoch: 171, batch loss: 22.569760004679363\n",
      "epoch: 172, batch loss: 23.40938965479533\n",
      "epoch: 173, batch loss: 22.511858304341633\n",
      "epoch: 174, batch loss: 22.84637490908305\n",
      "epoch: 175, batch loss: 22.299097617467243\n",
      "epoch: 176, batch loss: 23.210668961207073\n",
      "epoch: 177, batch loss: 22.539111455281574\n",
      "epoch: 178, batch loss: 24.303218523661297\n",
      "epoch: 179, batch loss: 22.187198956807453\n",
      "epoch: 180, batch loss: 23.072662115097046\n",
      "epoch: 181, batch loss: 22.88029972712199\n",
      "epoch: 182, batch loss: 21.961686611175537\n",
      "epoch: 183, batch loss: 22.279837767283123\n",
      "epoch: 184, batch loss: 22.250422795613606\n",
      "epoch: 185, batch loss: 22.394583225250244\n",
      "epoch: 186, batch loss: 21.72718834877014\n",
      "epoch: 187, batch loss: 21.79642105102539\n",
      "epoch: 188, batch loss: 22.18307574590047\n",
      "epoch: 189, batch loss: 21.533699830373127\n",
      "epoch: 190, batch loss: 21.435848315556843\n",
      "epoch: 191, batch loss: 21.608189741770428\n",
      "epoch: 192, batch loss: 21.8063702583313\n",
      "epoch: 193, batch loss: 21.961782932281494\n",
      "epoch: 194, batch loss: 21.980612913767498\n",
      "epoch: 195, batch loss: 21.304903745651245\n",
      "epoch: 196, batch loss: 21.888773441314697\n",
      "epoch: 197, batch loss: 21.242762009302776\n",
      "epoch: 198, batch loss: 21.79327932993571\n",
      "epoch: 199, batch loss: 21.887656370798748\n",
      "epoch: 200, batch loss: 21.68781614303589\n",
      "epoch: 201, batch loss: 21.880836327870686\n",
      "epoch: 202, batch loss: 21.30264075597127\n",
      "epoch: 203, batch loss: 21.465204000473022\n",
      "epoch: 204, batch loss: 21.85268211364746\n",
      "epoch: 205, batch loss: 21.802242279052734\n",
      "epoch: 206, batch loss: 21.178690433502197\n",
      "epoch: 207, batch loss: 21.182008743286133\n",
      "epoch: 208, batch loss: 21.03104003270467\n",
      "epoch: 209, batch loss: 20.726794560750324\n",
      "epoch: 210, batch loss: 21.028403600056965\n",
      "epoch: 211, batch loss: 21.59756914774577\n",
      "epoch: 212, batch loss: 21.221024990081787\n",
      "epoch: 213, batch loss: 20.708009163538616\n",
      "epoch: 214, batch loss: 21.137396335601807\n",
      "epoch: 215, batch loss: 21.106108109156292\n",
      "epoch: 216, batch loss: 20.92683474222819\n",
      "epoch: 217, batch loss: 21.115517775217693\n",
      "epoch: 218, batch loss: 21.271258513132732\n",
      "epoch: 219, batch loss: 20.73832901318868\n",
      "epoch: 220, batch loss: 20.908592065175373\n",
      "epoch: 221, batch loss: 21.676456212997437\n",
      "epoch: 222, batch loss: 21.151007493336994\n",
      "epoch: 223, batch loss: 20.699688116709392\n",
      "epoch: 224, batch loss: 21.567872126897175\n",
      "epoch: 225, batch loss: 21.07172441482544\n",
      "epoch: 226, batch loss: 20.956772963205974\n",
      "epoch: 227, batch loss: 20.41584388415019\n",
      "epoch: 228, batch loss: 21.72451400756836\n",
      "epoch: 229, batch loss: 20.76332672437032\n",
      "epoch: 230, batch loss: 20.705647548039753\n",
      "epoch: 231, batch loss: 21.65266442298889\n",
      "epoch: 232, batch loss: 21.00337568918864\n",
      "epoch: 233, batch loss: 20.94139003753662\n",
      "epoch: 234, batch loss: 21.112274408340454\n",
      "epoch: 235, batch loss: 20.476235389709473\n",
      "epoch: 236, batch loss: 20.693710565567017\n",
      "epoch: 237, batch loss: 20.80522616704305\n",
      "epoch: 238, batch loss: 20.47611657778422\n",
      "epoch: 239, batch loss: 20.48156722386678\n",
      "epoch: 240, batch loss: 21.19062678019206\n",
      "epoch: 241, batch loss: 20.549832264582317\n",
      "epoch: 242, batch loss: 20.486324469248455\n",
      "epoch: 243, batch loss: 20.289350589116413\n",
      "epoch: 244, batch loss: 20.915135542551678\n",
      "epoch: 245, batch loss: 20.90841245651245\n",
      "epoch: 246, batch loss: 20.421801805496216\n",
      "epoch: 247, batch loss: 21.218708992004395\n",
      "epoch: 248, batch loss: 20.101810057957966\n",
      "epoch: 249, batch loss: 20.142421801884968\n",
      "epoch: 250, batch loss: 20.121198177337646\n",
      "epoch: 251, batch loss: 20.345024824142456\n",
      "epoch: 252, batch loss: 20.73638145128886\n",
      "epoch: 253, batch loss: 20.065369844436646\n",
      "epoch: 254, batch loss: 20.42531379063924\n",
      "epoch: 255, batch loss: 20.43856080373128\n",
      "epoch: 256, batch loss: 19.86782701810201\n",
      "epoch: 257, batch loss: 20.689922491709392\n",
      "epoch: 258, batch loss: 20.421204169591267\n",
      "epoch: 259, batch loss: 20.123565832773846\n",
      "epoch: 260, batch loss: 20.22213689486186\n",
      "epoch: 261, batch loss: 21.009239594141643\n",
      "epoch: 262, batch loss: 19.93841282526652\n",
      "epoch: 263, batch loss: 20.885316610336304\n",
      "epoch: 264, batch loss: 20.86716373761495\n",
      "epoch: 265, batch loss: 19.87704062461853\n",
      "epoch: 266, batch loss: 20.815433899561565\n",
      "epoch: 267, batch loss: 19.94582454363505\n",
      "epoch: 268, batch loss: 19.901304721832275\n",
      "epoch: 269, batch loss: 19.623174508412678\n",
      "epoch: 270, batch loss: 19.811858495076496\n",
      "epoch: 271, batch loss: 19.612601200739544\n",
      "epoch: 272, batch loss: 19.883086442947388\n",
      "epoch: 273, batch loss: 20.32322072982788\n",
      "epoch: 274, batch loss: 19.991872151692707\n",
      "epoch: 275, batch loss: 20.666874249776203\n",
      "epoch: 276, batch loss: 21.321107546488445\n",
      "epoch: 277, batch loss: 19.64624540011088\n",
      "epoch: 278, batch loss: 19.577654282251995\n",
      "epoch: 279, batch loss: 19.631641546885174\n",
      "epoch: 280, batch loss: 20.35876226425171\n",
      "epoch: 281, batch loss: 19.626640876134235\n",
      "epoch: 282, batch loss: 19.21508526802063\n",
      "epoch: 283, batch loss: 21.10532347361247\n",
      "epoch: 284, batch loss: 19.50204674402873\n",
      "epoch: 285, batch loss: 19.645750761032104\n",
      "epoch: 286, batch loss: 19.65969451268514\n",
      "epoch: 287, batch loss: 19.765783071517944\n",
      "epoch: 288, batch loss: 19.59096908569336\n",
      "epoch: 289, batch loss: 19.304144223531086\n",
      "epoch: 290, batch loss: 19.44015924135844\n",
      "epoch: 291, batch loss: 19.9833238919576\n",
      "epoch: 292, batch loss: 19.79737599690755\n",
      "epoch: 293, batch loss: 20.0320143699646\n",
      "epoch: 294, batch loss: 19.76032368342082\n",
      "epoch: 295, batch loss: 20.14848740895589\n",
      "epoch: 296, batch loss: 19.409855365753174\n",
      "epoch: 297, batch loss: 19.950791358947754\n",
      "epoch: 298, batch loss: 19.561718304951984\n",
      "epoch: 299, batch loss: 20.092492421468098\n",
      "epoch: 300, batch loss: 20.539215962092083\n",
      "epoch: 1, batch loss: 774.216786702474\n",
      "epoch: 2, batch loss: 772.6108957926432\n",
      "epoch: 3, batch loss: 770.6679026285807\n",
      "epoch: 4, batch loss: 768.4843444824219\n",
      "epoch: 5, batch loss: 766.0519460042318\n",
      "epoch: 6, batch loss: 756.8188934326172\n",
      "epoch: 7, batch loss: 757.1009572347006\n",
      "epoch: 8, batch loss: 753.1102396647135\n",
      "epoch: 9, batch loss: 749.8110198974609\n",
      "epoch: 10, batch loss: 743.4463602701823\n",
      "epoch: 11, batch loss: 743.3902842203776\n",
      "epoch: 12, batch loss: 734.6156768798828\n",
      "epoch: 13, batch loss: 728.3239390055338\n",
      "epoch: 14, batch loss: 719.5007069905599\n",
      "epoch: 15, batch loss: 716.7886810302734\n",
      "epoch: 16, batch loss: 700.6200714111328\n",
      "epoch: 17, batch loss: 690.8111419677734\n",
      "epoch: 18, batch loss: 674.4691823323568\n",
      "epoch: 19, batch loss: 660.4028778076172\n",
      "epoch: 20, batch loss: 644.6203358968099\n",
      "epoch: 21, batch loss: 612.7618865966797\n",
      "epoch: 22, batch loss: 593.0990651448568\n",
      "epoch: 23, batch loss: 561.9145914713541\n",
      "epoch: 24, batch loss: 527.5509312947592\n",
      "epoch: 25, batch loss: 499.3430404663086\n",
      "epoch: 26, batch loss: 457.4461720784505\n",
      "epoch: 27, batch loss: 415.166742960612\n",
      "epoch: 28, batch loss: 379.36270395914715\n",
      "epoch: 29, batch loss: 339.27796936035156\n",
      "epoch: 30, batch loss: 293.62097295125324\n",
      "epoch: 31, batch loss: 263.5081075032552\n",
      "epoch: 32, batch loss: 232.1843350728353\n",
      "epoch: 33, batch loss: 202.91506322224936\n",
      "epoch: 34, batch loss: 182.33994928995767\n",
      "epoch: 35, batch loss: 163.95245615641275\n",
      "epoch: 36, batch loss: 139.5270792643229\n",
      "epoch: 37, batch loss: 128.04746309916177\n",
      "epoch: 38, batch loss: 118.62877591451009\n",
      "epoch: 39, batch loss: 104.43239307403564\n",
      "epoch: 40, batch loss: 99.99834314982097\n",
      "epoch: 41, batch loss: 94.20283762613933\n",
      "epoch: 42, batch loss: 91.69201405843098\n",
      "epoch: 43, batch loss: 87.07194582621257\n",
      "epoch: 44, batch loss: 82.57303555806477\n",
      "epoch: 45, batch loss: 81.00982475280762\n",
      "epoch: 46, batch loss: 81.19660059611003\n",
      "epoch: 47, batch loss: 78.60826365152995\n",
      "epoch: 48, batch loss: 75.23737716674805\n",
      "epoch: 49, batch loss: 73.26960468292236\n",
      "epoch: 50, batch loss: 73.32869942982991\n",
      "epoch: 51, batch loss: 70.83089542388916\n",
      "epoch: 52, batch loss: 70.43179829915364\n",
      "epoch: 53, batch loss: 67.5912316640218\n",
      "epoch: 54, batch loss: 65.41865921020508\n",
      "epoch: 55, batch loss: 63.82364304860433\n",
      "epoch: 56, batch loss: 65.39246527353923\n",
      "epoch: 57, batch loss: 60.74584992726644\n",
      "epoch: 58, batch loss: 61.01838080088297\n",
      "epoch: 59, batch loss: 59.967284202575684\n",
      "epoch: 60, batch loss: 59.250410079956055\n",
      "epoch: 61, batch loss: 57.5085391998291\n",
      "epoch: 62, batch loss: 57.70759073893229\n",
      "epoch: 63, batch loss: 55.87985038757324\n",
      "epoch: 64, batch loss: 54.29772345225016\n",
      "epoch: 65, batch loss: 53.82593568166097\n",
      "epoch: 66, batch loss: 52.3822660446167\n",
      "epoch: 67, batch loss: 53.561099688212074\n",
      "epoch: 68, batch loss: 52.609628995259605\n",
      "epoch: 69, batch loss: 52.991546948750816\n",
      "epoch: 70, batch loss: 49.584587732950844\n",
      "epoch: 71, batch loss: 49.96000703175863\n",
      "epoch: 72, batch loss: 47.763168017069496\n",
      "epoch: 73, batch loss: 48.436792055765785\n",
      "epoch: 74, batch loss: 46.958627700805664\n",
      "epoch: 75, batch loss: 47.054779052734375\n",
      "epoch: 76, batch loss: 45.19505993525187\n",
      "epoch: 77, batch loss: 46.19376150767008\n",
      "epoch: 78, batch loss: 43.46234019597372\n",
      "epoch: 79, batch loss: 45.62206522623698\n",
      "epoch: 80, batch loss: 42.43998829523722\n",
      "epoch: 81, batch loss: 44.55651887257894\n",
      "epoch: 82, batch loss: 41.98958778381348\n",
      "epoch: 83, batch loss: 41.8837677637736\n",
      "epoch: 84, batch loss: 40.87061564127604\n",
      "epoch: 85, batch loss: 39.92720079421997\n",
      "epoch: 86, batch loss: 39.999216397603355\n",
      "epoch: 87, batch loss: 39.28811693191528\n",
      "epoch: 88, batch loss: 39.49567381540934\n",
      "epoch: 89, batch loss: 39.25988515218099\n",
      "epoch: 90, batch loss: 39.13027175267538\n",
      "epoch: 91, batch loss: 37.699482440948486\n",
      "epoch: 92, batch loss: 37.21807066599528\n",
      "epoch: 93, batch loss: 37.83304738998413\n",
      "epoch: 94, batch loss: 37.16605186462402\n",
      "epoch: 95, batch loss: 37.54260587692261\n",
      "epoch: 96, batch loss: 35.07301410039266\n",
      "epoch: 97, batch loss: 36.086990197499595\n",
      "epoch: 98, batch loss: 35.117207845052086\n",
      "epoch: 99, batch loss: 35.24035612742106\n",
      "epoch: 100, batch loss: 33.95142380396525\n",
      "epoch: 101, batch loss: 35.69344997406006\n",
      "epoch: 102, batch loss: 33.38383404413859\n",
      "epoch: 103, batch loss: 35.05779457092285\n",
      "epoch: 104, batch loss: 34.67723830540975\n",
      "epoch: 105, batch loss: 32.81788285573324\n",
      "epoch: 106, batch loss: 31.78865655263265\n",
      "epoch: 107, batch loss: 31.433509190877277\n",
      "epoch: 108, batch loss: 31.435665766398113\n",
      "epoch: 109, batch loss: 31.150363445281982\n",
      "epoch: 110, batch loss: 32.588574727376304\n",
      "epoch: 111, batch loss: 31.229629516601562\n",
      "epoch: 112, batch loss: 30.333841959635418\n",
      "epoch: 113, batch loss: 30.812448978424072\n",
      "epoch: 114, batch loss: 30.62719488143921\n",
      "epoch: 115, batch loss: 30.168627580006916\n",
      "epoch: 116, batch loss: 29.66556676228841\n",
      "epoch: 117, batch loss: 30.941434065500896\n",
      "epoch: 118, batch loss: 30.48359473546346\n",
      "epoch: 119, batch loss: 29.41819190979004\n",
      "epoch: 120, batch loss: 28.651779492696125\n",
      "epoch: 121, batch loss: 28.869625250498455\n",
      "epoch: 122, batch loss: 28.926314036051433\n",
      "epoch: 123, batch loss: 27.800023635228474\n",
      "epoch: 124, batch loss: 28.103638648986816\n",
      "epoch: 125, batch loss: 28.50859785079956\n",
      "epoch: 126, batch loss: 28.771026134490967\n",
      "epoch: 127, batch loss: 27.633593559265137\n",
      "epoch: 128, batch loss: 29.494332154591877\n",
      "epoch: 129, batch loss: 27.17716868718465\n",
      "epoch: 130, batch loss: 28.21477158864339\n",
      "epoch: 131, batch loss: 28.45903476079305\n",
      "epoch: 132, batch loss: 27.185996373494465\n",
      "epoch: 133, batch loss: 26.95975923538208\n",
      "epoch: 134, batch loss: 26.782121499379475\n",
      "epoch: 135, batch loss: 27.65523640314738\n",
      "epoch: 136, batch loss: 26.719727357228596\n",
      "epoch: 137, batch loss: 26.069933891296387\n",
      "epoch: 138, batch loss: 26.25216245651245\n",
      "epoch: 139, batch loss: 25.896293958028156\n",
      "epoch: 140, batch loss: 25.91411813100179\n",
      "epoch: 141, batch loss: 26.37729819615682\n",
      "epoch: 142, batch loss: 25.881004969278973\n",
      "epoch: 143, batch loss: 26.24149990081787\n",
      "epoch: 144, batch loss: 25.616467157999676\n",
      "epoch: 145, batch loss: 25.071013530095417\n",
      "epoch: 146, batch loss: 25.443753878275555\n",
      "epoch: 147, batch loss: 26.696190039316814\n",
      "epoch: 148, batch loss: 25.816279888153076\n",
      "epoch: 149, batch loss: 25.11311960220337\n",
      "epoch: 150, batch loss: 24.46741835276286\n",
      "epoch: 151, batch loss: 25.166946570078533\n",
      "epoch: 152, batch loss: 24.96409734090169\n",
      "epoch: 153, batch loss: 24.774316787719727\n",
      "epoch: 154, batch loss: 25.554081916809082\n",
      "epoch: 155, batch loss: 24.604300181070965\n",
      "epoch: 156, batch loss: 24.008844296137493\n",
      "epoch: 157, batch loss: 24.648985862731934\n",
      "epoch: 158, batch loss: 24.388745307922363\n",
      "epoch: 159, batch loss: 24.240749677022297\n",
      "epoch: 160, batch loss: 24.598266283671062\n",
      "epoch: 161, batch loss: 23.801883379618328\n",
      "epoch: 162, batch loss: 24.82330060005188\n",
      "epoch: 163, batch loss: 24.05714511871338\n",
      "epoch: 164, batch loss: 23.949772357940674\n",
      "epoch: 165, batch loss: 23.796386241912842\n",
      "epoch: 166, batch loss: 23.506747643152874\n",
      "epoch: 167, batch loss: 24.160288333892822\n",
      "epoch: 168, batch loss: 23.59203863143921\n",
      "epoch: 169, batch loss: 24.544175148010254\n",
      "epoch: 170, batch loss: 23.239394982655842\n",
      "epoch: 171, batch loss: 23.347794850667317\n",
      "epoch: 172, batch loss: 23.079679012298584\n",
      "epoch: 173, batch loss: 22.955715497334797\n",
      "epoch: 174, batch loss: 23.79895893732707\n",
      "epoch: 175, batch loss: 23.183486779530842\n",
      "epoch: 176, batch loss: 23.288442770640057\n",
      "epoch: 177, batch loss: 23.034284909566242\n",
      "epoch: 178, batch loss: 23.62959321339925\n",
      "epoch: 179, batch loss: 22.940434455871582\n",
      "epoch: 180, batch loss: 23.94113524754842\n",
      "epoch: 181, batch loss: 22.939568837483723\n",
      "epoch: 182, batch loss: 22.75205373764038\n",
      "epoch: 183, batch loss: 23.043732166290283\n",
      "epoch: 184, batch loss: 24.330183347066242\n",
      "epoch: 185, batch loss: 22.516324520111084\n",
      "epoch: 186, batch loss: 22.62245241800944\n",
      "epoch: 187, batch loss: 22.606692949930828\n",
      "epoch: 188, batch loss: 22.54248587290446\n",
      "epoch: 189, batch loss: 22.20445227622986\n",
      "epoch: 190, batch loss: 24.064170519510906\n",
      "epoch: 191, batch loss: 22.94779674212138\n",
      "epoch: 192, batch loss: 22.24971063931783\n",
      "epoch: 193, batch loss: 22.71727403004964\n",
      "epoch: 194, batch loss: 23.090473016103108\n",
      "epoch: 195, batch loss: 22.489938259124756\n",
      "epoch: 196, batch loss: 22.720129648844402\n",
      "epoch: 197, batch loss: 21.98759897549947\n",
      "epoch: 198, batch loss: 22.022459665934246\n",
      "epoch: 199, batch loss: 23.421411355336506\n",
      "epoch: 200, batch loss: 22.41213051478068\n",
      "epoch: 201, batch loss: 23.04652237892151\n",
      "epoch: 202, batch loss: 22.02610985438029\n",
      "epoch: 203, batch loss: 21.78085978825887\n",
      "epoch: 204, batch loss: 22.239028771718342\n",
      "epoch: 205, batch loss: 21.573268095652264\n",
      "epoch: 206, batch loss: 21.4053049882253\n",
      "epoch: 207, batch loss: 22.25252326329549\n",
      "epoch: 208, batch loss: 21.861561457316082\n",
      "epoch: 209, batch loss: 21.758092482884724\n",
      "epoch: 210, batch loss: 21.539176384607952\n",
      "epoch: 211, batch loss: 22.318026860555012\n",
      "epoch: 212, batch loss: 21.93386936187744\n",
      "epoch: 213, batch loss: 22.44979461034139\n",
      "epoch: 214, batch loss: 21.96965805689494\n",
      "epoch: 215, batch loss: 22.339273532231648\n",
      "epoch: 216, batch loss: 21.52558469772339\n",
      "epoch: 217, batch loss: 21.86065944035848\n",
      "epoch: 218, batch loss: 21.20020929972331\n",
      "epoch: 219, batch loss: 21.478883584340412\n",
      "epoch: 220, batch loss: 22.96601390838623\n",
      "epoch: 221, batch loss: 21.437425931294758\n",
      "epoch: 222, batch loss: 21.378845532735188\n",
      "epoch: 223, batch loss: 21.542878309885662\n",
      "epoch: 224, batch loss: 20.85419726371765\n",
      "epoch: 225, batch loss: 21.734565019607544\n",
      "epoch: 226, batch loss: 21.756592830022175\n",
      "epoch: 227, batch loss: 21.31307593981425\n",
      "epoch: 228, batch loss: 20.833889643351238\n",
      "epoch: 229, batch loss: 21.50782799720764\n",
      "epoch: 230, batch loss: 21.009549856185913\n",
      "epoch: 231, batch loss: 21.045661608378094\n",
      "epoch: 232, batch loss: 21.050969918568928\n",
      "epoch: 233, batch loss: 22.022828261057537\n",
      "epoch: 234, batch loss: 21.212474187215168\n",
      "epoch: 235, batch loss: 21.45198615392049\n",
      "epoch: 236, batch loss: 21.08811855316162\n",
      "epoch: 237, batch loss: 21.152294953664143\n",
      "epoch: 238, batch loss: 20.565905570983887\n",
      "epoch: 239, batch loss: 20.979546864827473\n",
      "epoch: 240, batch loss: 21.83365774154663\n",
      "epoch: 241, batch loss: 21.782393137613933\n",
      "epoch: 242, batch loss: 21.10324501991272\n",
      "epoch: 243, batch loss: 20.809134483337402\n",
      "epoch: 244, batch loss: 20.476978699366253\n",
      "epoch: 245, batch loss: 21.73105065027873\n",
      "epoch: 246, batch loss: 20.983478387196858\n",
      "epoch: 247, batch loss: 20.907901525497437\n",
      "epoch: 248, batch loss: 21.31178339322408\n",
      "epoch: 249, batch loss: 20.273990472157795\n",
      "epoch: 250, batch loss: 20.56463932991028\n",
      "epoch: 251, batch loss: 20.677530606587727\n",
      "epoch: 252, batch loss: 20.278210878372192\n",
      "epoch: 253, batch loss: 20.681291580200195\n",
      "epoch: 254, batch loss: 20.47053098678589\n",
      "epoch: 255, batch loss: 20.915111541748047\n",
      "epoch: 256, batch loss: 21.59894386927287\n",
      "epoch: 257, batch loss: 20.540738821029663\n",
      "epoch: 258, batch loss: 21.653075695037842\n",
      "epoch: 259, batch loss: 20.85216720898946\n",
      "epoch: 260, batch loss: 20.290773630142212\n",
      "epoch: 261, batch loss: 20.465111414591473\n",
      "epoch: 262, batch loss: 21.1125971476237\n",
      "epoch: 263, batch loss: 20.824843168258667\n",
      "epoch: 264, batch loss: 20.08994261423747\n",
      "epoch: 265, batch loss: 20.716845512390137\n",
      "epoch: 266, batch loss: 20.297649383544922\n",
      "epoch: 267, batch loss: 20.65775418281555\n",
      "epoch: 268, batch loss: 20.250311692555744\n",
      "epoch: 269, batch loss: 19.785338242848713\n",
      "epoch: 270, batch loss: 20.205336093902588\n",
      "epoch: 271, batch loss: 20.274616559346516\n",
      "epoch: 272, batch loss: 20.68624472618103\n",
      "epoch: 273, batch loss: 20.999377409617107\n",
      "epoch: 274, batch loss: 20.553720951080322\n",
      "epoch: 275, batch loss: 19.93456466992696\n",
      "epoch: 276, batch loss: 20.251259406407673\n",
      "epoch: 277, batch loss: 20.08820907274882\n",
      "epoch: 278, batch loss: 20.46026309331258\n",
      "epoch: 279, batch loss: 21.249634106953938\n",
      "epoch: 280, batch loss: 20.17886765797933\n",
      "epoch: 281, batch loss: 20.177025000254314\n",
      "epoch: 282, batch loss: 19.936196486155193\n",
      "epoch: 283, batch loss: 20.418617963790894\n",
      "epoch: 284, batch loss: 20.106435855229694\n",
      "epoch: 285, batch loss: 20.950037082036335\n",
      "epoch: 286, batch loss: 20.446638345718384\n",
      "epoch: 287, batch loss: 20.439435561498005\n",
      "epoch: 288, batch loss: 20.74060869216919\n",
      "epoch: 289, batch loss: 20.703527688980103\n",
      "epoch: 290, batch loss: 20.198219935099285\n",
      "epoch: 291, batch loss: 19.77840455373128\n",
      "epoch: 292, batch loss: 19.93311659495036\n",
      "epoch: 293, batch loss: 19.663984139760334\n",
      "epoch: 294, batch loss: 20.595983346303303\n",
      "epoch: 295, batch loss: 20.05850688616435\n",
      "epoch: 296, batch loss: 19.65040198961894\n",
      "epoch: 297, batch loss: 20.81825057665507\n",
      "epoch: 298, batch loss: 19.82097339630127\n",
      "epoch: 299, batch loss: 20.91333285967509\n",
      "epoch: 300, batch loss: 19.67690396308899\n",
      "epoch: 1, batch loss: 787.8965759277344\n",
      "epoch: 2, batch loss: 778.9407958984375\n",
      "epoch: 3, batch loss: 777.4369761149088\n",
      "epoch: 4, batch loss: 775.8089803059896\n",
      "epoch: 5, batch loss: 780.9418589274088\n",
      "epoch: 6, batch loss: 778.4869588216146\n",
      "epoch: 7, batch loss: 778.0225118001302\n",
      "epoch: 8, batch loss: 769.1268005371094\n",
      "epoch: 9, batch loss: 762.0877787272135\n",
      "epoch: 10, batch loss: 761.5459798177084\n",
      "epoch: 11, batch loss: 757.4887644449869\n",
      "epoch: 12, batch loss: 749.4447021484375\n",
      "epoch: 13, batch loss: 744.9698791503906\n",
      "epoch: 14, batch loss: 733.0324961344401\n",
      "epoch: 15, batch loss: 727.9176890055338\n",
      "epoch: 16, batch loss: 714.7793172200521\n",
      "epoch: 17, batch loss: 701.4788767496744\n",
      "epoch: 18, batch loss: 681.4390665690104\n",
      "epoch: 19, batch loss: 666.6539967854818\n",
      "epoch: 20, batch loss: 642.0179239908854\n",
      "epoch: 21, batch loss: 611.0590413411459\n",
      "epoch: 22, batch loss: 587.7967529296875\n",
      "epoch: 23, batch loss: 561.4413274129232\n",
      "epoch: 24, batch loss: 513.2032318115234\n",
      "epoch: 25, batch loss: 473.81251525878906\n",
      "epoch: 26, batch loss: 434.4115447998047\n",
      "epoch: 27, batch loss: 394.32371012369794\n",
      "epoch: 28, batch loss: 347.7757822672526\n",
      "epoch: 29, batch loss: 304.083932240804\n",
      "epoch: 30, batch loss: 272.7760149637858\n",
      "epoch: 31, batch loss: 243.64237213134766\n",
      "epoch: 32, batch loss: 212.88044230143228\n",
      "epoch: 33, batch loss: 188.8902791341146\n",
      "epoch: 34, batch loss: 171.1972770690918\n",
      "epoch: 35, batch loss: 156.8366266886393\n",
      "epoch: 36, batch loss: 143.0822582244873\n",
      "epoch: 37, batch loss: 133.02425257364908\n",
      "epoch: 38, batch loss: 119.23847262064616\n",
      "epoch: 39, batch loss: 111.79030672709148\n",
      "epoch: 40, batch loss: 100.7932243347168\n",
      "epoch: 41, batch loss: 94.4573294321696\n",
      "epoch: 42, batch loss: 89.53016916910808\n",
      "epoch: 43, batch loss: 83.36048889160156\n",
      "epoch: 44, batch loss: 79.89220587412517\n",
      "epoch: 45, batch loss: 72.72655550638835\n",
      "epoch: 46, batch loss: 69.50485261281331\n",
      "epoch: 47, batch loss: 66.82292048136394\n",
      "epoch: 48, batch loss: 64.72872416178386\n",
      "epoch: 49, batch loss: 63.78000990549723\n",
      "epoch: 50, batch loss: 58.34788990020752\n",
      "epoch: 51, batch loss: 56.93278408050537\n",
      "epoch: 52, batch loss: 55.69351069132487\n",
      "epoch: 53, batch loss: 54.2320810953776\n",
      "epoch: 54, batch loss: 52.30828380584717\n",
      "epoch: 55, batch loss: 51.1330353418986\n",
      "epoch: 56, batch loss: 51.75903797149658\n",
      "epoch: 57, batch loss: 48.633317947387695\n",
      "epoch: 58, batch loss: 48.165384928385414\n",
      "epoch: 59, batch loss: 49.21131896972656\n",
      "epoch: 60, batch loss: 47.162381172180176\n",
      "epoch: 61, batch loss: 45.35667037963867\n",
      "epoch: 62, batch loss: 45.56024583180746\n",
      "epoch: 63, batch loss: 44.14195919036865\n",
      "epoch: 64, batch loss: 43.17737642923991\n",
      "epoch: 65, batch loss: 44.084349950154625\n",
      "epoch: 66, batch loss: 44.16407044728597\n",
      "epoch: 67, batch loss: 43.87083133061727\n",
      "epoch: 68, batch loss: 40.61430025100708\n",
      "epoch: 69, batch loss: 41.42190138498942\n",
      "epoch: 70, batch loss: 39.64286724726359\n",
      "epoch: 71, batch loss: 40.148142178853355\n",
      "epoch: 72, batch loss: 38.56006669998169\n",
      "epoch: 73, batch loss: 38.35124762852987\n",
      "epoch: 74, batch loss: 38.766035874684654\n",
      "epoch: 75, batch loss: 37.82246478398641\n",
      "epoch: 76, batch loss: 37.36473528544108\n",
      "epoch: 77, batch loss: 37.29880062739054\n",
      "epoch: 78, batch loss: 37.82730197906494\n",
      "epoch: 79, batch loss: 35.524257818857826\n",
      "epoch: 80, batch loss: 36.398134072621666\n",
      "epoch: 81, batch loss: 38.12200276056925\n",
      "epoch: 82, batch loss: 34.7717277208964\n",
      "epoch: 83, batch loss: 36.91420602798462\n",
      "epoch: 84, batch loss: 34.84998973210653\n",
      "epoch: 85, batch loss: 34.61780405044556\n",
      "epoch: 86, batch loss: 35.387790520985924\n",
      "epoch: 87, batch loss: 34.12691863377889\n",
      "epoch: 88, batch loss: 33.43749205271403\n",
      "epoch: 89, batch loss: 33.47598012288412\n",
      "epoch: 90, batch loss: 33.041273752848305\n",
      "epoch: 91, batch loss: 32.70458443959554\n",
      "epoch: 92, batch loss: 32.09944756825765\n",
      "epoch: 93, batch loss: 34.00467856725057\n",
      "epoch: 94, batch loss: 32.0084613164266\n",
      "epoch: 95, batch loss: 31.16197856267293\n",
      "epoch: 96, batch loss: 32.34543800354004\n",
      "epoch: 97, batch loss: 31.309969902038574\n",
      "epoch: 98, batch loss: 30.502417882283527\n",
      "epoch: 99, batch loss: 31.687273343404133\n",
      "epoch: 100, batch loss: 30.520177523295086\n",
      "epoch: 101, batch loss: 30.218493620554607\n",
      "epoch: 102, batch loss: 30.829419294993084\n",
      "epoch: 103, batch loss: 30.24315055211385\n",
      "epoch: 104, batch loss: 30.17776075998942\n",
      "epoch: 105, batch loss: 29.945295492808025\n",
      "epoch: 106, batch loss: 29.502726554870605\n",
      "epoch: 107, batch loss: 29.164726734161377\n",
      "epoch: 108, batch loss: 29.338675657908123\n",
      "epoch: 109, batch loss: 29.982107639312744\n",
      "epoch: 110, batch loss: 29.20907211303711\n",
      "epoch: 111, batch loss: 28.83572785059611\n",
      "epoch: 112, batch loss: 28.82093556722005\n",
      "epoch: 113, batch loss: 28.888916333516438\n",
      "epoch: 114, batch loss: 28.8982572555542\n",
      "epoch: 115, batch loss: 28.571253299713135\n",
      "epoch: 116, batch loss: 28.403820991516113\n",
      "epoch: 117, batch loss: 28.34767548243205\n",
      "epoch: 118, batch loss: 28.12710650761922\n",
      "epoch: 119, batch loss: 27.2491930325826\n",
      "epoch: 120, batch loss: 27.916041533152264\n",
      "epoch: 121, batch loss: 27.644724051157635\n",
      "epoch: 122, batch loss: 27.09149471918742\n",
      "epoch: 123, batch loss: 27.198843955993652\n",
      "epoch: 124, batch loss: 28.181586106618244\n",
      "epoch: 125, batch loss: 26.62142817179362\n",
      "epoch: 126, batch loss: 27.2944548924764\n",
      "epoch: 127, batch loss: 26.33088763554891\n",
      "epoch: 128, batch loss: 26.4094139734904\n",
      "epoch: 129, batch loss: 26.38262637456258\n",
      "epoch: 130, batch loss: 26.019441922505695\n",
      "epoch: 131, batch loss: 27.037694613138836\n",
      "epoch: 132, batch loss: 25.85375912984212\n",
      "epoch: 133, batch loss: 26.29660479227702\n",
      "epoch: 134, batch loss: 25.925564448038738\n",
      "epoch: 135, batch loss: 26.47278070449829\n",
      "epoch: 136, batch loss: 25.95213047663371\n",
      "epoch: 137, batch loss: 25.01194739341736\n",
      "epoch: 138, batch loss: 25.29249858856201\n",
      "epoch: 139, batch loss: 25.895466327667236\n",
      "epoch: 140, batch loss: 25.332141081492107\n",
      "epoch: 141, batch loss: 24.98669656117757\n",
      "epoch: 142, batch loss: 25.202102661132812\n",
      "epoch: 143, batch loss: 24.478938500086468\n",
      "epoch: 144, batch loss: 24.589898427327473\n",
      "epoch: 145, batch loss: 24.432810306549072\n",
      "epoch: 146, batch loss: 24.686363220214844\n",
      "epoch: 147, batch loss: 24.958291053771973\n",
      "epoch: 148, batch loss: 24.288235823313396\n",
      "epoch: 149, batch loss: 24.320719401041668\n",
      "epoch: 150, batch loss: 24.57486693064372\n",
      "epoch: 151, batch loss: 24.500147183736164\n",
      "epoch: 152, batch loss: 24.29203685124715\n",
      "epoch: 153, batch loss: 24.091901938120525\n",
      "epoch: 154, batch loss: 25.716505527496338\n",
      "epoch: 155, batch loss: 23.85973612467448\n",
      "epoch: 156, batch loss: 23.87387752532959\n",
      "epoch: 157, batch loss: 24.63680060704549\n",
      "epoch: 158, batch loss: 23.655876636505127\n",
      "epoch: 159, batch loss: 24.8112055460612\n",
      "epoch: 160, batch loss: 23.075508276621502\n",
      "epoch: 161, batch loss: 24.054856777191162\n",
      "epoch: 162, batch loss: 23.342532793680828\n",
      "epoch: 163, batch loss: 24.303000609079998\n",
      "epoch: 164, batch loss: 23.406475226084392\n",
      "epoch: 165, batch loss: 23.424182256062824\n",
      "epoch: 166, batch loss: 22.93403704961141\n",
      "epoch: 167, batch loss: 23.178912957509358\n",
      "epoch: 168, batch loss: 23.186248938242596\n",
      "epoch: 169, batch loss: 23.578781763712566\n",
      "epoch: 170, batch loss: 24.254202842712402\n",
      "epoch: 171, batch loss: 23.029461701711018\n",
      "epoch: 172, batch loss: 23.52317730585734\n",
      "epoch: 173, batch loss: 22.539280732472736\n",
      "epoch: 174, batch loss: 23.58105754852295\n",
      "epoch: 175, batch loss: 22.884108066558838\n",
      "epoch: 176, batch loss: 22.59572394688924\n",
      "epoch: 177, batch loss: 22.550180435180664\n",
      "epoch: 178, batch loss: 22.250760157903034\n",
      "epoch: 179, batch loss: 22.51269261042277\n",
      "epoch: 180, batch loss: 22.384225368499756\n",
      "epoch: 181, batch loss: 23.187474091847736\n",
      "epoch: 182, batch loss: 22.303278923034668\n",
      "epoch: 183, batch loss: 22.55693594614665\n",
      "epoch: 184, batch loss: 22.288098176320393\n",
      "epoch: 185, batch loss: 22.279823462168377\n",
      "epoch: 186, batch loss: 21.84291474024455\n",
      "epoch: 187, batch loss: 22.869018077850342\n",
      "epoch: 188, batch loss: 22.182281732559204\n",
      "epoch: 189, batch loss: 21.895554701487224\n",
      "epoch: 190, batch loss: 22.993231693903606\n",
      "epoch: 191, batch loss: 22.240030765533447\n",
      "epoch: 192, batch loss: 22.101596196492512\n",
      "epoch: 193, batch loss: 21.97221342722575\n",
      "epoch: 194, batch loss: 21.470808506011963\n",
      "epoch: 195, batch loss: 21.34882378578186\n",
      "epoch: 196, batch loss: 22.39810276031494\n",
      "epoch: 197, batch loss: 21.9082506497701\n",
      "epoch: 198, batch loss: 22.290858109792072\n",
      "epoch: 199, batch loss: 22.073537349700928\n",
      "epoch: 200, batch loss: 22.321126461029053\n",
      "epoch: 201, batch loss: 21.225096861521404\n",
      "epoch: 202, batch loss: 21.661505063374836\n",
      "epoch: 203, batch loss: 21.874509811401367\n",
      "epoch: 204, batch loss: 21.40984519322713\n",
      "epoch: 205, batch loss: 21.257025321324665\n",
      "epoch: 206, batch loss: 21.56596295038859\n",
      "epoch: 207, batch loss: 21.7978089650472\n",
      "epoch: 208, batch loss: 21.370508909225464\n",
      "epoch: 209, batch loss: 21.630654335021973\n",
      "epoch: 210, batch loss: 22.73781696955363\n",
      "epoch: 211, batch loss: 21.343680143356323\n",
      "epoch: 212, batch loss: 21.499703407287598\n",
      "epoch: 213, batch loss: 21.126148621241253\n",
      "epoch: 214, batch loss: 21.07889397939046\n",
      "epoch: 215, batch loss: 22.33314847946167\n",
      "epoch: 216, batch loss: 21.648295799891155\n",
      "epoch: 217, batch loss: 21.096620718638103\n",
      "epoch: 218, batch loss: 21.241124471028645\n",
      "epoch: 219, batch loss: 21.98888365427653\n",
      "epoch: 220, batch loss: 21.68286879857381\n",
      "epoch: 221, batch loss: 21.12943760553996\n",
      "epoch: 222, batch loss: 21.604080359141033\n",
      "epoch: 223, batch loss: 21.18756119410197\n",
      "epoch: 224, batch loss: 21.01164213816325\n",
      "epoch: 225, batch loss: 21.25705051422119\n",
      "epoch: 226, batch loss: 21.73894492785136\n",
      "epoch: 227, batch loss: 21.424851894378662\n",
      "epoch: 228, batch loss: 20.916836818059284\n",
      "epoch: 229, batch loss: 21.05622371037801\n",
      "epoch: 230, batch loss: 21.193257808685303\n",
      "epoch: 231, batch loss: 21.926143487294514\n",
      "epoch: 232, batch loss: 21.60261297225952\n",
      "epoch: 233, batch loss: 21.470986763636272\n",
      "epoch: 234, batch loss: 20.413832664489746\n",
      "epoch: 235, batch loss: 20.421132961908977\n",
      "epoch: 236, batch loss: 21.268933375676472\n",
      "epoch: 237, batch loss: 20.75418488184611\n",
      "epoch: 238, batch loss: 20.414565642674763\n",
      "epoch: 239, batch loss: 20.21038246154785\n",
      "epoch: 240, batch loss: 22.075029929478962\n",
      "epoch: 241, batch loss: 20.081302007039387\n",
      "epoch: 242, batch loss: 20.63807249069214\n",
      "epoch: 243, batch loss: 20.650099674860638\n",
      "epoch: 244, batch loss: 20.268840074539185\n",
      "epoch: 245, batch loss: 21.18653678894043\n",
      "epoch: 246, batch loss: 20.553434769312542\n",
      "epoch: 247, batch loss: 21.545334180196125\n",
      "epoch: 248, batch loss: 21.00327221552531\n",
      "epoch: 249, batch loss: 20.074483315149944\n",
      "epoch: 250, batch loss: 21.361637274424236\n",
      "epoch: 251, batch loss: 21.181575616200764\n",
      "epoch: 252, batch loss: 20.514876127243042\n",
      "epoch: 253, batch loss: 20.511664470036823\n",
      "epoch: 254, batch loss: 20.671342531840008\n",
      "epoch: 255, batch loss: 20.515530904134113\n",
      "epoch: 256, batch loss: 20.94670597712199\n",
      "epoch: 257, batch loss: 20.692386865615845\n",
      "epoch: 258, batch loss: 20.253836313883465\n",
      "epoch: 259, batch loss: 21.88999652862549\n",
      "epoch: 260, batch loss: 21.422616243362427\n",
      "epoch: 261, batch loss: 20.110302607218426\n",
      "epoch: 262, batch loss: 20.782636483510334\n",
      "epoch: 263, batch loss: 20.08956495920817\n",
      "epoch: 264, batch loss: 20.813457171122234\n",
      "epoch: 265, batch loss: 20.036004145940144\n",
      "epoch: 266, batch loss: 20.548868497212727\n",
      "epoch: 267, batch loss: 21.50164572397868\n",
      "epoch: 268, batch loss: 20.044934193293255\n",
      "epoch: 269, batch loss: 20.260332107543945\n",
      "epoch: 270, batch loss: 20.057073672612507\n",
      "epoch: 271, batch loss: 20.14016882578532\n",
      "epoch: 272, batch loss: 20.06065289179484\n",
      "epoch: 273, batch loss: 19.976216236750286\n",
      "epoch: 274, batch loss: 19.689661741256714\n",
      "epoch: 275, batch loss: 20.176049550374348\n",
      "epoch: 276, batch loss: 19.78564190864563\n",
      "epoch: 277, batch loss: 20.0493901570638\n",
      "epoch: 278, batch loss: 20.342294772466023\n",
      "epoch: 279, batch loss: 19.696142037709553\n",
      "epoch: 280, batch loss: 20.323316891988117\n",
      "epoch: 281, batch loss: 19.890771945317585\n",
      "epoch: 282, batch loss: 20.295133193333943\n",
      "epoch: 283, batch loss: 20.60871108373006\n",
      "epoch: 284, batch loss: 20.095295588175457\n",
      "epoch: 285, batch loss: 19.938958803812664\n",
      "epoch: 286, batch loss: 19.97472310066223\n",
      "epoch: 287, batch loss: 19.397746006647747\n",
      "epoch: 288, batch loss: 20.19565947850545\n",
      "epoch: 289, batch loss: 20.744736115137737\n",
      "epoch: 290, batch loss: 19.699729124704998\n",
      "epoch: 291, batch loss: 19.362709045410156\n",
      "epoch: 292, batch loss: 19.49302593866984\n",
      "epoch: 293, batch loss: 19.685622851053875\n",
      "epoch: 294, batch loss: 19.697065671284992\n",
      "epoch: 295, batch loss: 19.399765332539875\n",
      "epoch: 296, batch loss: 19.531625429789226\n",
      "epoch: 297, batch loss: 21.471622625986736\n",
      "epoch: 298, batch loss: 19.962894598642986\n",
      "epoch: 299, batch loss: 19.541157643000286\n",
      "epoch: 300, batch loss: 19.930572032928467\n",
      "epoch: 1, batch loss: 787.5525410970052\n",
      "epoch: 2, batch loss: 782.855224609375\n",
      "epoch: 3, batch loss: 781.5957336425781\n",
      "epoch: 4, batch loss: 771.9991861979166\n",
      "epoch: 5, batch loss: 767.9192352294922\n",
      "epoch: 6, batch loss: 763.8387095133463\n",
      "epoch: 7, batch loss: 760.0772043863932\n",
      "epoch: 8, batch loss: 758.2008819580078\n",
      "epoch: 9, batch loss: 748.3149363199869\n",
      "epoch: 10, batch loss: 741.6049601236979\n",
      "epoch: 11, batch loss: 733.8308715820312\n",
      "epoch: 12, batch loss: 720.8685607910156\n",
      "epoch: 13, batch loss: 717.8581237792969\n",
      "epoch: 14, batch loss: 711.5986785888672\n",
      "epoch: 15, batch loss: 690.3390452067057\n",
      "epoch: 16, batch loss: 680.7671152750651\n",
      "epoch: 17, batch loss: 670.1387176513672\n",
      "epoch: 18, batch loss: 651.5722961425781\n",
      "epoch: 19, batch loss: 627.6312052408854\n",
      "epoch: 20, batch loss: 596.7257029215494\n",
      "epoch: 21, batch loss: 578.5916849772135\n",
      "epoch: 22, batch loss: 542.5115687052408\n",
      "epoch: 23, batch loss: 516.7627487182617\n",
      "epoch: 24, batch loss: 475.4982426961263\n",
      "epoch: 25, batch loss: 437.07026926676434\n",
      "epoch: 26, batch loss: 402.0972137451172\n",
      "epoch: 27, batch loss: 368.2913767496745\n",
      "epoch: 28, batch loss: 329.49931081136066\n",
      "epoch: 29, batch loss: 288.8547999064128\n",
      "epoch: 30, batch loss: 251.51459248860678\n",
      "epoch: 31, batch loss: 226.11596171061197\n",
      "epoch: 32, batch loss: 196.35249455769858\n",
      "epoch: 33, batch loss: 177.1543057759603\n",
      "epoch: 34, batch loss: 155.15846888224283\n",
      "epoch: 35, batch loss: 139.67609786987305\n",
      "epoch: 36, batch loss: 125.69048563639323\n",
      "epoch: 37, batch loss: 120.15635617574056\n",
      "epoch: 38, batch loss: 112.92785708109538\n",
      "epoch: 39, batch loss: 102.71551132202148\n",
      "epoch: 40, batch loss: 98.16787910461426\n",
      "epoch: 41, batch loss: 93.49583276112874\n",
      "epoch: 42, batch loss: 87.06834983825684\n",
      "epoch: 43, batch loss: 81.05822531382243\n",
      "epoch: 44, batch loss: 78.95041275024414\n",
      "epoch: 45, batch loss: 74.54404640197754\n",
      "epoch: 46, batch loss: 71.76068751017253\n",
      "epoch: 47, batch loss: 72.55200894673665\n",
      "epoch: 48, batch loss: 66.23790836334229\n",
      "epoch: 49, batch loss: 62.47179381052653\n",
      "epoch: 50, batch loss: 61.60264205932617\n",
      "epoch: 51, batch loss: 58.379982471466064\n",
      "epoch: 52, batch loss: 59.713340759277344\n",
      "epoch: 53, batch loss: 56.966855684916176\n",
      "epoch: 54, batch loss: 54.7879695892334\n",
      "epoch: 55, batch loss: 54.64203484853109\n",
      "epoch: 56, batch loss: 52.17749818166097\n",
      "epoch: 57, batch loss: 49.19187148412069\n",
      "epoch: 58, batch loss: 50.299743016560875\n",
      "epoch: 59, batch loss: 49.08354568481445\n",
      "epoch: 60, batch loss: 46.218118826548256\n",
      "epoch: 61, batch loss: 47.050109227498375\n",
      "epoch: 62, batch loss: 44.679493268330894\n",
      "epoch: 63, batch loss: 43.307763735453285\n",
      "epoch: 64, batch loss: 42.87205982208252\n",
      "epoch: 65, batch loss: 42.817710081736244\n",
      "epoch: 66, batch loss: 42.34546677271525\n",
      "epoch: 67, batch loss: 40.96331755320231\n",
      "epoch: 68, batch loss: 43.22152662277222\n",
      "epoch: 69, batch loss: 40.337879021962486\n",
      "epoch: 70, batch loss: 38.76253922780355\n",
      "epoch: 71, batch loss: 38.00495847066244\n",
      "epoch: 72, batch loss: 38.62630271911621\n",
      "epoch: 73, batch loss: 37.721485455830894\n",
      "epoch: 74, batch loss: 36.952081044514976\n",
      "epoch: 75, batch loss: 36.116605281829834\n",
      "epoch: 76, batch loss: 35.132164319356285\n",
      "epoch: 77, batch loss: 35.96586434046427\n",
      "epoch: 78, batch loss: 34.73309405644735\n",
      "epoch: 79, batch loss: 34.438053925832115\n",
      "epoch: 80, batch loss: 33.45517619450887\n",
      "epoch: 81, batch loss: 34.13550837834676\n",
      "epoch: 82, batch loss: 33.14341910680135\n",
      "epoch: 83, batch loss: 34.37692276636759\n",
      "epoch: 84, batch loss: 32.86203988393148\n",
      "epoch: 85, batch loss: 32.40808629989624\n",
      "epoch: 86, batch loss: 33.50697151819865\n",
      "epoch: 87, batch loss: 31.876404762268066\n",
      "epoch: 88, batch loss: 33.043680349985756\n",
      "epoch: 89, batch loss: 31.416968027750652\n",
      "epoch: 90, batch loss: 30.637425899505615\n",
      "epoch: 91, batch loss: 32.748739878336586\n",
      "epoch: 92, batch loss: 30.23944632212321\n",
      "epoch: 93, batch loss: 29.385851860046387\n",
      "epoch: 94, batch loss: 29.67736053466797\n",
      "epoch: 95, batch loss: 29.365906079610188\n",
      "epoch: 96, batch loss: 29.682408173878986\n",
      "epoch: 97, batch loss: 29.836348215738933\n",
      "epoch: 98, batch loss: 29.910876750946045\n",
      "epoch: 99, batch loss: 29.780718485514324\n",
      "epoch: 100, batch loss: 28.744067668914795\n",
      "epoch: 101, batch loss: 28.208248297373455\n",
      "epoch: 102, batch loss: 28.552020867665608\n",
      "epoch: 103, batch loss: 28.602197806040447\n",
      "epoch: 104, batch loss: 27.907841046651203\n",
      "epoch: 105, batch loss: 29.373141924540203\n",
      "epoch: 106, batch loss: 27.847598393758137\n",
      "epoch: 107, batch loss: 27.098841826121014\n",
      "epoch: 108, batch loss: 27.438449382781982\n",
      "epoch: 109, batch loss: 27.103754997253418\n",
      "epoch: 110, batch loss: 29.143473625183105\n",
      "epoch: 111, batch loss: 27.272460142771404\n",
      "epoch: 112, batch loss: 26.380061705907185\n",
      "epoch: 113, batch loss: 27.102874914805096\n",
      "epoch: 114, batch loss: 26.95742352803548\n",
      "epoch: 115, batch loss: 26.092424710591633\n",
      "epoch: 116, batch loss: 26.890131155649822\n",
      "epoch: 117, batch loss: 26.54432725906372\n",
      "epoch: 118, batch loss: 26.92641274134318\n",
      "epoch: 119, batch loss: 26.985599040985107\n",
      "epoch: 120, batch loss: 25.693960666656494\n",
      "epoch: 121, batch loss: 25.972817579905193\n",
      "epoch: 122, batch loss: 26.171266555786133\n",
      "epoch: 123, batch loss: 25.5872057278951\n",
      "epoch: 124, batch loss: 25.269477208455402\n",
      "epoch: 125, batch loss: 24.95730193456014\n",
      "epoch: 126, batch loss: 25.972576936086018\n",
      "epoch: 127, batch loss: 26.045027414957683\n",
      "epoch: 128, batch loss: 25.39127190907796\n",
      "epoch: 129, batch loss: 26.426103115081787\n",
      "epoch: 130, batch loss: 24.85341993967692\n",
      "epoch: 131, batch loss: 25.108596801757812\n",
      "epoch: 132, batch loss: 24.828143914540608\n",
      "epoch: 133, batch loss: 24.6106276512146\n",
      "epoch: 134, batch loss: 24.4097482363383\n",
      "epoch: 135, batch loss: 24.226839462916057\n",
      "epoch: 136, batch loss: 24.264734745025635\n",
      "epoch: 137, batch loss: 23.82990527153015\n",
      "epoch: 138, batch loss: 24.29318316777547\n",
      "epoch: 139, batch loss: 25.131688912709553\n",
      "epoch: 140, batch loss: 24.548651854197185\n",
      "epoch: 141, batch loss: 24.069132804870605\n",
      "epoch: 142, batch loss: 24.8382085164388\n",
      "epoch: 143, batch loss: 24.367412726084392\n",
      "epoch: 144, batch loss: 24.27785571416219\n",
      "epoch: 145, batch loss: 23.373105605443318\n",
      "epoch: 146, batch loss: 25.06778160730998\n",
      "epoch: 147, batch loss: 23.4793275197347\n",
      "epoch: 148, batch loss: 24.588090737660725\n",
      "epoch: 149, batch loss: 23.436617851257324\n",
      "epoch: 150, batch loss: 23.67213757832845\n",
      "epoch: 151, batch loss: 23.109901984532673\n",
      "epoch: 152, batch loss: 23.162049690882366\n",
      "epoch: 153, batch loss: 23.70821762084961\n",
      "epoch: 154, batch loss: 23.5177640914917\n",
      "epoch: 155, batch loss: 23.190570036570232\n",
      "epoch: 156, batch loss: 22.70342477162679\n",
      "epoch: 157, batch loss: 23.387314399083454\n",
      "epoch: 158, batch loss: 22.952511469523113\n",
      "epoch: 159, batch loss: 22.868436336517334\n",
      "epoch: 160, batch loss: 22.754048426946003\n",
      "epoch: 161, batch loss: 23.112016995747883\n",
      "epoch: 162, batch loss: 23.02769962946574\n",
      "epoch: 163, batch loss: 22.996387322743733\n",
      "epoch: 164, batch loss: 23.222660144170124\n",
      "epoch: 165, batch loss: 22.77575747172038\n",
      "epoch: 166, batch loss: 23.937308073043823\n",
      "epoch: 167, batch loss: 22.64818048477173\n",
      "epoch: 168, batch loss: 22.99177090326945\n",
      "epoch: 169, batch loss: 22.483898957570393\n",
      "epoch: 170, batch loss: 22.362926642100017\n",
      "epoch: 171, batch loss: 22.586333592732746\n",
      "epoch: 172, batch loss: 22.88832426071167\n",
      "epoch: 173, batch loss: 22.42844041188558\n",
      "epoch: 174, batch loss: 23.28736448287964\n",
      "epoch: 175, batch loss: 22.55006472269694\n",
      "epoch: 176, batch loss: 22.167326768239338\n",
      "epoch: 177, batch loss: 22.17435884475708\n",
      "epoch: 178, batch loss: 22.43687629699707\n",
      "epoch: 179, batch loss: 22.257730801900227\n",
      "epoch: 180, batch loss: 22.237833340962727\n",
      "epoch: 181, batch loss: 22.168769200642902\n",
      "epoch: 182, batch loss: 21.972208976745605\n",
      "epoch: 183, batch loss: 21.544511556625366\n",
      "epoch: 184, batch loss: 21.586220582326252\n",
      "epoch: 185, batch loss: 21.635861953099568\n",
      "epoch: 186, batch loss: 21.583677053451538\n",
      "epoch: 187, batch loss: 21.294981002807617\n",
      "epoch: 188, batch loss: 22.554115931193035\n",
      "epoch: 189, batch loss: 21.56434941291809\n",
      "epoch: 190, batch loss: 22.189517498016357\n",
      "epoch: 191, batch loss: 21.2034486134847\n",
      "epoch: 192, batch loss: 22.59382136662801\n",
      "epoch: 193, batch loss: 21.96736470858256\n",
      "epoch: 194, batch loss: 21.832281907399494\n",
      "epoch: 195, batch loss: 21.335997343063354\n",
      "epoch: 196, batch loss: 21.563941876093548\n",
      "epoch: 197, batch loss: 21.58285641670227\n",
      "epoch: 198, batch loss: 21.300137440363567\n",
      "epoch: 199, batch loss: 21.564401467641193\n",
      "epoch: 200, batch loss: 21.310543060302734\n",
      "epoch: 201, batch loss: 21.8622563680013\n",
      "epoch: 202, batch loss: 20.579191088676453\n",
      "epoch: 203, batch loss: 21.67160193125407\n",
      "epoch: 204, batch loss: 21.95448891321818\n",
      "epoch: 205, batch loss: 21.676894982655842\n",
      "epoch: 206, batch loss: 22.538054704666138\n",
      "epoch: 207, batch loss: 21.133886019388836\n",
      "epoch: 208, batch loss: 21.2486678759257\n",
      "epoch: 209, batch loss: 21.474729776382446\n",
      "epoch: 210, batch loss: 20.81225363413493\n",
      "epoch: 211, batch loss: 20.969694058100384\n",
      "epoch: 212, batch loss: 21.539820671081543\n",
      "epoch: 213, batch loss: 20.978586117426556\n",
      "epoch: 214, batch loss: 22.55194838841756\n",
      "epoch: 215, batch loss: 21.284016450246174\n",
      "epoch: 216, batch loss: 20.737239917119343\n",
      "epoch: 217, batch loss: 21.207473119099934\n",
      "epoch: 218, batch loss: 20.456881364186604\n",
      "epoch: 219, batch loss: 20.504285256067913\n",
      "epoch: 220, batch loss: 21.348453044891357\n",
      "epoch: 221, batch loss: 20.981772502263386\n",
      "epoch: 222, batch loss: 20.531327724456787\n",
      "epoch: 223, batch loss: 21.983764171600342\n",
      "epoch: 224, batch loss: 21.19521967569987\n",
      "epoch: 225, batch loss: 20.655523935953777\n",
      "epoch: 226, batch loss: 20.496081749598186\n",
      "epoch: 227, batch loss: 21.555518627166748\n",
      "epoch: 228, batch loss: 20.840171416600544\n",
      "epoch: 229, batch loss: 21.12878672281901\n",
      "epoch: 230, batch loss: 20.940841277440388\n",
      "epoch: 231, batch loss: 20.64641348520915\n",
      "epoch: 232, batch loss: 20.30363194147746\n",
      "epoch: 233, batch loss: 20.528997898101807\n",
      "epoch: 234, batch loss: 20.76322325070699\n",
      "epoch: 235, batch loss: 20.38512436548869\n",
      "epoch: 236, batch loss: 22.46374225616455\n",
      "epoch: 237, batch loss: 20.51729726791382\n",
      "epoch: 238, batch loss: 20.64781355857849\n",
      "epoch: 239, batch loss: 20.58921225865682\n",
      "epoch: 240, batch loss: 20.198508262634277\n",
      "epoch: 241, batch loss: 20.36935329437256\n",
      "epoch: 242, batch loss: 21.506794452667236\n",
      "epoch: 243, batch loss: 20.35116783777873\n",
      "epoch: 244, batch loss: 20.141019264856975\n",
      "epoch: 245, batch loss: 20.291664282480877\n",
      "epoch: 246, batch loss: 20.73244031270345\n",
      "epoch: 247, batch loss: 20.379306157430012\n",
      "epoch: 248, batch loss: 21.123738209406536\n",
      "epoch: 249, batch loss: 20.678181171417236\n",
      "epoch: 250, batch loss: 19.917906363805134\n",
      "epoch: 251, batch loss: 20.294617811838787\n",
      "epoch: 252, batch loss: 21.022319316864014\n",
      "epoch: 253, batch loss: 20.88471206029256\n",
      "epoch: 254, batch loss: 19.92707832654317\n",
      "epoch: 255, batch loss: 20.607242743174236\n",
      "epoch: 256, batch loss: 20.2126042842865\n",
      "epoch: 257, batch loss: 20.685734351476032\n",
      "epoch: 258, batch loss: 20.066075722376507\n",
      "epoch: 259, batch loss: 21.556408723195393\n",
      "epoch: 260, batch loss: 19.613276720046997\n",
      "epoch: 261, batch loss: 19.921589612960815\n",
      "epoch: 262, batch loss: 20.19316864013672\n",
      "epoch: 263, batch loss: 19.729238271713257\n",
      "epoch: 264, batch loss: 19.959028005599976\n",
      "epoch: 265, batch loss: 20.61118483543396\n",
      "epoch: 266, batch loss: 21.360511859258015\n",
      "epoch: 267, batch loss: 20.042504628499348\n",
      "epoch: 268, batch loss: 20.034199555714924\n",
      "epoch: 269, batch loss: 20.114272753397625\n",
      "epoch: 270, batch loss: 19.994669437408447\n",
      "epoch: 271, batch loss: 19.83866556485494\n",
      "epoch: 272, batch loss: 20.323884169260662\n",
      "epoch: 273, batch loss: 19.77618606885274\n",
      "epoch: 274, batch loss: 19.683963378270466\n",
      "epoch: 275, batch loss: 20.197147051493328\n",
      "epoch: 276, batch loss: 19.835357983907063\n",
      "epoch: 277, batch loss: 19.72731415430705\n",
      "epoch: 278, batch loss: 20.340132236480713\n",
      "epoch: 279, batch loss: 20.043395678202312\n",
      "epoch: 280, batch loss: 19.769200960795086\n",
      "epoch: 281, batch loss: 20.205263137817383\n",
      "epoch: 282, batch loss: 19.47754693031311\n",
      "epoch: 283, batch loss: 19.858166694641113\n",
      "epoch: 284, batch loss: 20.0641188621521\n",
      "epoch: 285, batch loss: 20.125669558842976\n",
      "epoch: 286, batch loss: 19.477697213490803\n",
      "epoch: 287, batch loss: 19.9598605632782\n",
      "epoch: 288, batch loss: 20.092798391977947\n",
      "epoch: 289, batch loss: 19.445475816726685\n",
      "epoch: 290, batch loss: 19.915605306625366\n",
      "epoch: 291, batch loss: 19.358975172042847\n",
      "epoch: 292, batch loss: 19.643016417821247\n",
      "epoch: 293, batch loss: 19.536174376805622\n",
      "epoch: 294, batch loss: 20.244688828786213\n",
      "epoch: 295, batch loss: 19.768996636072796\n",
      "epoch: 296, batch loss: 19.531233708063763\n",
      "epoch: 297, batch loss: 19.994091192881267\n",
      "epoch: 298, batch loss: 19.77590036392212\n",
      "epoch: 299, batch loss: 19.696111520131428\n",
      "epoch: 300, batch loss: 20.633948644002277\n",
      "epoch: 1, batch loss: 783.0946502685547\n",
      "epoch: 2, batch loss: 776.9013468424479\n",
      "epoch: 3, batch loss: 774.5886433919271\n",
      "epoch: 4, batch loss: 768.5972798665365\n",
      "epoch: 5, batch loss: 769.1081186930338\n",
      "epoch: 6, batch loss: 763.9523468017578\n",
      "epoch: 7, batch loss: 755.7865142822266\n",
      "epoch: 8, batch loss: 751.6805775960287\n",
      "epoch: 9, batch loss: 744.7793019612631\n",
      "epoch: 10, batch loss: 738.8113810221354\n",
      "epoch: 11, batch loss: 732.8917083740234\n",
      "epoch: 12, batch loss: 721.9725952148438\n",
      "epoch: 13, batch loss: 706.9723663330078\n",
      "epoch: 14, batch loss: 691.6364288330078\n",
      "epoch: 15, batch loss: 674.745127360026\n",
      "epoch: 16, batch loss: 651.4825286865234\n",
      "epoch: 17, batch loss: 622.1396840413412\n",
      "epoch: 18, batch loss: 595.7957661946615\n",
      "epoch: 19, batch loss: 563.6407572428385\n",
      "epoch: 20, batch loss: 524.1297658284506\n",
      "epoch: 21, batch loss: 487.20434824625653\n",
      "epoch: 22, batch loss: 446.94617462158203\n",
      "epoch: 23, batch loss: 394.6210301717122\n",
      "epoch: 24, batch loss: 356.1103744506836\n",
      "epoch: 25, batch loss: 305.83152770996094\n",
      "epoch: 26, batch loss: 270.9365743001302\n",
      "epoch: 27, batch loss: 224.98113886515299\n",
      "epoch: 28, batch loss: 194.74239349365234\n",
      "epoch: 29, batch loss: 167.6286481221517\n",
      "epoch: 30, batch loss: 142.15190442403158\n",
      "epoch: 31, batch loss: 122.5470479329427\n",
      "epoch: 32, batch loss: 106.31629308064778\n",
      "epoch: 33, batch loss: 95.26003774007161\n",
      "epoch: 34, batch loss: 83.706298828125\n",
      "epoch: 35, batch loss: 78.56622378031413\n",
      "epoch: 36, batch loss: 71.95864454905193\n",
      "epoch: 37, batch loss: 68.31986649831136\n",
      "epoch: 38, batch loss: 62.595726013183594\n",
      "epoch: 39, batch loss: 58.07003211975098\n",
      "epoch: 40, batch loss: 55.54245853424072\n",
      "epoch: 41, batch loss: 53.116964975992836\n",
      "epoch: 42, batch loss: 50.67414061228434\n",
      "epoch: 43, batch loss: 48.27061653137207\n",
      "epoch: 44, batch loss: 46.890881856282554\n",
      "epoch: 45, batch loss: 46.74885622660319\n",
      "epoch: 46, batch loss: 44.57380962371826\n",
      "epoch: 47, batch loss: 43.486301263173424\n",
      "epoch: 48, batch loss: 42.724191188812256\n",
      "epoch: 49, batch loss: 41.73389768600464\n",
      "epoch: 50, batch loss: 40.9492834409078\n",
      "epoch: 51, batch loss: 41.82984749476115\n",
      "epoch: 52, batch loss: 39.41476043065389\n",
      "epoch: 53, batch loss: 40.22900724411011\n",
      "epoch: 54, batch loss: 39.0079353650411\n",
      "epoch: 55, batch loss: 37.877158323923744\n",
      "epoch: 56, batch loss: 38.489551067352295\n",
      "epoch: 57, batch loss: 38.00545152028402\n",
      "epoch: 58, batch loss: 37.64000701904297\n",
      "epoch: 59, batch loss: 36.26090701421102\n",
      "epoch: 60, batch loss: 36.02913602193197\n",
      "epoch: 61, batch loss: 36.96964009602865\n",
      "epoch: 62, batch loss: 36.22326771418253\n",
      "epoch: 63, batch loss: 35.32075119018555\n",
      "epoch: 64, batch loss: 37.54379749298096\n",
      "epoch: 65, batch loss: 35.02291282018026\n",
      "epoch: 66, batch loss: 35.605313777923584\n",
      "epoch: 67, batch loss: 34.776588439941406\n",
      "epoch: 68, batch loss: 33.5779382387797\n",
      "epoch: 69, batch loss: 34.28577629725138\n",
      "epoch: 70, batch loss: 34.18088340759277\n",
      "epoch: 71, batch loss: 34.46201149622599\n",
      "epoch: 72, batch loss: 34.64990393320719\n",
      "epoch: 73, batch loss: 33.10452286402384\n",
      "epoch: 74, batch loss: 33.0353430112203\n",
      "epoch: 75, batch loss: 33.09860070546468\n",
      "epoch: 76, batch loss: 32.31527694066366\n",
      "epoch: 77, batch loss: 32.07666333516439\n",
      "epoch: 78, batch loss: 32.25812896092733\n",
      "epoch: 79, batch loss: 32.07776419321696\n",
      "epoch: 80, batch loss: 32.47449556986491\n",
      "epoch: 81, batch loss: 31.080705165863037\n",
      "epoch: 82, batch loss: 33.05562448501587\n",
      "epoch: 83, batch loss: 30.783045927683514\n",
      "epoch: 84, batch loss: 30.6788223584493\n",
      "epoch: 85, batch loss: 30.12777837117513\n",
      "epoch: 86, batch loss: 30.29566176732381\n",
      "epoch: 87, batch loss: 30.15300528208415\n",
      "epoch: 88, batch loss: 31.277592817942303\n",
      "epoch: 89, batch loss: 31.051023960113525\n",
      "epoch: 90, batch loss: 30.531213601430256\n",
      "epoch: 91, batch loss: 29.537427107493084\n",
      "epoch: 92, batch loss: 29.398818969726562\n",
      "epoch: 93, batch loss: 29.60328499476115\n",
      "epoch: 94, batch loss: 30.50241009394328\n",
      "epoch: 95, batch loss: 28.59542719523112\n",
      "epoch: 96, batch loss: 28.889005184173584\n",
      "epoch: 97, batch loss: 28.535629431406658\n",
      "epoch: 98, batch loss: 28.657219568888348\n",
      "epoch: 99, batch loss: 28.26566203435262\n",
      "epoch: 100, batch loss: 29.419854799906414\n",
      "epoch: 101, batch loss: 28.764620621999104\n",
      "epoch: 102, batch loss: 28.244747002919514\n",
      "epoch: 103, batch loss: 28.45952033996582\n",
      "epoch: 104, batch loss: 28.026611646016438\n",
      "epoch: 105, batch loss: 27.53163766860962\n",
      "epoch: 106, batch loss: 27.21917708714803\n",
      "epoch: 107, batch loss: 28.18130413691203\n",
      "epoch: 108, batch loss: 28.364485263824463\n",
      "epoch: 109, batch loss: 28.48597288131714\n",
      "epoch: 110, batch loss: 27.237542470296223\n",
      "epoch: 111, batch loss: 27.792483965555828\n",
      "epoch: 112, batch loss: 26.900778929392498\n",
      "epoch: 113, batch loss: 26.973714510599773\n",
      "epoch: 114, batch loss: 26.774603207906086\n",
      "epoch: 115, batch loss: 27.12466828028361\n",
      "epoch: 116, batch loss: 26.586846987406414\n",
      "epoch: 117, batch loss: 26.227325280507404\n",
      "epoch: 118, batch loss: 26.665196418762207\n",
      "epoch: 119, batch loss: 25.781862099965412\n",
      "epoch: 120, batch loss: 25.85438299179077\n",
      "epoch: 121, batch loss: 26.650848070780437\n",
      "epoch: 122, batch loss: 25.607582887013752\n",
      "epoch: 123, batch loss: 26.183764934539795\n",
      "epoch: 124, batch loss: 26.831426461537678\n",
      "epoch: 125, batch loss: 25.369566122690838\n",
      "epoch: 126, batch loss: 26.13526725769043\n",
      "epoch: 127, batch loss: 25.60269371668498\n",
      "epoch: 128, batch loss: 25.53438599904378\n",
      "epoch: 129, batch loss: 25.309563000996906\n",
      "epoch: 130, batch loss: 25.603947798411053\n",
      "epoch: 131, batch loss: 25.113747437795002\n",
      "epoch: 132, batch loss: 24.598398447036743\n",
      "epoch: 133, batch loss: 24.958847045898438\n",
      "epoch: 134, batch loss: 25.638437906901043\n",
      "epoch: 135, batch loss: 24.77034330368042\n",
      "epoch: 136, batch loss: 25.56661065419515\n",
      "epoch: 137, batch loss: 25.319861094156902\n",
      "epoch: 138, batch loss: 25.73071050643921\n",
      "epoch: 139, batch loss: 24.784271558125813\n",
      "epoch: 140, batch loss: 24.31081740061442\n",
      "epoch: 141, batch loss: 24.39195903142293\n",
      "epoch: 142, batch loss: 23.76394240061442\n",
      "epoch: 143, batch loss: 24.20927858352661\n",
      "epoch: 144, batch loss: 24.16839027404785\n",
      "epoch: 145, batch loss: 24.475646495819092\n",
      "epoch: 146, batch loss: 24.626819133758545\n",
      "epoch: 147, batch loss: 25.59169594446818\n",
      "epoch: 148, batch loss: 24.286107540130615\n",
      "epoch: 149, batch loss: 24.694270133972168\n",
      "epoch: 150, batch loss: 24.036261717478435\n",
      "epoch: 151, batch loss: 23.490309079488117\n",
      "epoch: 152, batch loss: 23.73452075322469\n",
      "epoch: 153, batch loss: 23.177407264709473\n",
      "epoch: 154, batch loss: 24.388118584950764\n",
      "epoch: 155, batch loss: 23.686555067698162\n",
      "epoch: 156, batch loss: 23.02519718805949\n",
      "epoch: 157, batch loss: 23.218256950378418\n",
      "epoch: 158, batch loss: 23.194295406341553\n",
      "epoch: 159, batch loss: 23.805869420369465\n",
      "epoch: 160, batch loss: 23.1036278406779\n",
      "epoch: 161, batch loss: 24.030593872070312\n",
      "epoch: 162, batch loss: 23.034216562906902\n",
      "epoch: 163, batch loss: 23.501081148783367\n",
      "epoch: 164, batch loss: 23.471025069554646\n",
      "epoch: 165, batch loss: 22.98141272862752\n",
      "epoch: 166, batch loss: 24.133163452148438\n",
      "epoch: 167, batch loss: 23.076812903086346\n",
      "epoch: 168, batch loss: 22.747773965199787\n",
      "epoch: 169, batch loss: 23.207409938176472\n",
      "epoch: 170, batch loss: 22.405505577723186\n",
      "epoch: 171, batch loss: 22.912468592325848\n",
      "epoch: 172, batch loss: 23.27364190419515\n",
      "epoch: 173, batch loss: 23.9420649210612\n",
      "epoch: 174, batch loss: 23.346648693084717\n",
      "epoch: 175, batch loss: 22.723762671152752\n",
      "epoch: 176, batch loss: 22.756669600804646\n",
      "epoch: 177, batch loss: 22.645602703094482\n",
      "epoch: 178, batch loss: 22.29477008183797\n",
      "epoch: 179, batch loss: 22.062156041463215\n",
      "epoch: 180, batch loss: 22.60015932718913\n",
      "epoch: 181, batch loss: 22.86170228322347\n",
      "epoch: 182, batch loss: 22.456907272338867\n",
      "epoch: 183, batch loss: 22.19863247871399\n",
      "epoch: 184, batch loss: 22.935976425806682\n",
      "epoch: 185, batch loss: 22.514852205912273\n",
      "epoch: 186, batch loss: 22.543988863627117\n",
      "epoch: 187, batch loss: 21.774237632751465\n",
      "epoch: 188, batch loss: 21.714335997899372\n",
      "epoch: 189, batch loss: 22.32507522900899\n",
      "epoch: 190, batch loss: 22.146467367808025\n",
      "epoch: 191, batch loss: 22.755339860916138\n",
      "epoch: 192, batch loss: 22.021351019541424\n",
      "epoch: 193, batch loss: 21.584295352300007\n",
      "epoch: 194, batch loss: 21.878760655721027\n",
      "epoch: 195, batch loss: 21.976129690806072\n",
      "epoch: 196, batch loss: 21.50593360265096\n",
      "epoch: 197, batch loss: 21.537668704986572\n",
      "epoch: 198, batch loss: 22.205291271209717\n",
      "epoch: 199, batch loss: 21.916737874348957\n",
      "epoch: 200, batch loss: 21.508950233459473\n",
      "epoch: 201, batch loss: 22.913114070892334\n",
      "epoch: 202, batch loss: 22.511873881022137\n",
      "epoch: 203, batch loss: 21.489945888519287\n",
      "epoch: 204, batch loss: 21.769724289576214\n",
      "epoch: 205, batch loss: 21.36001427968343\n",
      "epoch: 206, batch loss: 21.689162095387776\n",
      "epoch: 207, batch loss: 21.48062006632487\n",
      "epoch: 208, batch loss: 21.20559525489807\n",
      "epoch: 209, batch loss: 21.830917676289875\n",
      "epoch: 210, batch loss: 21.322129249572754\n",
      "epoch: 211, batch loss: 22.004238923390705\n",
      "epoch: 212, batch loss: 22.15438675880432\n",
      "epoch: 213, batch loss: 22.328279336293537\n",
      "epoch: 214, batch loss: 21.007280826568604\n",
      "epoch: 215, batch loss: 22.177806695302326\n",
      "epoch: 216, batch loss: 21.52922511100769\n",
      "epoch: 217, batch loss: 21.462791601816814\n",
      "epoch: 218, batch loss: 20.853634039560955\n",
      "epoch: 219, batch loss: 21.13508939743042\n",
      "epoch: 220, batch loss: 21.19856882095337\n",
      "epoch: 221, batch loss: 21.21265721321106\n",
      "epoch: 222, batch loss: 20.773366689682007\n",
      "epoch: 223, batch loss: 21.442167282104492\n",
      "epoch: 224, batch loss: 20.940457820892334\n",
      "epoch: 225, batch loss: 20.6821235815684\n",
      "epoch: 226, batch loss: 20.63993302981059\n",
      "epoch: 227, batch loss: 21.17176103591919\n",
      "epoch: 228, batch loss: 21.135859807332356\n",
      "epoch: 229, batch loss: 21.21635103225708\n",
      "epoch: 230, batch loss: 21.934403896331787\n",
      "epoch: 231, batch loss: 20.893521308898926\n",
      "epoch: 232, batch loss: 20.645057280858357\n",
      "epoch: 233, batch loss: 21.07393439610799\n",
      "epoch: 234, batch loss: 20.763079802195232\n",
      "epoch: 235, batch loss: 20.57747999827067\n",
      "epoch: 236, batch loss: 21.777486324310303\n",
      "epoch: 237, batch loss: 20.63039008776347\n",
      "epoch: 238, batch loss: 20.383629480997723\n",
      "epoch: 239, batch loss: 20.722596009572346\n",
      "epoch: 240, batch loss: 20.314356962839764\n",
      "epoch: 241, batch loss: 20.741177717844646\n",
      "epoch: 242, batch loss: 20.638264815012615\n",
      "epoch: 243, batch loss: 22.24590500195821\n",
      "epoch: 244, batch loss: 20.23913272221883\n",
      "epoch: 245, batch loss: 20.12048888206482\n",
      "epoch: 246, batch loss: 20.78529143333435\n",
      "epoch: 247, batch loss: 20.88335116704305\n",
      "epoch: 248, batch loss: 21.059462308883667\n",
      "epoch: 249, batch loss: 20.541724999745686\n",
      "epoch: 250, batch loss: 20.083332777023315\n",
      "epoch: 251, batch loss: 20.367713769276936\n",
      "epoch: 252, batch loss: 20.568238496780396\n",
      "epoch: 253, batch loss: 20.259513934453327\n",
      "epoch: 254, batch loss: 21.242079734802246\n",
      "epoch: 255, batch loss: 20.727037986119587\n",
      "epoch: 256, batch loss: 20.330592393875122\n",
      "epoch: 257, batch loss: 21.174882491429646\n",
      "epoch: 258, batch loss: 20.212614218393963\n",
      "epoch: 259, batch loss: 20.28485377629598\n",
      "epoch: 260, batch loss: 19.80227780342102\n",
      "epoch: 261, batch loss: 20.243096510569256\n",
      "epoch: 262, batch loss: 21.122876803080242\n",
      "epoch: 263, batch loss: 20.24114457766215\n",
      "epoch: 264, batch loss: 19.89953096707662\n",
      "epoch: 265, batch loss: 20.659892241160076\n",
      "epoch: 266, batch loss: 20.399306774139404\n",
      "epoch: 267, batch loss: 19.858829975128174\n",
      "epoch: 268, batch loss: 20.444709062576294\n",
      "epoch: 269, batch loss: 19.881999890009563\n",
      "epoch: 270, batch loss: 20.315212567647297\n",
      "epoch: 271, batch loss: 20.065823554992676\n",
      "epoch: 272, batch loss: 20.341923077901203\n",
      "epoch: 273, batch loss: 20.72591694196065\n",
      "epoch: 274, batch loss: 20.390230814615887\n",
      "epoch: 275, batch loss: 19.948521216710407\n",
      "epoch: 276, batch loss: 20.083653291066486\n",
      "epoch: 277, batch loss: 20.58965826034546\n",
      "epoch: 278, batch loss: 20.57674543062846\n",
      "epoch: 279, batch loss: 20.538474559783936\n",
      "epoch: 280, batch loss: 20.637420733769734\n",
      "epoch: 281, batch loss: 20.111015955607098\n",
      "epoch: 282, batch loss: 20.255642255147297\n",
      "epoch: 283, batch loss: 19.628010988235474\n",
      "epoch: 284, batch loss: 20.007132530212402\n",
      "epoch: 285, batch loss: 20.051988522211712\n",
      "epoch: 286, batch loss: 20.50348385175069\n",
      "epoch: 287, batch loss: 21.049671411514282\n",
      "epoch: 288, batch loss: 19.954225301742554\n",
      "epoch: 289, batch loss: 19.89153814315796\n",
      "epoch: 290, batch loss: 20.30211528142293\n",
      "epoch: 291, batch loss: 20.616921106974285\n",
      "epoch: 292, batch loss: 20.14106814066569\n",
      "epoch: 293, batch loss: 20.449207464853924\n",
      "epoch: 294, batch loss: 19.55010501543681\n",
      "epoch: 295, batch loss: 19.975857734680176\n",
      "epoch: 296, batch loss: 20.37214469909668\n",
      "epoch: 297, batch loss: 19.89296817779541\n",
      "epoch: 298, batch loss: 20.170888980229694\n",
      "epoch: 299, batch loss: 20.24038831392924\n",
      "epoch: 300, batch loss: 19.27413996060689\n",
      "epoch: 1, batch loss: 778.8519287109375\n",
      "epoch: 2, batch loss: 779.9054870605469\n",
      "epoch: 3, batch loss: 772.8080749511719\n",
      "epoch: 4, batch loss: 769.7429402669271\n",
      "epoch: 5, batch loss: 760.7622019449869\n",
      "epoch: 6, batch loss: 755.9899953206381\n",
      "epoch: 7, batch loss: 752.5177307128906\n",
      "epoch: 8, batch loss: 751.8245798746744\n",
      "epoch: 9, batch loss: 742.9432474772135\n",
      "epoch: 10, batch loss: 738.0252583821615\n",
      "epoch: 11, batch loss: 728.993398030599\n",
      "epoch: 12, batch loss: 716.7269083658854\n",
      "epoch: 13, batch loss: 708.7430979410807\n",
      "epoch: 14, batch loss: 697.4610443115234\n",
      "epoch: 15, batch loss: 682.9855397542318\n",
      "epoch: 16, batch loss: 666.9287109375\n",
      "epoch: 17, batch loss: 649.0473225911459\n",
      "epoch: 18, batch loss: 630.2307078043619\n",
      "epoch: 19, batch loss: 609.5064290364584\n",
      "epoch: 20, batch loss: 576.3204091389974\n",
      "epoch: 21, batch loss: 550.1815617879232\n",
      "epoch: 22, batch loss: 520.2788670857748\n",
      "epoch: 23, batch loss: 494.09676869710285\n",
      "epoch: 24, batch loss: 455.7933578491211\n",
      "epoch: 25, batch loss: 427.1462631225586\n",
      "epoch: 26, batch loss: 391.84436289469403\n",
      "epoch: 27, batch loss: 362.0526351928711\n",
      "epoch: 28, batch loss: 338.45093790690106\n",
      "epoch: 29, batch loss: 312.87002817789715\n",
      "epoch: 30, batch loss: 288.1131286621094\n",
      "epoch: 31, batch loss: 272.02008056640625\n",
      "epoch: 32, batch loss: 256.1307563781738\n",
      "epoch: 33, batch loss: 241.07782745361328\n",
      "epoch: 34, batch loss: 232.0475196838379\n",
      "epoch: 35, batch loss: 221.57066599527994\n",
      "epoch: 36, batch loss: 209.9649225870768\n",
      "epoch: 37, batch loss: 198.7892723083496\n",
      "epoch: 38, batch loss: 192.0620257059733\n",
      "epoch: 39, batch loss: 182.41427103678384\n",
      "epoch: 40, batch loss: 175.00500361124674\n",
      "epoch: 41, batch loss: 169.19856643676758\n",
      "epoch: 42, batch loss: 163.64135360717773\n",
      "epoch: 43, batch loss: 154.91869990030924\n",
      "epoch: 44, batch loss: 148.77869160970053\n",
      "epoch: 45, batch loss: 142.20806948343912\n",
      "epoch: 46, batch loss: 134.910919825236\n",
      "epoch: 47, batch loss: 127.91575113932292\n",
      "epoch: 48, batch loss: 121.48977724711101\n",
      "epoch: 49, batch loss: 116.02508862813313\n",
      "epoch: 50, batch loss: 109.7503236134847\n",
      "epoch: 51, batch loss: 104.46559143066406\n",
      "epoch: 52, batch loss: 100.16518529256184\n",
      "epoch: 53, batch loss: 92.38509559631348\n",
      "epoch: 54, batch loss: 91.10621897379558\n",
      "epoch: 55, batch loss: 84.0353234608968\n",
      "epoch: 56, batch loss: 81.26636918385823\n",
      "epoch: 57, batch loss: 75.44976552327473\n",
      "epoch: 58, batch loss: 72.86027940114339\n",
      "epoch: 59, batch loss: 69.86213080088298\n",
      "epoch: 60, batch loss: 64.85333093007405\n",
      "epoch: 61, batch loss: 63.72745259602865\n",
      "epoch: 62, batch loss: 61.609758377075195\n",
      "epoch: 63, batch loss: 58.64491208394369\n",
      "epoch: 64, batch loss: 55.97072728474935\n",
      "epoch: 65, batch loss: 55.709444999694824\n",
      "epoch: 66, batch loss: 52.96962038675944\n",
      "epoch: 67, batch loss: 51.27099355061849\n",
      "epoch: 68, batch loss: 49.611677487691246\n",
      "epoch: 69, batch loss: 47.62186654408773\n",
      "epoch: 70, batch loss: 47.36224333445231\n",
      "epoch: 71, batch loss: 45.6973991394043\n",
      "epoch: 72, batch loss: 45.14476362864176\n",
      "epoch: 73, batch loss: 44.030905405680336\n",
      "epoch: 74, batch loss: 43.61608918507894\n",
      "epoch: 75, batch loss: 43.256014347076416\n",
      "epoch: 76, batch loss: 42.59448258082072\n",
      "epoch: 77, batch loss: 43.030189196268715\n",
      "epoch: 78, batch loss: 40.74085839589437\n",
      "epoch: 79, batch loss: 40.66658719380697\n",
      "epoch: 80, batch loss: 39.187524477640785\n",
      "epoch: 81, batch loss: 39.60429604848226\n",
      "epoch: 82, batch loss: 39.459279696146645\n",
      "epoch: 83, batch loss: 38.49611488978068\n",
      "epoch: 84, batch loss: 38.209450562795006\n",
      "epoch: 85, batch loss: 36.46627696355184\n",
      "epoch: 86, batch loss: 36.5067408879598\n",
      "epoch: 87, batch loss: 37.52395566304525\n",
      "epoch: 88, batch loss: 36.200581073760986\n",
      "epoch: 89, batch loss: 35.842354933420815\n",
      "epoch: 90, batch loss: 35.39521106084188\n",
      "epoch: 91, batch loss: 34.3556744257609\n",
      "epoch: 92, batch loss: 33.87736336390177\n",
      "epoch: 93, batch loss: 33.85862382253011\n",
      "epoch: 94, batch loss: 33.57561731338501\n",
      "epoch: 95, batch loss: 35.31566127141317\n",
      "epoch: 96, batch loss: 33.6689338684082\n",
      "epoch: 97, batch loss: 33.43079010645548\n",
      "epoch: 98, batch loss: 33.29817374547323\n",
      "epoch: 99, batch loss: 31.970804532368977\n",
      "epoch: 100, batch loss: 31.60443369547526\n",
      "epoch: 101, batch loss: 32.195052782694496\n",
      "epoch: 102, batch loss: 32.008389472961426\n",
      "epoch: 103, batch loss: 31.579650402069092\n",
      "epoch: 104, batch loss: 30.780608018239338\n",
      "epoch: 105, batch loss: 31.39319880803426\n",
      "epoch: 106, batch loss: 31.2327462832133\n",
      "epoch: 107, batch loss: 30.2431960105896\n",
      "epoch: 108, batch loss: 30.64757013320923\n",
      "epoch: 109, batch loss: 30.505456924438477\n",
      "epoch: 110, batch loss: 30.001331488291424\n",
      "epoch: 111, batch loss: 30.708716074625652\n",
      "epoch: 112, batch loss: 30.386107126871746\n",
      "epoch: 113, batch loss: 29.270849227905273\n",
      "epoch: 114, batch loss: 28.78452666600545\n",
      "epoch: 115, batch loss: 29.16003719965617\n",
      "epoch: 116, batch loss: 29.260594367980957\n",
      "epoch: 117, batch loss: 28.645618120829266\n",
      "epoch: 118, batch loss: 28.809149583180744\n",
      "epoch: 119, batch loss: 28.839370409647625\n",
      "epoch: 120, batch loss: 28.198063373565674\n",
      "epoch: 121, batch loss: 28.114655653635662\n",
      "epoch: 122, batch loss: 28.073887983957928\n",
      "epoch: 123, batch loss: 27.795198440551758\n",
      "epoch: 124, batch loss: 27.67277956008911\n",
      "epoch: 125, batch loss: 26.87649393081665\n",
      "epoch: 126, batch loss: 27.52991533279419\n",
      "epoch: 127, batch loss: 27.029431343078613\n",
      "epoch: 128, batch loss: 27.14696979522705\n",
      "epoch: 129, batch loss: 26.468483606974285\n",
      "epoch: 130, batch loss: 26.97135130564372\n",
      "epoch: 131, batch loss: 27.14277712504069\n",
      "epoch: 132, batch loss: 27.1219383875529\n",
      "epoch: 133, batch loss: 25.82968219121297\n",
      "epoch: 134, batch loss: 25.996328989664715\n",
      "epoch: 135, batch loss: 26.0403954188029\n",
      "epoch: 136, batch loss: 26.367281277974445\n",
      "epoch: 137, batch loss: 25.67513386408488\n",
      "epoch: 138, batch loss: 27.09533468882243\n",
      "epoch: 139, batch loss: 25.510279178619385\n",
      "epoch: 140, batch loss: 25.497945149739582\n",
      "epoch: 141, batch loss: 25.83033577601115\n",
      "epoch: 142, batch loss: 26.04729175567627\n",
      "epoch: 143, batch loss: 25.307063420613606\n",
      "epoch: 144, batch loss: 25.300011157989502\n",
      "epoch: 145, batch loss: 25.381489435831707\n",
      "epoch: 146, batch loss: 25.72274128595988\n",
      "epoch: 147, batch loss: 25.117305914560955\n",
      "epoch: 148, batch loss: 25.348082065582275\n",
      "epoch: 149, batch loss: 24.64112917582194\n",
      "epoch: 150, batch loss: 24.836873690287273\n",
      "epoch: 151, batch loss: 25.173712094624836\n",
      "epoch: 152, batch loss: 25.460990111033123\n",
      "epoch: 153, batch loss: 24.265737533569336\n",
      "epoch: 154, batch loss: 25.36020851135254\n",
      "epoch: 155, batch loss: 24.11038573582967\n",
      "epoch: 156, batch loss: 23.8916490872701\n",
      "epoch: 157, batch loss: 24.418479124704998\n",
      "epoch: 158, batch loss: 23.920454184214275\n",
      "epoch: 159, batch loss: 24.043970108032227\n",
      "epoch: 160, batch loss: 24.112114588419598\n",
      "epoch: 161, batch loss: 24.0600905418396\n",
      "epoch: 162, batch loss: 23.46792443593343\n",
      "epoch: 163, batch loss: 23.786982536315918\n",
      "epoch: 164, batch loss: 23.403708457946777\n",
      "epoch: 165, batch loss: 23.224112033843994\n",
      "epoch: 166, batch loss: 23.936004320780437\n",
      "epoch: 167, batch loss: 23.442025661468506\n",
      "epoch: 168, batch loss: 23.29001474380493\n",
      "epoch: 169, batch loss: 23.391589323679607\n",
      "epoch: 170, batch loss: 22.840532700220745\n",
      "epoch: 171, batch loss: 22.916202704111736\n",
      "epoch: 172, batch loss: 22.881404240926106\n",
      "epoch: 173, batch loss: 23.456035614013672\n",
      "epoch: 174, batch loss: 23.492408275604248\n",
      "epoch: 175, batch loss: 22.60718075434367\n",
      "epoch: 176, batch loss: 22.41425410906474\n",
      "epoch: 177, batch loss: 23.955176830291748\n",
      "epoch: 178, batch loss: 23.474719047546387\n",
      "epoch: 179, batch loss: 22.419012784957886\n",
      "epoch: 180, batch loss: 22.171504815419514\n",
      "epoch: 181, batch loss: 22.759480953216553\n",
      "epoch: 182, batch loss: 22.082813501358032\n",
      "epoch: 183, batch loss: 23.085748195648193\n",
      "epoch: 184, batch loss: 22.259910583496094\n",
      "epoch: 185, batch loss: 21.87539569536845\n",
      "epoch: 186, batch loss: 22.02276309331258\n",
      "epoch: 187, batch loss: 22.626721064249676\n",
      "epoch: 188, batch loss: 21.784619967142742\n",
      "epoch: 189, batch loss: 22.213145176569622\n",
      "epoch: 190, batch loss: 22.425718148549397\n",
      "epoch: 191, batch loss: 21.635364373524983\n",
      "epoch: 192, batch loss: 22.75572411219279\n",
      "epoch: 193, batch loss: 21.807811578114826\n",
      "epoch: 194, batch loss: 21.346803347269695\n",
      "epoch: 195, batch loss: 21.315319776535034\n",
      "epoch: 196, batch loss: 22.380542755126953\n",
      "epoch: 197, batch loss: 22.167466084162395\n",
      "epoch: 198, batch loss: 21.555408318837483\n",
      "epoch: 199, batch loss: 21.083519220352173\n",
      "epoch: 200, batch loss: 21.017085075378418\n",
      "epoch: 201, batch loss: 21.95039463043213\n",
      "epoch: 202, batch loss: 21.090516010920208\n",
      "epoch: 203, batch loss: 21.381497859954834\n",
      "epoch: 204, batch loss: 21.424545605977375\n",
      "epoch: 205, batch loss: 21.10039512316386\n",
      "epoch: 206, batch loss: 22.340603828430176\n",
      "epoch: 207, batch loss: 21.421073118845623\n",
      "epoch: 208, batch loss: 22.768766085306805\n",
      "epoch: 209, batch loss: 22.02859592437744\n",
      "epoch: 210, batch loss: 21.032052357991535\n",
      "epoch: 211, batch loss: 21.05419095357259\n",
      "epoch: 212, batch loss: 21.010603427886963\n",
      "epoch: 213, batch loss: 21.499405225118\n",
      "epoch: 214, batch loss: 21.496674219767254\n",
      "epoch: 215, batch loss: 20.929848829905193\n",
      "epoch: 216, batch loss: 20.9845388730367\n",
      "epoch: 217, batch loss: 21.03384041786194\n",
      "epoch: 218, batch loss: 20.788795789082844\n",
      "epoch: 219, batch loss: 20.66515811284383\n",
      "epoch: 220, batch loss: 20.698781569798786\n",
      "epoch: 221, batch loss: 21.0252423286438\n",
      "epoch: 222, batch loss: 21.43750786781311\n",
      "epoch: 223, batch loss: 21.27887026468913\n",
      "epoch: 224, batch loss: 20.76746129989624\n",
      "epoch: 225, batch loss: 20.346421321233112\n",
      "epoch: 226, batch loss: 20.763198216756184\n",
      "epoch: 227, batch loss: 20.74792544047038\n",
      "epoch: 228, batch loss: 20.891310532887776\n",
      "epoch: 229, batch loss: 20.856375376383465\n",
      "epoch: 230, batch loss: 20.498089631398518\n",
      "epoch: 231, batch loss: 20.460888385772705\n",
      "epoch: 232, batch loss: 20.130258162816364\n",
      "epoch: 233, batch loss: 20.132497708002727\n",
      "epoch: 234, batch loss: 21.153599500656128\n",
      "epoch: 235, batch loss: 21.021119594573975\n",
      "epoch: 236, batch loss: 20.246700048446655\n",
      "epoch: 237, batch loss: 21.046645561854046\n",
      "epoch: 238, batch loss: 20.468840678532917\n",
      "epoch: 239, batch loss: 20.79259951909383\n",
      "epoch: 240, batch loss: 20.42668914794922\n",
      "epoch: 241, batch loss: 20.625173489252727\n",
      "epoch: 242, batch loss: 20.248843669891357\n",
      "epoch: 243, batch loss: 20.11751437187195\n",
      "epoch: 244, batch loss: 20.583869059880573\n",
      "epoch: 245, batch loss: 20.099297126134235\n",
      "epoch: 246, batch loss: 20.578226884206135\n",
      "epoch: 247, batch loss: 19.96626043319702\n",
      "epoch: 248, batch loss: 20.587937116622925\n",
      "epoch: 249, batch loss: 19.699630578358967\n",
      "epoch: 250, batch loss: 20.323506116867065\n",
      "epoch: 251, batch loss: 20.898593823115032\n",
      "epoch: 252, batch loss: 20.39053241411845\n",
      "epoch: 253, batch loss: 19.618929386138916\n",
      "epoch: 254, batch loss: 20.294235706329346\n",
      "epoch: 255, batch loss: 20.075347741444904\n",
      "epoch: 256, batch loss: 20.13228901227315\n",
      "epoch: 257, batch loss: 20.303128004074097\n",
      "epoch: 258, batch loss: 19.689659118652344\n",
      "epoch: 259, batch loss: 20.58193055788676\n",
      "epoch: 260, batch loss: 19.822091658910114\n",
      "epoch: 261, batch loss: 20.745102008183796\n",
      "epoch: 262, batch loss: 19.98071813583374\n",
      "epoch: 263, batch loss: 20.00248146057129\n",
      "epoch: 264, batch loss: 20.502813736597698\n",
      "epoch: 265, batch loss: 20.00291347503662\n",
      "epoch: 266, batch loss: 19.794955730438232\n",
      "epoch: 267, batch loss: 19.66594139734904\n",
      "epoch: 268, batch loss: 19.281216939290363\n",
      "epoch: 269, batch loss: 19.779041051864624\n",
      "epoch: 270, batch loss: 20.085506280263264\n",
      "epoch: 271, batch loss: 19.776069164276123\n",
      "epoch: 272, batch loss: 19.31523601214091\n",
      "epoch: 273, batch loss: 19.953083912531536\n",
      "epoch: 274, batch loss: 19.485839525858562\n",
      "epoch: 275, batch loss: 19.231489896774292\n",
      "epoch: 276, batch loss: 19.257561763127644\n",
      "epoch: 277, batch loss: 19.293461402257282\n",
      "epoch: 278, batch loss: 20.5393701394399\n",
      "epoch: 279, batch loss: 19.650930245717365\n",
      "epoch: 280, batch loss: 19.503869374593098\n",
      "epoch: 281, batch loss: 19.348176956176758\n",
      "epoch: 282, batch loss: 19.530229806900024\n",
      "epoch: 283, batch loss: 20.065478801727295\n",
      "epoch: 284, batch loss: 20.102660576502483\n",
      "epoch: 285, batch loss: 20.214261611302693\n",
      "epoch: 286, batch loss: 19.05052884419759\n",
      "epoch: 287, batch loss: 20.286955912907917\n",
      "epoch: 288, batch loss: 19.84999918937683\n",
      "epoch: 289, batch loss: 19.251476367314655\n",
      "epoch: 290, batch loss: 19.71499538421631\n",
      "epoch: 291, batch loss: 19.15999166170756\n",
      "epoch: 292, batch loss: 19.609817107518513\n",
      "epoch: 293, batch loss: 19.177209854125977\n",
      "epoch: 294, batch loss: 18.939176718393963\n",
      "epoch: 295, batch loss: 18.999338547388714\n",
      "epoch: 296, batch loss: 19.46837528546651\n",
      "epoch: 297, batch loss: 19.198349634806316\n",
      "epoch: 298, batch loss: 19.80577023824056\n",
      "epoch: 299, batch loss: 19.172484556833904\n",
      "epoch: 300, batch loss: 20.377042293548584\n",
      "epoch: 1, batch loss: 786.4232432047526\n",
      "epoch: 2, batch loss: 783.1111246744791\n",
      "epoch: 3, batch loss: 786.9267985026041\n",
      "epoch: 4, batch loss: 778.4778493245443\n",
      "epoch: 5, batch loss: 777.1712799072266\n",
      "epoch: 6, batch loss: 778.3566843668619\n",
      "epoch: 7, batch loss: 770.6630452473959\n",
      "epoch: 8, batch loss: 763.0971323649088\n",
      "epoch: 9, batch loss: 758.5738677978516\n",
      "epoch: 10, batch loss: 759.1529795328776\n",
      "epoch: 11, batch loss: 754.1963094075521\n",
      "epoch: 12, batch loss: 743.2587280273438\n",
      "epoch: 13, batch loss: 736.1467742919922\n",
      "epoch: 14, batch loss: 725.4825693766276\n",
      "epoch: 15, batch loss: 721.9141693115234\n",
      "epoch: 16, batch loss: 706.7690277099609\n",
      "epoch: 17, batch loss: 689.6986440022787\n",
      "epoch: 18, batch loss: 673.6510467529297\n",
      "epoch: 19, batch loss: 647.2320810953776\n",
      "epoch: 20, batch loss: 617.4137115478516\n",
      "epoch: 21, batch loss: 597.6384582519531\n",
      "epoch: 22, batch loss: 559.0159123738607\n",
      "epoch: 23, batch loss: 513.804339090983\n",
      "epoch: 24, batch loss: 481.1382090250651\n",
      "epoch: 25, batch loss: 433.4315643310547\n",
      "epoch: 26, batch loss: 390.8988037109375\n",
      "epoch: 27, batch loss: 348.0759582519531\n",
      "epoch: 28, batch loss: 300.23978424072266\n",
      "epoch: 29, batch loss: 262.9188232421875\n",
      "epoch: 30, batch loss: 221.3231633504232\n",
      "epoch: 31, batch loss: 191.43075052897134\n",
      "epoch: 32, batch loss: 165.00588607788086\n",
      "epoch: 33, batch loss: 141.57287661234537\n",
      "epoch: 34, batch loss: 124.6849759419759\n",
      "epoch: 35, batch loss: 112.8580977121989\n",
      "epoch: 36, batch loss: 101.45423380533855\n",
      "epoch: 37, batch loss: 96.75058682759602\n",
      "epoch: 38, batch loss: 89.55836423238118\n",
      "epoch: 39, batch loss: 86.23089631398518\n",
      "epoch: 40, batch loss: 77.895933787028\n",
      "epoch: 41, batch loss: 78.80988025665283\n",
      "epoch: 42, batch loss: 73.50764115651448\n",
      "epoch: 43, batch loss: 71.33164437611897\n",
      "epoch: 44, batch loss: 69.37472947438557\n",
      "epoch: 45, batch loss: 64.79962412516277\n",
      "epoch: 46, batch loss: 64.41108830769856\n",
      "epoch: 47, batch loss: 65.41352208455403\n",
      "epoch: 48, batch loss: 60.52749061584473\n",
      "epoch: 49, batch loss: 60.662638664245605\n",
      "epoch: 50, batch loss: 57.60138511657715\n",
      "epoch: 51, batch loss: 58.076459884643555\n",
      "epoch: 52, batch loss: 55.18897660573324\n",
      "epoch: 53, batch loss: 53.71452522277832\n",
      "epoch: 54, batch loss: 52.14539925257365\n",
      "epoch: 55, batch loss: 52.93745994567871\n",
      "epoch: 56, batch loss: 51.5179074605306\n",
      "epoch: 57, batch loss: 51.00882085164388\n",
      "epoch: 58, batch loss: 50.15825621287028\n",
      "epoch: 59, batch loss: 47.70611413319906\n",
      "epoch: 60, batch loss: 49.534106731414795\n",
      "epoch: 61, batch loss: 47.44419606526693\n",
      "epoch: 62, batch loss: 46.65629514058431\n",
      "epoch: 63, batch loss: 44.47356096903483\n",
      "epoch: 64, batch loss: 44.60028696060181\n",
      "epoch: 65, batch loss: 44.61491870880127\n",
      "epoch: 66, batch loss: 42.150580724080406\n",
      "epoch: 67, batch loss: 43.39103126525879\n",
      "epoch: 68, batch loss: 42.19468069076538\n",
      "epoch: 69, batch loss: 40.812111695607506\n",
      "epoch: 70, batch loss: 41.98973894119263\n",
      "epoch: 71, batch loss: 41.43417771657308\n",
      "epoch: 72, batch loss: 39.30250771840414\n",
      "epoch: 73, batch loss: 40.91566801071167\n",
      "epoch: 74, batch loss: 39.14722983042399\n",
      "epoch: 75, batch loss: 38.39081907272339\n",
      "epoch: 76, batch loss: 38.25481986999512\n",
      "epoch: 77, batch loss: 37.74783802032471\n",
      "epoch: 78, batch loss: 37.223164558410645\n",
      "epoch: 79, batch loss: 36.52465089162191\n",
      "epoch: 80, batch loss: 35.873444398244224\n",
      "epoch: 81, batch loss: 36.476860682169594\n",
      "epoch: 82, batch loss: 37.563809076944985\n",
      "epoch: 83, batch loss: 35.06980037689209\n",
      "epoch: 84, batch loss: 34.54674561818441\n",
      "epoch: 85, batch loss: 34.01050806045532\n",
      "epoch: 86, batch loss: 34.001768271128334\n",
      "epoch: 87, batch loss: 33.60870107014974\n",
      "epoch: 88, batch loss: 34.28065379460653\n",
      "epoch: 89, batch loss: 33.81587394078573\n",
      "epoch: 90, batch loss: 32.94758971532186\n",
      "epoch: 91, batch loss: 33.85264492034912\n",
      "epoch: 92, batch loss: 33.399068196614586\n",
      "epoch: 93, batch loss: 32.87386465072632\n",
      "epoch: 94, batch loss: 31.50298039118449\n",
      "epoch: 95, batch loss: 32.29881890614828\n",
      "epoch: 96, batch loss: 31.565467675526936\n",
      "epoch: 97, batch loss: 31.21257972717285\n",
      "epoch: 98, batch loss: 30.98688856760661\n",
      "epoch: 99, batch loss: 30.628326257069904\n",
      "epoch: 100, batch loss: 29.858044306437176\n",
      "epoch: 101, batch loss: 30.00158103307088\n",
      "epoch: 102, batch loss: 29.606163501739502\n",
      "epoch: 103, batch loss: 29.683911005655926\n",
      "epoch: 104, batch loss: 31.344863732655842\n",
      "epoch: 105, batch loss: 29.702552636464436\n",
      "epoch: 106, batch loss: 29.22520097096761\n",
      "epoch: 107, batch loss: 29.176900386810303\n",
      "epoch: 108, batch loss: 30.227479775746662\n",
      "epoch: 109, batch loss: 29.294285774230957\n",
      "epoch: 110, batch loss: 28.663207689921062\n",
      "epoch: 111, batch loss: 28.9365398089091\n",
      "epoch: 112, batch loss: 27.766196727752686\n",
      "epoch: 113, batch loss: 27.922680060068767\n",
      "epoch: 114, batch loss: 28.505289395650227\n",
      "epoch: 115, batch loss: 27.806432565053303\n",
      "epoch: 116, batch loss: 27.530029614766438\n",
      "epoch: 117, batch loss: 27.506747404734295\n",
      "epoch: 118, batch loss: 26.99498112996419\n",
      "epoch: 119, batch loss: 26.949325402577717\n",
      "epoch: 120, batch loss: 26.773356755574543\n",
      "epoch: 121, batch loss: 27.407713254292805\n",
      "epoch: 122, batch loss: 26.739518642425537\n",
      "epoch: 123, batch loss: 26.84449529647827\n",
      "epoch: 124, batch loss: 27.902815500895183\n",
      "epoch: 125, batch loss: 26.319741408030193\n",
      "epoch: 126, batch loss: 25.951171398162842\n",
      "epoch: 127, batch loss: 27.261826197306316\n",
      "epoch: 128, batch loss: 25.66194200515747\n",
      "epoch: 129, batch loss: 26.434011459350586\n",
      "epoch: 130, batch loss: 27.03618844350179\n",
      "epoch: 131, batch loss: 27.498553435007732\n",
      "epoch: 132, batch loss: 25.877624988555908\n",
      "epoch: 133, batch loss: 25.251000722249348\n",
      "epoch: 134, batch loss: 25.115890661875408\n",
      "epoch: 135, batch loss: 25.192803064982098\n",
      "epoch: 136, batch loss: 25.27787176767985\n",
      "epoch: 137, batch loss: 24.851472059885662\n",
      "epoch: 138, batch loss: 24.884311199188232\n",
      "epoch: 139, batch loss: 25.118516604105633\n",
      "epoch: 140, batch loss: 24.72039333979289\n",
      "epoch: 141, batch loss: 24.524331251780193\n",
      "epoch: 142, batch loss: 25.374749422073364\n",
      "epoch: 143, batch loss: 24.186341444651287\n",
      "epoch: 144, batch loss: 25.439204692840576\n",
      "epoch: 145, batch loss: 24.54697593053182\n",
      "epoch: 146, batch loss: 25.144917329152424\n",
      "epoch: 147, batch loss: 23.881370703379314\n",
      "epoch: 148, batch loss: 25.180952231089275\n",
      "epoch: 149, batch loss: 23.82907708485921\n",
      "epoch: 150, batch loss: 23.58311152458191\n",
      "epoch: 151, batch loss: 23.93486801783244\n",
      "epoch: 152, batch loss: 23.4493145942688\n",
      "epoch: 153, batch loss: 24.033780097961426\n",
      "epoch: 154, batch loss: 23.46397352218628\n",
      "epoch: 155, batch loss: 23.902591387430828\n",
      "epoch: 156, batch loss: 24.353015899658203\n",
      "epoch: 157, batch loss: 23.982470353444416\n",
      "epoch: 158, batch loss: 23.0645858446757\n",
      "epoch: 159, batch loss: 22.956480741500854\n",
      "epoch: 160, batch loss: 23.36459557215373\n",
      "epoch: 161, batch loss: 23.525429884592693\n",
      "epoch: 162, batch loss: 23.655880133310955\n",
      "epoch: 163, batch loss: 22.97081995010376\n",
      "epoch: 164, batch loss: 22.618407408396404\n",
      "epoch: 165, batch loss: 22.933561325073242\n",
      "epoch: 166, batch loss: 23.21223251024882\n",
      "epoch: 167, batch loss: 22.696878671646118\n",
      "epoch: 168, batch loss: 22.66290060679118\n",
      "epoch: 169, batch loss: 22.878018856048584\n",
      "epoch: 170, batch loss: 22.20804762840271\n",
      "epoch: 171, batch loss: 22.59566609064738\n",
      "epoch: 172, batch loss: 23.049932320912678\n",
      "epoch: 173, batch loss: 22.540417194366455\n",
      "epoch: 174, batch loss: 22.58233372370402\n",
      "epoch: 175, batch loss: 22.50666419665019\n",
      "epoch: 176, batch loss: 22.156029144922893\n",
      "epoch: 177, batch loss: 21.888938983281452\n",
      "epoch: 178, batch loss: 22.92033926645915\n",
      "epoch: 179, batch loss: 23.449517409006756\n",
      "epoch: 180, batch loss: 21.63043014208476\n",
      "epoch: 181, batch loss: 23.382235527038574\n",
      "epoch: 182, batch loss: 22.141948858896892\n",
      "epoch: 183, batch loss: 22.380982875823975\n",
      "epoch: 184, batch loss: 22.891829013824463\n",
      "epoch: 185, batch loss: 21.831050237019856\n",
      "epoch: 186, batch loss: 21.423083702723186\n",
      "epoch: 187, batch loss: 21.629637161890667\n",
      "epoch: 188, batch loss: 21.542505741119385\n",
      "epoch: 189, batch loss: 21.845752080281574\n",
      "epoch: 190, batch loss: 21.598150650660198\n",
      "epoch: 191, batch loss: 21.877710501352947\n",
      "epoch: 192, batch loss: 21.57315460840861\n",
      "epoch: 193, batch loss: 21.6012765566508\n",
      "epoch: 194, batch loss: 22.252971013387043\n",
      "epoch: 195, batch loss: 22.226030667622883\n",
      "epoch: 196, batch loss: 21.984224796295166\n",
      "epoch: 197, batch loss: 22.147504170735676\n",
      "epoch: 198, batch loss: 21.185030619303387\n",
      "epoch: 199, batch loss: 21.649639050165813\n",
      "epoch: 200, batch loss: 20.86194960276286\n",
      "epoch: 201, batch loss: 20.99342481295268\n",
      "epoch: 202, batch loss: 21.156309127807617\n",
      "epoch: 203, batch loss: 21.176833311716717\n",
      "epoch: 204, batch loss: 21.15965525309245\n",
      "epoch: 205, batch loss: 21.87300157546997\n",
      "epoch: 206, batch loss: 21.3115332921346\n",
      "epoch: 207, batch loss: 20.97919487953186\n",
      "epoch: 208, batch loss: 21.477204004923504\n",
      "epoch: 209, batch loss: 21.24432396888733\n",
      "epoch: 210, batch loss: 21.683959086736042\n",
      "epoch: 211, batch loss: 21.378138462702434\n",
      "epoch: 212, batch loss: 21.160391012827557\n",
      "epoch: 213, batch loss: 20.71520121892293\n",
      "epoch: 214, batch loss: 21.363324006398518\n",
      "epoch: 215, batch loss: 21.163548469543457\n",
      "epoch: 216, batch loss: 21.900585333506267\n",
      "epoch: 217, batch loss: 21.280776977539062\n",
      "epoch: 218, batch loss: 20.72218592961629\n",
      "epoch: 219, batch loss: 20.74759308497111\n",
      "epoch: 220, batch loss: 22.095704396565754\n",
      "epoch: 221, batch loss: 20.995898564656574\n",
      "epoch: 222, batch loss: 20.308960755666096\n",
      "epoch: 223, batch loss: 21.632758537928265\n",
      "epoch: 224, batch loss: 20.631083806355793\n",
      "epoch: 225, batch loss: 20.658328692118328\n",
      "epoch: 226, batch loss: 20.634459733963013\n",
      "epoch: 227, batch loss: 21.627568403879803\n",
      "epoch: 228, batch loss: 20.67933750152588\n",
      "epoch: 229, batch loss: 20.353209018707275\n",
      "epoch: 230, batch loss: 20.179625511169434\n",
      "epoch: 231, batch loss: 20.606817722320557\n",
      "epoch: 232, batch loss: 20.390447934468586\n",
      "epoch: 233, batch loss: 20.519929885864258\n",
      "epoch: 234, batch loss: 20.508597294489544\n",
      "epoch: 235, batch loss: 20.315212965011597\n",
      "epoch: 236, batch loss: 20.17322548230489\n",
      "epoch: 237, batch loss: 20.640432198842365\n",
      "epoch: 238, batch loss: 20.366209665934246\n",
      "epoch: 239, batch loss: 20.151838541030884\n",
      "epoch: 240, batch loss: 20.06678541501363\n",
      "epoch: 241, batch loss: 19.86294198036194\n",
      "epoch: 242, batch loss: 20.161678552627563\n",
      "epoch: 243, batch loss: 20.63390652338664\n",
      "epoch: 244, batch loss: 20.33029532432556\n",
      "epoch: 245, batch loss: 20.655949592590332\n",
      "epoch: 246, batch loss: 21.28738848368327\n",
      "epoch: 247, batch loss: 19.936169624328613\n",
      "epoch: 248, batch loss: 19.96632393201192\n",
      "epoch: 249, batch loss: 21.10557158788045\n",
      "epoch: 250, batch loss: 20.0910648504893\n",
      "epoch: 251, batch loss: 20.213958740234375\n",
      "epoch: 252, batch loss: 19.692313114802044\n",
      "epoch: 253, batch loss: 20.523341576258343\n",
      "epoch: 254, batch loss: 20.26648513476054\n",
      "epoch: 255, batch loss: 19.61675715446472\n",
      "epoch: 256, batch loss: 20.312741200129192\n",
      "epoch: 257, batch loss: 19.997451066970825\n",
      "epoch: 258, batch loss: 19.888201634089153\n",
      "epoch: 259, batch loss: 19.781866947809856\n",
      "epoch: 260, batch loss: 19.99864363670349\n",
      "epoch: 261, batch loss: 20.476758003234863\n",
      "epoch: 262, batch loss: 20.44651214281718\n",
      "epoch: 263, batch loss: 19.50983754793803\n",
      "epoch: 264, batch loss: 20.408480882644653\n",
      "epoch: 265, batch loss: 19.569603045781452\n",
      "epoch: 266, batch loss: 20.03593937555949\n",
      "epoch: 267, batch loss: 20.372470060984295\n",
      "epoch: 268, batch loss: 19.888442516326904\n",
      "epoch: 269, batch loss: 19.664896647135418\n",
      "epoch: 270, batch loss: 20.26364254951477\n",
      "epoch: 271, batch loss: 20.00440263748169\n",
      "epoch: 272, batch loss: 19.99841769536336\n",
      "epoch: 273, batch loss: 19.97565213839213\n",
      "epoch: 274, batch loss: 19.9948627948761\n",
      "epoch: 275, batch loss: 19.766674359639484\n",
      "epoch: 276, batch loss: 20.287880818049114\n",
      "epoch: 277, batch loss: 20.577837308247883\n",
      "epoch: 278, batch loss: 19.623663743336994\n",
      "epoch: 279, batch loss: 20.73593584696452\n",
      "epoch: 280, batch loss: 19.838115692138672\n",
      "epoch: 281, batch loss: 19.781402428944904\n",
      "epoch: 282, batch loss: 19.326254447301228\n",
      "epoch: 283, batch loss: 19.30791409810384\n",
      "epoch: 284, batch loss: 19.395761251449585\n",
      "epoch: 285, batch loss: 19.588611523310345\n",
      "epoch: 286, batch loss: 19.52807394663493\n",
      "epoch: 287, batch loss: 19.958327452341717\n",
      "epoch: 288, batch loss: 19.403106212615967\n",
      "epoch: 289, batch loss: 19.30321725209554\n",
      "epoch: 290, batch loss: 19.02408464749654\n",
      "epoch: 291, batch loss: 20.323599497477215\n",
      "epoch: 292, batch loss: 19.529952843983967\n",
      "epoch: 293, batch loss: 19.352195739746094\n",
      "epoch: 294, batch loss: 19.21790138880412\n",
      "epoch: 295, batch loss: 19.06952142715454\n",
      "epoch: 296, batch loss: 19.44012649854024\n",
      "epoch: 297, batch loss: 19.691771507263184\n",
      "epoch: 298, batch loss: 19.622372150421143\n",
      "epoch: 299, batch loss: 19.457316319147747\n",
      "epoch: 300, batch loss: 19.374645312627155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24.577520806585724"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss = []\n",
    "for i in range(10):\n",
    "    test_loss.append(train_mmoe())\n",
    "np.array(test_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28835be80>]"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].plot(train_loss)\n",
    "axes[1].plot(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.1554, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1, y2, y3, y4, y5 = sbm(X_test)\n",
    "criterion(y1, y_test[:, 0].unsqueeze(-1)) + criterion(y2, y_test[:, 1].unsqueeze(-1)) + criterion(y3, y_test[:, 2].unsqueeze(-1)) + criterion(y4, y_test[:, 3].unsqueeze(-1)) + criterion(y5, y_test[:, 4].unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.8366, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1, y2, y3, y4, y5 = mmoe(X_test)\n",
    "criterion(y1, y_test[:, 0].unsqueeze(-1)) + criterion(y2, y_test[:, 1].unsqueeze(-1)) + criterion(y3, y_test[:, 2].unsqueeze(-1)) + criterion(y4, y_test[:, 3].unsqueeze(-1)) + criterion(y5, y_test[:, 4].unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2298e+00, -1.4372e-15, -1.0431e-15,  0.0000e+00,  3.5232e-13,\n",
       "        -1.2255e+00, -1.9575e-01, -4.9312e-01,  5.2069e-02, -6.6861e-01,\n",
       "         7.8489e-01,  1.2274e-01])"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mmoe, \"mmoe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmoe = torch.load(\"mmoe.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tch12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
